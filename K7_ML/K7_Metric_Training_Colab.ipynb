{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K7 Metric Training with G₂ Holonomy - Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB')\n",
    "else:\n",
    "    print(' NO GPU! Go to Runtime → Change runtime type → GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib scipy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CompactG2Network(nn.Module):\n",
    "    '''Compact network for Colab - optimized for T4 memory.'''\n",
    "    def __init__(self, hidden_dims=[256,256,128], num_freq=32):\n",
    "        super().__init__()\n",
    "        self.register_buffer('B', torch.randn(7, num_freq) * 2.0)\n",
    "        layers = []\n",
    "        prev = 2 * num_freq\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), nn.SiLU(), nn.LayerNorm(h)]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 28))  # Upper tri of 7x7\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        with torch.no_grad():\n",
    "            self.mlp[-1].weight.mul_(0.01)\n",
    "            self.mlp[-1].bias.zero_()\n",
    "\n",
    "    def forward(self, coords):\n",
    "        x = 2*np.pi * coords @ self.B\n",
    "        x = torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
    "        upper = self.mlp(x)\n",
    "        batch = coords.shape[0]\n",
    "        metric = torch.zeros(batch, 7, 7, device=coords.device)\n",
    "        idx = 0\n",
    "        for i in range(7):\n",
    "            for j in range(i,7):\n",
    "                if i==j:\n",
    "                    metric[:,i,j] = torch.nn.functional.softplus(upper[:,idx]) + 0.1\n",
    "                else:\n",
    "                    metric[:,i,j] = metric[:,j,i] = upper[:,idx] * 0.1\n",
    "                idx += 1\n",
    "        return metric + torch.eye(7, device=coords.device).unsqueeze(0)\n",
    "\n",
    "print('✓ Network defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ricci_loss_fast(metric, coords):\n",
    "    '''Fast Ricci loss - simplified for speed.'''\n",
    "    batch = metric.shape[0]\n",
    "    device = metric.device\n",
    "    metric_inv = torch.linalg.inv(metric)\n",
    "    ricci = torch.zeros(batch, 7, 7, device=device)\n",
    "    for i in range(7):\n",
    "        grad_i = torch.autograd.grad(\n",
    "            metric[:,:,:].sum(), coords,\n",
    "            create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        for j in range(7):\n",
    "            ricci[:,i,j] = torch.sum(metric_inv[:,i,:] * grad_i[:,j].unsqueeze(-1))\n",
    "    return ricci\n",
    "\n",
    "print('✓ Ricci computation defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = CompactG2Network().to(device)\n",
    "        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.opt, T_0=500, eta_min=1e-7)\n",
    "        self.history = {'epoch':[], 'loss':[], 'ricci':[]}\n",
    "    \n",
    "    def train_epoch(self, epoch, batch_size=512):\n",
    "        self.model.train()\n",
    "        coords = torch.randn(batch_size, 7, device=self.device) * 5.0\n",
    "        coords.requires_grad_(True)\n",
    "        metric = self.model(coords)\n",
    "        ricci = ricci_loss_fast(metric, coords)\n",
    "        ricci_loss = torch.mean(ricci**2)\n",
    "        reg = torch.mean((metric - torch.eye(7, device=self.device))**2)\n",
    "        total = (10.0 if epoch > 1000 else 1.0) * ricci_loss + 0.01 * reg\n",
    "        self.opt.zero_grad()\n",
    "        total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.opt.step()\n",
    "        self.scheduler.step()\n",
    "        self.history['epoch'].append(epoch)\n",
    "        self.history['loss'].append(total.item())\n",
    "        self.history['ricci'].append(ricci_loss.item())\n",
    "        return total.item(), ricci_loss.item()\n",
    "\n",
    "print('✓ Trainer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(device)\n",
    "print(f'Device: {device}')\n",
    "print(f'Parameters: {sum(p.numel() for p in trainer.model.parameters()):,}')\n",
    "print('\\nTraining...\\n')\n",
    "\n",
    "total_epochs = 10000\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    loss, ricci = trainer.train_epoch(epoch)\n",
    "    if epoch % 50 == 0 or epoch == total_epochs-1:\n",
    "        elapsed = time.time()-start\n",
    "        eta = elapsed/(epoch+1)*(total_epochs-epoch-1)\n",
    "        print(f'Epoch {epoch:4d}/{total_epochs} | Loss: {loss:.6e} | Ricci: {ricci:.6e} | {elapsed/60:.1f}min | ETA: {eta/60:.1f}min')\n",
    "    if epoch % 300 == 0 and epoch > 0:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "        ax[0].semilogy(trainer.history['epoch'], trainer.history['loss'], 'b-', lw=2)\n",
    "        ax[0].set_title('Total Loss')\n",
    "        ax[0].grid(alpha=0.3)\n",
    "        ax[1].semilogy(trainer.history['epoch'], trainer.history['ricci'], 'g-', lw=2)\n",
    "        ax[1].set_title('Ricci Loss')\n",
    "        ax[1].grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f'Epoch {epoch} | Loss: {loss:.6e} | Ricci: {ricci:.6e}')\n",
    "\n",
    "print(f'\\n✓ Complete! Time: {(time.time()-start)/60:.1f}min')\n",
    "print(f'Final loss: {trainer.history[\"loss\"][-1]:.6e}')\n",
    "print(f'Reduction: {trainer.history[\"loss\"][0]/trainer.history[\"loss\"][-1]:.1f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "epochs = np.array(trainer.history['epoch'])\n",
    "losses = np.array(trainer.history['loss'])\n",
    "ricci = np.array(trainer.history['ricci'])\n",
    "\n",
    "ax[0].semilogy(epochs, losses, 'b-', lw=2, label='Total')\n",
    "ax[0].semilogy(epochs, ricci, 'g-', lw=2, alpha=0.7, label='Ricci')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training Curves')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "if len(losses) > 100:\n",
    "    conv = -np.gradient(np.log(losses+1e-10))\n",
    "    smooth = np.convolve(conv, np.ones(100)/100, 'valid')\n",
    "    ax[1].plot(epochs[:len(smooth)], smooth, 'r-', lw=2)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Convergence Rate')\n",
    "    ax[1].set_title('Efficiency')\n",
    "    ax[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "test_pts = torch.tensor([[0.,0.,0.,0.,0.,0.,0.], [1.,0.,0.,0.,0.,0.,0.], [0.,1.,0.,0.,0.,0.,0.]], device=device)\n",
    "with torch.no_grad():\n",
    "    metrics = trainer.model(test_pts)\n",
    "\n",
    "print('\\nLearned Metric:')\n",
    "for i, pt in enumerate(test_pts):\n",
    "    g = metrics[i].cpu().numpy()\n",
    "    eig = np.linalg.eigvalsh(g)\n",
    "    print(f'\\nPoint {i+1}: {pt.cpu().numpy()}')\n",
    "    print(f'  Diagonal: {np.diag(g)}')\n",
    "    print(f'  Eigenvalues: {eig}')\n",
    "    print(f'  Det: {np.linalg.det(g):.6f}')\n",
    "    print(f'  Condition: {eig.max()/eig.min():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {\n",
    "    'model': trainer.model.state_dict(),\n",
    "    'optimizer': trainer.opt.state_dict(),\n",
    "    'history': trainer.history,\n",
    "    'final_loss': trainer.history['loss'][-1]\n",
    "}\n",
    "torch.save(ckpt, 'k7_metric_final.pt')\n",
    "print('✓ Saved: k7_metric_final.pt')\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(trainer.history).to_csv('history.csv', index=False)\n",
    "print('✓ Saved: history.csv')\n",
    "\n",
    "print('\\n Download files from left panel')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
