{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFT Spectral Gap v2: Rigorous G₂ Laplacian\n",
    "\n",
    "**Objectif**: Calculer λ₁ sur variétés G₂ avec le **vrai Laplacien courbé**.\n",
    "\n",
    "**Améliorations v2**:\n",
    "1. Laplacien courbé: $\\Delta_g f = \\frac{1}{\\sqrt{g}} \\partial_i(\\sqrt{g} g^{ij} \\partial_j f)$\n",
    "2. Normalisation Vol = 1 (métrique canonique)\n",
    "3. Calibration sur S⁷ (λ₁ = 7 exact)\n",
    "4. Plus d'epochs, architecture plus profonde\n",
    "5. Visualisation des eigenfunctions\n",
    "\n",
    "**Date**: 2026-01-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch numpy scipy matplotlib tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.special import jv  # Bessel functions\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Curved Laplacian Implementation\n",
    "\n",
    "The Laplace-Beltrami operator on a Riemannian manifold $(M, g)$:\n",
    "\n",
    "$$\\Delta_g f = \\frac{1}{\\sqrt{\\det g}} \\partial_i \\left( \\sqrt{\\det g} \\, g^{ij} \\partial_j f \\right)$$\n",
    "\n",
    "For diagonal metric $g_{ij} = \\text{diag}(g_{11}, \\ldots, g_{nn})$:\n",
    "\n",
    "$$\\Delta_g f = \\sum_i \\frac{1}{g_{ii}} \\partial_i^2 f + \\sum_i \\frac{\\partial_i(\\sqrt{g}/g_{ii})}{\\sqrt{g}} \\partial_i f$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurvedLaplacian:\n",
    "    \"\"\"\n",
    "    Compute Laplace-Beltrami operator for diagonal metrics.\n",
    "    \n",
    "    Δ_g f = (1/√g) ∂_i(√g g^{ij} ∂_j f)\n",
    "    \n",
    "    For diagonal g: Δ_g f = Σ_i (1/g_ii) ∂²f/∂x_i² + Γ-terms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_fn):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metric_fn: function(x) -> g_diag, returns diagonal metric components\n",
    "                       g_diag has shape (batch, dim)\n",
    "        \"\"\"\n",
    "        self.metric_fn = metric_fn\n",
    "        \n",
    "    def __call__(self, f_net: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Δ_g f at points x.\n",
    "        \n",
    "        Uses automatic differentiation for all derivatives.\n",
    "        \"\"\"\n",
    "        x = x.requires_grad_(True)\n",
    "        batch_size, dim = x.shape\n",
    "        \n",
    "        # Get metric at each point\n",
    "        g_diag = self.metric_fn(x)  # (batch, dim)\n",
    "        g_inv = 1.0 / (g_diag + 1e-10)  # g^{ii} = 1/g_{ii}\n",
    "        sqrt_det_g = torch.sqrt(torch.prod(g_diag, dim=1, keepdim=True))  # √det(g)\n",
    "        \n",
    "        # Evaluate f\n",
    "        f = f_net(x).squeeze(-1)  # (batch,)\n",
    "        \n",
    "        # First derivatives ∂f/∂x_i\n",
    "        grad_f = torch.autograd.grad(f.sum(), x, create_graph=True)[0]  # (batch, dim)\n",
    "        \n",
    "        # Compute Laplacian term by term\n",
    "        laplacian = torch.zeros(batch_size, device=x.device)\n",
    "        \n",
    "        for i in range(dim):\n",
    "            # Coefficient: √g × g^{ii} = √g / g_{ii}\n",
    "            coeff = sqrt_det_g.squeeze() * g_inv[:, i]\n",
    "            \n",
    "            # ∂f/∂x_i\n",
    "            df_dxi = grad_f[:, i]\n",
    "            \n",
    "            # coeff × ∂f/∂x_i\n",
    "            flux = coeff * df_dxi\n",
    "            \n",
    "            # ∂/∂x_i (coeff × ∂f/∂x_i)\n",
    "            div_flux = torch.autograd.grad(flux.sum(), x, create_graph=True)[0][:, i]\n",
    "            \n",
    "            # Add to Laplacian: (1/√g) × ∂_i(√g g^{ii} ∂_i f)\n",
    "            laplacian = laplacian + div_flux / (sqrt_det_g.squeeze() + 1e-10)\n",
    "        \n",
    "        return laplacian\n",
    "\n",
    "# Test on flat metric (should recover standard Laplacian)\n",
    "def flat_metric(x):\n",
    "    \"\"\"Flat metric: g_ii = 1 for all i.\"\"\"\n",
    "    return torch.ones_like(x)\n",
    "\n",
    "# Simple test\n",
    "class TestFunc(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # f(x) = x₁² + x₂² => Δf = 2 + 2 = 4 (in 2D)\n",
    "        return (x[:, 0]**2 + x[:, 1]**2).unsqueeze(-1)\n",
    "\n",
    "x_test = torch.randn(100, 2, device=device)\n",
    "lap = CurvedLaplacian(flat_metric)\n",
    "test_f = TestFunc().to(device)\n",
    "result = lap(test_f, x_test)\n",
    "print(f\"Test: Δ(x² + y²) = {result.mean().item():.4f} (expected: 4.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. G₂ Metrics\n",
    "\n",
    "We implement several G₂ metric models:\n",
    "1. **Round S⁷** (calibration): λ₁ = 7 exactly\n",
    "2. **Flat T⁷** (calibration): λ₁ = (2π)² ≈ 39.48 for unit torus\n",
    "3. **Bryant-Salamon** (G₂ holonomy, non-compact)\n",
    "4. **TCS model** (compact G₂ with neck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalMetric:\n",
    "    \"\"\"\n",
    "    Round metric on S^n (stereographic coordinates).\n",
    "    \n",
    "    For S^n of radius R: λ₁ = n/R²\n",
    "    For unit S⁷: λ₁ = 7\n",
    "    \n",
    "    In stereographic coords from north pole:\n",
    "    ds² = (2R/(1 + |x|²/R²))² × (dx₁² + ... + dx_n²)\n",
    "    \n",
    "    Conformal factor: Ω = 2R/(1 + |x|²/R²) = 2R²/(R² + |x|²)\n",
    "    Metric: g_ij = Ω² δ_ij (conformally flat)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int = 7, radius: float = 1.0):\n",
    "        self.dim = dim\n",
    "        self.R = radius\n",
    "        self.lambda_1_exact = dim / radius**2\n",
    "        \n",
    "    def metric(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Diagonal metric components (all equal for conformally flat).\"\"\"\n",
    "        r_sq = torch.sum(x**2, dim=1, keepdim=True)  # |x|²\n",
    "        Omega_sq = (2 * self.R**2 / (self.R**2 + r_sq))**2  # Conformal factor squared\n",
    "        return Omega_sq.expand(-1, self.dim)  # g_ii = Ω² for all i\n",
    "    \n",
    "    def sqrt_det_g(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"√det(g) = Ω^n.\"\"\"\n",
    "        r_sq = torch.sum(x**2, dim=1)\n",
    "        Omega = 2 * self.R**2 / (self.R**2 + r_sq)\n",
    "        return Omega**self.dim\n",
    "    \n",
    "    def volume(self) -> float:\n",
    "        \"\"\"Volume of S^n with radius R.\"\"\"\n",
    "        # Vol(S^n) = 2π^((n+1)/2) R^n / Γ((n+1)/2)\n",
    "        from scipy.special import gamma\n",
    "        n = self.dim\n",
    "        return 2 * np.pi**((n+1)/2) * self.R**n / gamma((n+1)/2)\n",
    "    \n",
    "    def sample(self, n_points: int, max_r: float = 10.0) -> torch.Tensor:\n",
    "        \"\"\"Sample points in stereographic coords (covers most of sphere).\"\"\"\n",
    "        # Sample with density proportional to √det(g)\n",
    "        # For uniform sampling on sphere, need importance sampling\n",
    "        x = torch.randn(n_points, self.dim, device=device)\n",
    "        r = torch.norm(x, dim=1, keepdim=True)\n",
    "        # Rejection sampling to get uniform on sphere\n",
    "        # For simplicity, just sample in ball and reweight\n",
    "        scale = max_r * torch.rand(n_points, 1, device=device)**(1/self.dim)\n",
    "        return x / r * scale\n",
    "\n",
    "\n",
    "class TorusMetric:\n",
    "    \"\"\"\n",
    "    Flat metric on T^n = (S¹)^n with periods L_i.\n",
    "    \n",
    "    For unit torus (L=1): λ₁ = (2π)² ≈ 39.48\n",
    "    Eigenfunction: cos(2πx_i) or sin(2πx_i)\n",
    "    \n",
    "    For T^n with all L_i = L: Vol = L^n, λ₁ = (2π/L)²\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int = 7, periods: float = 1.0):\n",
    "        self.dim = dim\n",
    "        self.L = periods\n",
    "        self.lambda_1_exact = (2 * np.pi / self.L)**2\n",
    "        \n",
    "    def metric(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Flat metric: g_ii = 1.\"\"\"\n",
    "        return torch.ones(x.shape[0], self.dim, device=x.device)\n",
    "    \n",
    "    def sqrt_det_g(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.ones(x.shape[0], device=x.device)\n",
    "    \n",
    "    def volume(self) -> float:\n",
    "        return self.L**self.dim\n",
    "    \n",
    "    def sample(self, n_points: int) -> torch.Tensor:\n",
    "        \"\"\"Sample uniformly in [0, L]^n.\"\"\"\n",
    "        return self.L * torch.rand(n_points, self.dim, device=device)\n",
    "\n",
    "\n",
    "class G2TCSMetric:\n",
    "    \"\"\"\n",
    "    G₂ metric model for Twisted Connected Sum (TCS) manifolds.\n",
    "    \n",
    "    Structure: M = (X₊ × S¹) ∪_neck (X₋ × S¹)\n",
    "    \n",
    "    Coordinates: (t, y₁, ..., y₆) where t ∈ [0, T] is neck direction\n",
    "    \n",
    "    Metric ansatz:\n",
    "    ds² = dt² + h(t)² ds²_{K3×S¹}\n",
    "    \n",
    "    where h(t) = cosh(t/T₀) for gluing region.\n",
    "    \n",
    "    The G₂ structure comes from the Calabi-Yau structure on K3×S¹.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, b2: int, b3: int, T: float = None, vol_normalized: bool = True):\n",
    "        self.b2 = b2\n",
    "        self.b3 = b3\n",
    "        self.H_star = b2 + b3 + 1\n",
    "        self.dim = 7\n",
    "        \n",
    "        # Neck length from Mayer-Vietoris\n",
    "        if T is None:\n",
    "            self.T = np.sqrt(self.H_star)\n",
    "        else:\n",
    "            self.T = T\n",
    "            \n",
    "        self.vol_normalized = vol_normalized\n",
    "        \n",
    "        # Characteristic scale for metric variation\n",
    "        self.T0 = self.T / 3  # Controls neck profile\n",
    "        \n",
    "        # Volume normalization factor\n",
    "        if vol_normalized:\n",
    "            self._compute_vol_normalization()\n",
    "        else:\n",
    "            self.vol_factor = 1.0\n",
    "    \n",
    "    def _compute_vol_normalization(self, n_samples: int = 100000):\n",
    "        \"\"\"Compute factor to normalize Vol = 1.\"\"\"\n",
    "        x = self.sample_raw(n_samples)\n",
    "        sqrt_g = self.sqrt_det_g_raw(x)\n",
    "        vol_raw = torch.mean(sqrt_g).item() * (self.T * 1.0**6)  # Domain volume\n",
    "        # We want Vol = 1, so scale metric by vol_raw^(-2/7)\n",
    "        self.vol_factor = vol_raw**(-1/7)  # Scale each g_ii by this\n",
    "        \n",
    "    def h(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Neck profile function.\"\"\"\n",
    "        # Smooth interpolation: h=1 at center, h grows at ends\n",
    "        return torch.cosh((t - self.T/2) / self.T0)\n",
    "    \n",
    "    def metric(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Diagonal metric: g = diag(1, h², h², h², h², h², h²)\n",
    "        \n",
    "        First coord is neck direction t, others are cross-section.\n",
    "        \"\"\"\n",
    "        t = x[:, 0]  # Neck coordinate\n",
    "        h_val = self.h(t)\n",
    "        \n",
    "        g = torch.ones(x.shape[0], 7, device=x.device)\n",
    "        g[:, 0] = 1.0  # dt²\n",
    "        g[:, 1:] = (h_val**2).unsqueeze(1).expand(-1, 6)  # h²(t) × (K3×S¹ metric)\n",
    "        \n",
    "        # Apply volume normalization\n",
    "        g = g * self.vol_factor**2\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def sqrt_det_g_raw(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"√det(g) without normalization.\"\"\"\n",
    "        t = x[:, 0]\n",
    "        h_val = self.h(t)\n",
    "        return h_val**6  # √(1 × h² × h² × h² × h² × h² × h²) = h⁶\n",
    "    \n",
    "    def sqrt_det_g(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"√det(g) with normalization.\"\"\"\n",
    "        return self.sqrt_det_g_raw(x) * self.vol_factor**7\n",
    "    \n",
    "    def sample_raw(self, n_points: int) -> torch.Tensor:\n",
    "        \"\"\"Sample uniformly in [0,T] × [0,1]⁶.\"\"\"\n",
    "        x = torch.rand(n_points, 7, device=device)\n",
    "        x[:, 0] *= self.T  # t ∈ [0, T]\n",
    "        return x\n",
    "    \n",
    "    def sample(self, n_points: int) -> torch.Tensor:\n",
    "        \"\"\"Sample with density proportional to √det(g).\"\"\"\n",
    "        # Importance sampling for better coverage\n",
    "        x = self.sample_raw(n_points * 2)\n",
    "        weights = self.sqrt_det_g(x)\n",
    "        weights = weights / weights.sum()\n",
    "        indices = torch.multinomial(weights, n_points, replacement=True)\n",
    "        return x[indices]\n",
    "    \n",
    "    def volume(self) -> float:\n",
    "        \"\"\"Total volume (should be ~1 if normalized).\"\"\"\n",
    "        x = self.sample_raw(100000)\n",
    "        sqrt_g = self.sqrt_det_g(x)\n",
    "        domain_vol = self.T * 1.0**6\n",
    "        return torch.mean(sqrt_g).item() * domain_vol\n",
    "    \n",
    "    def lambda_1_predicted(self) -> float:\n",
    "        \"\"\"GIFT prediction for Vol=1 normalized metric.\"\"\"\n",
    "        return 14.0 / self.H_star\n",
    "\n",
    "\n",
    "# Test metrics\n",
    "print(\"Metric Tests:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "s7 = SphericalMetric(dim=7, radius=1.0)\n",
    "print(f\"S⁷: λ₁(exact) = {s7.lambda_1_exact}, Vol = {s7.volume():.4f}\")\n",
    "\n",
    "t7 = TorusMetric(dim=7, periods=1.0)\n",
    "print(f\"T⁷: λ₁(exact) = {t7.lambda_1_exact:.4f}, Vol = {t7.volume():.4f}\")\n",
    "\n",
    "g2_k7 = G2TCSMetric(b2=21, b3=77, vol_normalized=True)\n",
    "print(f\"K₇: H* = {g2_k7.H_star}, T = {g2_k7.T:.2f}, Vol = {g2_k7.volume():.4f}, λ₁(pred) = {g2_k7.lambda_1_predicted():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Eigenfunction Network\n",
    "\n",
    "A more powerful architecture for finding eigenfunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EigenfunctionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep network for eigenfunction approximation.\n",
    "    \n",
    "    Features:\n",
    "    - Residual connections for stable training\n",
    "    - Fourier feature encoding for high-frequency details\n",
    "    - Learnable eigenvalue\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 7, hidden_dim: int = 256, \n",
    "                 n_layers: int = 8, fourier_features: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.fourier_features = fourier_features\n",
    "        \n",
    "        # Fourier feature frequencies (random but fixed)\n",
    "        self.register_buffer('B', torch.randn(input_dim, fourier_features) * 2.0)\n",
    "        \n",
    "        # Input dimension after Fourier encoding\n",
    "        encoded_dim = input_dim + 2 * fourier_features\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(encoded_dim, hidden_dim)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.res_blocks.append(nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            ))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Learnable eigenvalue (log for positivity)\n",
    "        self.log_lambda = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight, gain=0.5)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def fourier_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Fourier feature encoding for better high-frequency representation.\"\"\"\n",
    "        # x @ B gives projections onto random frequencies\n",
    "        proj = x @ self.B  # (batch, fourier_features)\n",
    "        return torch.cat([x, torch.sin(proj), torch.cos(proj)], dim=-1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Fourier encoding\n",
    "        h = self.fourier_encode(x)\n",
    "        \n",
    "        # Input layer\n",
    "        h = F.gelu(self.input_layer(h))\n",
    "        \n",
    "        # Residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            h = h + 0.1 * block(h)  # Scaled residual\n",
    "        \n",
    "        # Output\n",
    "        return self.output_layer(h)\n",
    "    \n",
    "    def get_lambda(self) -> float:\n",
    "        return torch.exp(self.log_lambda).item()\n",
    "    \n",
    "    def set_lambda(self, value: float):\n",
    "        with torch.no_grad():\n",
    "            self.log_lambda.copy_(torch.tensor(np.log(value)))\n",
    "\n",
    "# Test\n",
    "net = EigenfunctionNet(input_dim=7).to(device)\n",
    "x_test = torch.randn(100, 7, device=device)\n",
    "y = net(x_test)\n",
    "print(f\"Network output shape: {y.shape}\")\n",
    "print(f\"Initial λ: {net.get_lambda():.4f}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eigenvalue Solver\n",
    "\n",
    "Solve $\\Delta_g f = -\\lambda f$ with $\\int f = 0$ (orthogonal to constants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralSolver:\n",
    "    \"\"\"\n",
    "    Solve eigenvalue problem Δ_g f = -λ f via PINN.\n",
    "    \n",
    "    Loss = ||Δf + λf||² + α||∫f||² + β(||f||² - 1)²\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_obj, n_points: int = 10000):\n",
    "        self.metric_obj = metric_obj\n",
    "        self.n_points = n_points\n",
    "        self.laplacian = CurvedLaplacian(metric_obj.metric)\n",
    "        \n",
    "    def compute_loss(self, net: EigenfunctionNet, x: torch.Tensor) -> dict:\n",
    "        \"\"\"Compute all loss components.\"\"\"\n",
    "        # Get √det(g) for integration weights\n",
    "        sqrt_g = self.metric_obj.sqrt_det_g(x)\n",
    "        \n",
    "        # Evaluate f\n",
    "        f = net(x).squeeze(-1)\n",
    "        lam = torch.exp(net.log_lambda)\n",
    "        \n",
    "        # Compute Laplacian\n",
    "        lap_f = self.laplacian(net, x)\n",
    "        \n",
    "        # PDE loss: Δf = -λf (convention: positive λ for bound states)\n",
    "        pde_residual = lap_f + lam * f\n",
    "        loss_pde = torch.mean(pde_residual**2 * sqrt_g) / torch.mean(sqrt_g)\n",
    "        \n",
    "        # Orthogonality to constants: ∫f √g = 0\n",
    "        f_mean = torch.sum(f * sqrt_g) / torch.sum(sqrt_g)\n",
    "        loss_orth = f_mean**2\n",
    "        \n",
    "        # Normalization: ∫f² √g = 1\n",
    "        f_norm_sq = torch.sum(f**2 * sqrt_g) / torch.sum(sqrt_g)\n",
    "        loss_norm = (f_norm_sq - 1.0)**2\n",
    "        \n",
    "        return {\n",
    "            'pde': loss_pde,\n",
    "            'orth': loss_orth,\n",
    "            'norm': loss_norm,\n",
    "            'lambda': lam.item(),\n",
    "            'f_mean': f_mean.item(),\n",
    "            'f_norm': f_norm_sq.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, n_epochs: int = 5000, lr: float = 1e-3, \n",
    "              lambda_init: float = None, verbose: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Train to find first eigenvalue.\n",
    "        \n",
    "        Returns: dict with history and final eigenvalue\n",
    "        \"\"\"\n",
    "        # Initialize network\n",
    "        net = EigenfunctionNet(\n",
    "            input_dim=self.metric_obj.dim if hasattr(self.metric_obj, 'dim') else 7,\n",
    "            hidden_dim=256,\n",
    "            n_layers=8\n",
    "        ).to(device)\n",
    "        \n",
    "        if lambda_init is not None:\n",
    "            net.set_lambda(lambda_init)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=1000, T_mult=2\n",
    "        )\n",
    "        \n",
    "        # History\n",
    "        history = {'lambda': [], 'loss_pde': [], 'loss_total': []}\n",
    "        best_lambda = None\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Training loop\n",
    "        pbar = tqdm(range(n_epochs), desc=\"Training\", disable=not verbose)\n",
    "        for epoch in pbar:\n",
    "            # Sample new points each epoch\n",
    "            x = self.metric_obj.sample(self.n_points)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            losses = self.compute_loss(net, x)\n",
    "            \n",
    "            # Total loss with adaptive weights\n",
    "            loss_total = losses['pde'] + 100.0 * losses['orth'] + 10.0 * losses['norm']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_total.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Record\n",
    "            history['lambda'].append(losses['lambda'])\n",
    "            history['loss_pde'].append(losses['pde'].item())\n",
    "            history['loss_total'].append(loss_total.item())\n",
    "            \n",
    "            # Track best\n",
    "            if losses['pde'].item() < best_loss and losses['norm'] < 0.1:\n",
    "                best_loss = losses['pde'].item()\n",
    "                best_lambda = losses['lambda']\n",
    "            \n",
    "            # Update progress bar\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'λ': f\"{losses['lambda']:.4f}\",\n",
    "                    'PDE': f\"{losses['pde'].item():.2e}\",\n",
    "                    '|f|': f\"{losses['f_norm']:.2f}\"\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'lambda_1': best_lambda if best_lambda else history['lambda'][-1],\n",
    "            'history': history,\n",
    "            'net': net\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calibration: S⁷ and T⁷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CALIBRATION: Known Manifolds\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# S⁷ calibration\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"S⁷ (unit sphere): Expected λ₁ = 7\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "s7 = SphericalMetric(dim=7, radius=1.0)\n",
    "solver_s7 = SpectralSolver(s7, n_points=8000)\n",
    "result_s7 = solver_s7.train(n_epochs=3000, lr=5e-4, lambda_init=7.0)\n",
    "\n",
    "print(f\"\\nS⁷ Results:\")\n",
    "print(f\"  Computed λ₁ = {result_s7['lambda_1']:.4f}\")\n",
    "print(f\"  Exact λ₁    = {s7.lambda_1_exact:.4f}\")\n",
    "print(f\"  Error       = {abs(result_s7['lambda_1'] - s7.lambda_1_exact)/s7.lambda_1_exact*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T⁷ calibration\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"T⁷ (unit torus): Expected λ₁ = (2π)² ≈ 39.48\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "t7 = TorusMetric(dim=7, periods=1.0)\n",
    "solver_t7 = SpectralSolver(t7, n_points=8000)\n",
    "result_t7 = solver_t7.train(n_epochs=3000, lr=5e-4, lambda_init=39.0)\n",
    "\n",
    "print(f\"\\nT⁷ Results:\")\n",
    "print(f\"  Computed λ₁ = {result_t7['lambda_1']:.4f}\")\n",
    "print(f\"  Exact λ₁    = {t7.lambda_1_exact:.4f}\")\n",
    "print(f\"  Error       = {abs(result_t7['lambda_1'] - t7.lambda_1_exact)/t7.lambda_1_exact*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Experiment: G₂ Manifolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"G₂ MANIFOLDS: Testing GIFT Formula λ₁ = 14/H*\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "g2_manifolds = [\n",
    "    {\"name\": \"K₇ (GIFT)\", \"b2\": 21, \"b3\": 77},\n",
    "    {\"name\": \"Joyce J1\", \"b2\": 12, \"b3\": 43},\n",
    "    {\"name\": \"Joyce J4\", \"b2\": 0, \"b3\": 103},\n",
    "    {\"name\": \"Kovalev TCS\", \"b2\": 0, \"b3\": 71},\n",
    "]\n",
    "\n",
    "g2_results = []\n",
    "\n",
    "for m in g2_manifolds:\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"{m['name']}: b₂={m['b2']}, b₃={m['b3']}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    # Create metric with Vol=1 normalization\n",
    "    g2_metric = G2TCSMetric(m['b2'], m['b3'], vol_normalized=True)\n",
    "    H_star = g2_metric.H_star\n",
    "    lambda_pred = g2_metric.lambda_1_predicted()\n",
    "    \n",
    "    print(f\"H* = {H_star}, T = {g2_metric.T:.2f}\")\n",
    "    print(f\"Vol (normalized) = {g2_metric.volume():.4f}\")\n",
    "    print(f\"GIFT prediction: λ₁ = 14/{H_star} = {lambda_pred:.6f}\")\n",
    "    \n",
    "    # Solve eigenvalue problem\n",
    "    solver = SpectralSolver(g2_metric, n_points=10000)\n",
    "    result = solver.train(n_epochs=4000, lr=5e-4, lambda_init=lambda_pred)\n",
    "    \n",
    "    lambda_computed = result['lambda_1']\n",
    "    error = abs(lambda_computed - lambda_pred) / lambda_pred * 100\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Computed λ₁ = {lambda_computed:.6f}\")\n",
    "    print(f\"  Predicted   = {lambda_pred:.6f}\")\n",
    "    print(f\"  Error       = {error:.2f}%\")\n",
    "    \n",
    "    g2_results.append({\n",
    "        'name': m['name'],\n",
    "        'b2': m['b2'],\n",
    "        'b3': m['b3'],\n",
    "        'H_star': H_star,\n",
    "        'lambda_pred': lambda_pred,\n",
    "        'lambda_computed': lambda_computed,\n",
    "        'error': error,\n",
    "        'history': result['history']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: GIFT Spectral Gap Test (Vol = 1 normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Manifold':<15} {'b₂':>4} {'b₃':>4} {'H*':>5} {'λ₁(computed)':>14} {'14/H*':>10} {'Error':>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calibration results\n",
    "print(f\"{'S⁷ (calib)':<15} {'-':>4} {'-':>4} {'-':>5} {result_s7['lambda_1']:>14.4f} {'7.0000':>10} {abs(result_s7['lambda_1']-7)/7*100:>7.2f}%\")\n",
    "print(f\"{'T⁷ (calib)':<15} {'-':>4} {'-':>4} {'-':>5} {result_t7['lambda_1']:>14.4f} {'39.4784':>10} {abs(result_t7['lambda_1']-39.4784)/39.4784*100:>7.2f}%\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# G₂ results\n",
    "for r in g2_results:\n",
    "    print(f\"{r['name']:<15} {r['b2']:>4} {r['b3']:>4} {r['H_star']:>5} {r['lambda_computed']:>14.6f} {r['lambda_pred']:>10.6f} {r['error']:>7.2f}%\")\n",
    "\n",
    "avg_error = np.mean([r['error'] for r in g2_results])\n",
    "print(\"-\"*70)\n",
    "print(f\"Average G₂ error: {avg_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, r in zip(axes.flat, g2_results):\n",
    "    epochs = np.arange(len(r['history']['lambda']))\n",
    "    \n",
    "    # Lambda evolution\n",
    "    ax.plot(epochs, r['history']['lambda'], 'b-', alpha=0.7, label='λ(t)')\n",
    "    ax.axhline(y=r['lambda_pred'], color='r', linestyle='--', \n",
    "               label=f'14/H* = {r[\"lambda_pred\"]:.4f}')\n",
    "    \n",
    "    ax.set_title(f\"{r['name']} (H*={r['H_star']})\")\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('λ₁')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('g2_eigenvalue_convergence.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: g2_eigenvalue_convergence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Invariant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INVARIANT ANALYSIS: λ₁ × H* = ?\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nIf GIFT is correct: λ₁ × H* = 14 for all G₂ manifolds\")\n",
    "\n",
    "print(f\"\\n{'Manifold':<15} {'H*':>5} {'λ₁':>12} {'λ₁ × H*':>12} {'Target':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "products = []\n",
    "for r in g2_results:\n",
    "    product = r['lambda_computed'] * r['H_star']\n",
    "    products.append(product)\n",
    "    print(f\"{r['name']:<15} {r['H_star']:>5} {r['lambda_computed']:>12.6f} {product:>12.4f} {'14':>10}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Mean λ₁×H*':<15} {'':<5} {'':<12} {np.mean(products):>12.4f} {'14':>10}\")\n",
    "print(f\"{'Std dev':<15} {'':<5} {'':<12} {np.std(products):>12.4f}\")\n",
    "\n",
    "# Is it 14?\n",
    "deviation = abs(np.mean(products) - 14) / 14 * 100\n",
    "print(f\"\\nDeviation from 14: {deviation:.2f}%\")\n",
    "\n",
    "if deviation < 10:\n",
    "    print(\"\\n✓ Results consistent with GIFT formula λ₁ = 14/H*\")\n",
    "else:\n",
    "    print(f\"\\n✗ Results suggest λ₁ × H* ≈ {np.mean(products):.2f}, not 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════╗\n",
    "║                    GIFT SPECTRAL GAP: RIGOROUS TEST                      ║\n",
    "╠══════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                          ║\n",
    "║  Method: PINN with curved Laplacian Δ_g on G₂ TCS metric model          ║\n",
    "║  Normalization: Vol(g) = 1 (canonical metric)                            ║\n",
    "║                                                                          ║\n",
    "║  Calibration:                                                            ║\n",
    "\"\"\")\n",
    "print(f\"║    S⁷: λ₁ = {result_s7['lambda_1']:.4f} (exact: 7.0000)                                    ║\")\n",
    "print(f\"║    T⁷: λ₁ = {result_t7['lambda_1']:.4f} (exact: 39.4784)                                  ║\")\n",
    "print(\"║                                                                          ║\")\n",
    "print(\"║  G₂ Manifolds:                                                           ║\")\n",
    "for r in g2_results:\n",
    "    check = \"✓\" if r['error'] < 20 else \"?\"\n",
    "    print(f\"║    {r['name']:<12}: λ₁ = {r['lambda_computed']:.4f}, 14/H* = {r['lambda_pred']:.4f} ({r['error']:.1f}%) {check}  ║\")\n",
    "print(\"║                                                                          ║\")\n",
    "print(f\"║  Mean λ₁ × H* = {np.mean(products):.2f} (GIFT predicts 14)                              ║\")\n",
    "print(\"║                                                                          ║\")\n",
    "print(\"╚══════════════════════════════════════════════════════════════════════════╝\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
