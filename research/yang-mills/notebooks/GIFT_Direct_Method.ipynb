{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GIFT Direct Eigenvalue Computation (Optimized)\n\n**Méthode directe optimisée** : Pour une métrique séparable g = g₁×g₂×...×g₇\n\nLes valeurs propres 7D sont des **sommes** de valeurs propres 1D:\n- λ_total = λ₁⁽¹⁾ + λ₂⁽²⁾ + ... + λ₇⁽⁷⁾\n- Premier état excité: λ₁(7D) = min_d λ₁(L_d)\n\nOn résout 7 petits problèmes 1D au lieu d'un énorme problème 7D!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import eigsh\nimport matplotlib.pyplot as plt\nprint('Ready!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_laplacian_1d(n, L, g_func):\n    \"\"\"\n    Build 1D curved Laplacian on periodic domain [0, L]\n    \n    Δ_g f = (1/√g) d/dx (√g/g × df/dx) = (1/√g) d/dx (1/√g × df/dx)\n    \n    For metric g(x), this is a negative semi-definite operator.\n    \"\"\"\n    h = L / n\n    x = np.linspace(0, L, n, endpoint=False)\n    g = g_func(x)\n    sqrt_g = np.sqrt(g)\n    \n    # Coefficients at half-points\n    A_plus = 2.0 / (sqrt_g + np.roll(sqrt_g, -1))   # A at x + h/2\n    A_minus = 2.0 / (sqrt_g + np.roll(sqrt_g, 1))   # A at x - h/2\n    \n    # Sparse tridiagonal with periodic wrapping\n    diag_main = -(A_plus + A_minus) / h**2\n    diag_upper = A_plus / h**2\n    diag_lower = A_minus / h**2\n    \n    # Build sparse matrix\n    L_mat = diags([diag_main, diag_upper[:-1], diag_lower[1:], \n                   [diag_lower[0]], [diag_upper[-1]]],\n                  [0, 1, -1, n-1, -(n-1)], format='csr')\n    return L_mat, x, g\n\ndef get_lambda1_1d(n, L, g_func):\n    \"\"\"Get first non-zero eigenvalue of 1D Laplacian\"\"\"\n    L_mat, _, _ = build_laplacian_1d(n, L, g_func)\n    # Eigenvalues are negative; we want the smallest |λ| > 0\n    evals, _ = eigsh(L_mat, k=3, which='LM', sigma=0)\n    evals = np.sort(evals)  # Most negative to least negative\n    # λ₀ ≈ 0, λ₁ is first non-zero (most negative after 0)\n    return -evals[-2]  # Return positive value\n\ndef get_lambda1_7d_separable(n, L, g_funcs):\n    \"\"\"\n    For separable metric, λ₁(7D) = min over d of λ₁(1D, direction d)\n    \n    This is because the first excited state has one direction excited\n    and all others in ground state (λ=0).\n    \"\"\"\n    lambda1_list = []\n    for d, g_func in enumerate(g_funcs):\n        lam = get_lambda1_1d(n, L, g_func)\n        lambda1_list.append(lam)\n    return min(lambda1_list), lambda1_list"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validate 1D Laplacian on flat torus\nprint('='*60)\nprint('VALIDATION: 1D Flat Laplacian')\nprint('On [0, 2π] with g=1: λ₁ = 1 (from sin(x), cos(x))')\nprint('='*60)\n\ndef flat_metric(x):\n    return np.ones_like(x)\n\nfor n in [50, 100, 200, 500]:\n    lam = get_lambda1_1d(n, 2*np.pi, flat_metric)\n    print(f'n={n:4d}: λ₁ = {lam:.8f}, error = {abs(lam-1)*100:.4f}%')\n\nprint('\\n=> 1D solver is accurate!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 1: Flat torus T^7 (calibration)\nprint('='*60)\nprint('TEST 1: Flat Torus T^7 (calibration)')\nprint('Expected: λ₁ = 1.0 (from e^{ix_j})')\nprint('='*60)\n\nn = 200  # Fine grid for 1D (fast!)\nL = 2 * np.pi\ng_funcs = [flat_metric] * 7\n\nlambda_1, lam_list = get_lambda1_7d_separable(n, L, g_funcs)\nprint(f'\\nλ₁(1D) for each direction: {[f\"{l:.6f}\" for l in lam_list]}')\nprint(f'λ₁(7D) = min = {lambda_1:.6f}')\nprint(f'Expected: 1.0')\nprint(f'Error: {abs(lambda_1-1)*100:.4f}%')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 2: Scaled torus (sanity check)\n# If g = c², then λ₁ = 1/c² (metric scales eigenvalues)\n# For c² = H*/14, we get λ₁ = 14/H*\nprint('='*60)\nprint('TEST 2: Scaled Torus (sanity check)')\nprint('g = H*/14 => λ₁ = 14/H*')\nprint('='*60)\n\nn = 200\nL = 2 * np.pi\n\nfor H_star in [56, 72, 99, 104]:\n    c_sq = H_star / 14.0\n    \n    def scaled_metric(x, c2=c_sq):\n        return c2 * np.ones_like(x)\n    \n    g_funcs = [lambda x, c2=c_sq: c2 * np.ones_like(x)] * 7\n    lambda_1, _ = get_lambda1_7d_separable(n, L, g_funcs)\n    \n    expected = 14.0 / H_star\n    error = abs(lambda_1 - expected) / expected * 100\n    print(f'H*={H_star:3d}: λ₁={lambda_1:.6f}, 14/H*={expected:.6f}, error={error:.4f}%')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 3: TCS-like metric (the REAL test)\n# Neck direction: g(t) = cosh²((t-T/2)/T₀) where T ~ √H*, T₀ = T/3\n# Other directions: flat\n# This is a VARIABLE metric - NOT designed to give 14/H*\nprint('='*60)\nprint('TEST 3: TCS Metric (variable, the REAL test)')\nprint('Neck: g(t) = cosh²((t-T/2)/T₀), T=√H*, T₀=T/3')\nprint('='*60)\n\nn = 500  # High resolution for accuracy\nresults_tcs = []\n\nfor name, b2, b3 in [('K7', 21, 77), ('J1', 12, 43), ('J4', 0, 103), ('Kov', 0, 71)]:\n    H_star = b2 + b3 + 1\n    T = np.sqrt(H_star)\n    T0 = T / 3\n    \n    # Neck metric: g(t) = cosh²((t-T/2)/T₀)\n    # Domain is [0, T], but we use [0, 2π] and scale\n    L_neck = T\n    \n    def neck_metric(x, T=T, T0=T0):\n        \"\"\"TCS neck profile on domain [0, T]\"\"\"\n        return np.cosh((x - T/2) / T0)**2\n    \n    # Compute λ₁ for neck direction (domain [0, T])\n    lambda_neck = get_lambda1_1d(n, L_neck, neck_metric)\n    \n    # Flat directions have λ₁ = 1 on [0, 2π]\n    # But if we normalize all to same domain T, flat gives λ₁ = (2π/T)²\n    # So neck eigenvalue competes with flat eigenvalues\n    \n    # For separable TCS: λ₁(7D) = min(λ_neck, λ_flat)\n    # where λ_flat = (2π/T)² for each flat direction on domain T\n    L_flat = 2 * np.pi\n    lambda_flat = get_lambda1_1d(n, L_flat, flat_metric)\n    \n    lambda_1 = min(lambda_neck, lambda_flat)\n    \n    expected = 14.0 / H_star\n    product = lambda_1 * H_star\n    \n    print(f'{name}: H*={H_star:3d}, λ_neck={lambda_neck:.6f}, λ_flat={lambda_flat:.6f}')\n    print(f'       λ₁(7D)={lambda_1:.6f}, λ₁×H*={product:.2f} (target: 14)')\n    print()\n    results_tcs.append((name, H_star, lambda_1, lambda_neck, lambda_flat, product))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary table\nprint('='*60)\nprint('SUMMARY: TCS Metric Results')\nprint('='*60)\nprint(f\"{'Manifold':<8} {'H*':>4} {'λ_neck':>10} {'λ_flat':>10} {'λ₁×H*':>10}\")\nprint('-'*50)\nproducts = []\nfor name, H_star, lam1, lam_n, lam_f, prod in results_tcs:\n    winner = 'neck' if lam_n < lam_f else 'flat'\n    print(f'{name:<8} {H_star:>4} {lam_n:>10.4f} {lam_f:>10.4f} {prod:>10.2f} ({winner})')\n    products.append(prod)\nprint('-'*50)\nprint(f\"{'Mean':<8} {'':<4} {'':<10} {'':<10} {np.mean(products):>10.2f}\")\nprint(f\"{'Std':<8} {'':<4} {'':<10} {'':<10} {np.std(products):>10.2f}\")\nprint(f'\\nTarget: λ₁ × H* = 14')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyse and conclusion\nprint('='*60)\nprint('ANALYSIS')\nprint('='*60)\n\nmean_prod = np.mean(products)\nprint(f'\\nMean λ₁ × H* = {mean_prod:.2f}')\nprint(f'GIFT prediction = 14.00')\nprint(f'Deviation = {abs(mean_prod - 14)/14*100:.1f}%')\n\n# Interpretation\nprint('\\n--- INTERPRETATION ---')\nprint('''\nLe modèle TCS séparable (neck + tore plat) ne capture pas la \ngéométrie G₂ complète. Les valeurs propres dépendent de :\n1. La longueur du neck T ~ √H*\n2. Le profil métrique cosh² dans le neck  \n3. La taille du tore transverse\n\nPour tester λ₁ × H* = 14, il faudrait :\n- Une métrique G₂ COMPLÈTE (non séparable)\n- Le bon ratio entre neck et sections transverses\n- Possiblement la métrique de Ricci-flat exacte\n''')\n\n# Key insight\nprint('--- KEY INSIGHT ---')\nprint('λ_neck dépend du profil cosh² sur [0, √H*]')\nprint('λ_flat = 1 (constant)')\nprint('Le minimum détermine λ₁(7D)')\nprint()\nprint('Pour que λ₁×H* = 14, il faut λ₁ = 14/H*')\nprint('Avec H*=99: λ₁ ≈ 0.1414')\nprint('Le neck profile doit être ajusté pour donner cette valeur exacte.')"
  },
  {
   "cell_type": "code",
   "source": "# BONUS: Find neck length that gives λ₁ × H* = 14\nprint('='*60)\nprint('BONUS: Reverse engineering - what T gives λ₁×H* = 14?')\nprint('='*60)\n\nfrom scipy.optimize import brentq\n\ndef lambda_neck_for_T(T, H_star, n=500):\n    \"\"\"Compute λ_neck for given neck length T\"\"\"\n    T0 = T / 3\n    def neck_metric(x):\n        return np.cosh((x - T/2) / T0)**2\n    return get_lambda1_1d(n, T, neck_metric)\n\nfor name, b2, b3 in [('K7', 21, 77), ('J1', 12, 43)]:\n    H_star = b2 + b3 + 1\n    target_lambda = 14.0 / H_star\n    \n    # Find T such that λ_neck(T) = 14/H*\n    def objective(T):\n        return lambda_neck_for_T(T, H_star) - target_lambda\n    \n    # Search in range [1, 50]\n    try:\n        T_opt = brentq(objective, 0.5, 50)\n        T_theory = np.sqrt(H_star)  # GIFT T = √H*\n        ratio = T_opt / T_theory\n        print(f'{name} (H*={H_star}): T_optimal = {T_opt:.4f}, T_GIFT = {T_theory:.4f}, ratio = {ratio:.4f}')\n    except:\n        print(f'{name}: No solution found in range [0.5, 50]')\n\nprint('\\n=> Si ratio ≈ 1, le modèle TCS avec T=√H* donne naturellement λ₁×H*=14')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}