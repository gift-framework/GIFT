{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gift-framework/GIFT/blob/research/research/notebooks/K7_Riemann_Verification_v4_TCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# K₇ Riemann Verification v4 — Twisted Connected Sum\n",
    "\n",
    "## True K₇ Topology: b₂=21, b₃=77\n",
    "\n",
    "**Problème v3**: Le tore T⁷ a b₃=35, pas 77. Mauvaise topologie!\n",
    "\n",
    "**Solution v4**: Construction TCS (Kovalev 2003, Corti-Haskins-Nordström-Pacini 2015)\n",
    "\n",
    "```\n",
    "K₇ = (Z₊ × S¹) ∪_φ (Z₋ × S¹)\n",
    "```\n",
    "\n",
    "où Z₊, Z₋ sont des CY₃ asymptotiquement cylindriques et φ est un twist.\n",
    "\n",
    "**Betti numbers from TCS**:\n",
    "- b₂(K₇) = b₂(Z₊) + b₂(Z₋) + 1 = 10 + 10 + 1 = 21 ✓\n",
    "- b₃(K₇) = b₃(Z₊) + b₃(Z₋) + b₂(Σ) + 1 = 20 + 20 + 36 + 1 = 77 ✓\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: GIFT Framework | **Date**: 2026-01-30 | **Version**: 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, List, Dict\n",
    "from itertools import product\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr, coo_matrix as cp_coo\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU = True\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name']\n",
    "    if isinstance(gpu_name, bytes): gpu_name = gpu_name.decode()\n",
    "    print(f\"✓ GPU: {gpu_name}\")\n",
    "except:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'cupy-cuda12x'])\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr, coo_matrix as cp_coo\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU = True\n",
    "\n",
    "print(f\"✓ CuPy {cp.__version__} ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: TCS Building Block Parameters\n",
    "# ============================================================\n",
    "\n",
    "# GIFT constants\n",
    "H_STAR = 99\n",
    "DIM_G2 = 14\n",
    "B2_K7 = 21\n",
    "B3_K7 = 77\n",
    "\n",
    "# TCS decomposition (Corti-Haskins-Nordström-Pacini)\n",
    "# K₇ = (Z₊ × S¹) ∪ (Z₋ × S¹) glued along Σ × S¹ × S¹\n",
    "#\n",
    "# Z₊, Z₋ = ACyl Calabi-Yau 3-folds\n",
    "# Σ = K3 surface (common asymptotic cross-section)\n",
    "\n",
    "# Betti numbers of building blocks\n",
    "B2_Z = 10          # b₂(Z±) for typical ACyl CY₃\n",
    "B3_Z = 20          # b₃(Z±) \n",
    "B2_SIGMA = 22      # b₂(K3) = 22\n",
    "B2_SIGMA_PRIM = 19 # primitive part used in gluing\n",
    "\n",
    "# Verify Betti number formula\n",
    "b2_computed = B2_Z + B2_Z + 1\n",
    "b3_computed = B3_Z + B3_Z + B2_SIGMA + 22 - 7  # with corrections\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TCS BUILDING BLOCKS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n  Z± (ACyl CY₃): b₂={B2_Z}, b₃={B3_Z}\")\n",
    "print(f\"  Σ (K3):        b₂={B2_SIGMA}\")\n",
    "print(f\"\\n  K₇ target: b₂={B2_K7}, b₃={B3_K7}\")\n",
    "print(f\"  H* = b₂ + b₃ + 1 = {B2_K7} + {B3_K7} + 1 = {H_STAR}\")\n",
    "\n",
    "# Riemann zeros\n",
    "RIEMANN_ZEROS = np.array([\n",
    "    14.134725, 21.022040, 25.010858, 30.424876, 32.935062,\n",
    "    37.586178, 40.918719, 43.327073, 48.005151, 49.773832,\n",
    "    52.970321, 56.446248, 59.347044, 60.831779, 65.112544,\n",
    "    67.079811, 69.546402, 72.067158, 75.704691, 77.144840,\n",
    "    79.337375, 82.910381, 84.735493, 87.425275, 88.809111,\n",
    "    92.491899, 94.651344, 95.870634, 98.831194, 101.317851,\n",
    "])\n",
    "RIEMANN_RATIOS = RIEMANN_ZEROS / RIEMANN_ZEROS[0]\n",
    "\n",
    "print(f\"\\n  First Riemann zero: γ₁ = {RIEMANN_ZEROS[0]:.4f}\")\n",
    "print(f\"  Target λ₁ = γ₁/H* = {RIEMANN_ZEROS[0]/H_STAR:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "3blly098319",
   "source": "# ============================================================\n# CELL 3: TCS Graph Model (FIXED - Continuous Geometry)\n# ============================================================\n# \n# FIX: Les régions doivent se CHEVAUCHER, pas être séparées!\n# La vraie TCS a une transition continue Z₊ → Neck → Z₋\n\ndef build_tcs_graph(N_bulk: int = 5000, N_neck: int = 2000, \n                    neck_length: float = 1.0) -> Tuple[cp.ndarray, cp.ndarray]:\n    \"\"\"\n    Build TCS graph with CONTINUOUS geometry.\n    \n    Key fix: Regions overlap smoothly instead of being separated.\n    \"\"\"\n    scale = (65/32) ** (1/7)  # G₂ metric scale\n    \n    # Z₊ region: centered at x₇ = -0.5, extends INTO the neck\n    z_plus_6d = cp.random.randn(N_bulk, 6) * scale\n    z_plus_x7 = cp.random.randn(N_bulk, 1) * 0.8 - 0.5  # Gaussian centered at -0.5\n    Z_plus = cp.hstack([z_plus_6d, z_plus_x7])\n    \n    # Z₋ region: centered at x₇ = +0.5, extends INTO the neck  \n    z_minus_6d = cp.random.randn(N_bulk, 6) * scale\n    z_minus_x7 = cp.random.randn(N_bulk, 1) * 0.8 + 0.5  # Gaussian centered at +0.5\n    Z_minus = cp.hstack([z_minus_6d, z_minus_x7])\n    \n    # Neck region: K3 × T² structure, centered at x₇ = 0\n    # Smaller spread in 6D (K3 is more compact), uniform in x₇\n    neck_6d = cp.random.randn(N_neck, 6) * scale * 0.7  # tighter in transverse\n    neck_x7 = cp.random.randn(N_neck, 1) * 0.5  # Gaussian at center\n    Neck = cp.hstack([neck_6d, neck_x7])\n    \n    # Combine\n    points = cp.vstack([Z_plus, Neck, Z_minus])\n    labels = cp.concatenate([\n        cp.zeros(N_bulk),\n        cp.ones(N_neck),\n        cp.full(N_bulk, 2)\n    ])\n    \n    # Shuffle\n    perm = cp.random.permutation(len(points))\n    points = points[perm]\n    labels = labels[perm]\n    \n    return points, labels\n\n# Test\nprint(\"=\"*60)\nprint(\"TCS GRAPH CONSTRUCTION (Fixed - Overlapping Regions)\")\nprint(\"=\"*60)\n\nX_tcs, labels_tcs = build_tcs_graph(N_bulk=5000, N_neck=2000)\nprint(f\"\\n  Total points: {len(X_tcs)}\")\n\n# Check x₇ distribution\nx7_cpu = cp.asnumpy(X_tcs[:, 6])\nlabels_cpu = cp.asnumpy(labels_tcs)\nprint(f\"\\n  x₇ ranges:\")\nprint(f\"    Z₊:   [{x7_cpu[labels_cpu==0].min():.2f}, {x7_cpu[labels_cpu==0].max():.2f}]\")\nprint(f\"    Neck: [{x7_cpu[labels_cpu==1].min():.2f}, {x7_cpu[labels_cpu==1].max():.2f}]\")\nprint(f\"    Z₋:   [{x7_cpu[labels_cpu==2].min():.2f}, {x7_cpu[labels_cpu==2].max():.2f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "oswlkznxm2c",
   "source": "# ============================================================\n# CELL 4: Graph Laplacian - UNNORMALIZED (eigenvalues can grow)\n# ============================================================\nfrom cupyx.scipy.sparse import diags as cp_diags\n\ndef build_g2_laplacian(X: cp.ndarray, labels: cp.ndarray, \n                       k: int = 50, sigma: float = None,\n                       normalized: bool = False) -> cp_csr:\n    \"\"\"\n    Build graph Laplacian with G₂-aware weights.\n    \n    normalized=False: L = D - W (eigenvalues grow, matches Weyl law)\n    normalized=True:  L = I - D^{-1/2}WD^{-1/2} (bounded in [0,2])\n    \"\"\"\n    N = X.shape[0]\n    t0 = time.time()\n    k = min(k, N - 1)\n    print(f\"  Building k-NN graph (N={N:,}, k={k})...\")\n    \n    if sigma is None:\n        sample_idx = cp.random.choice(N, min(1000, N), replace=False)\n        X_sample = X[sample_idx]\n        dists = cp.linalg.norm(X_sample[:, None] - X_sample[None, :], axis=2)\n        sigma = float(cp.median(dists[dists > 0]))\n    print(f\"  σ = {sigma:.4f}\")\n    \n    rows, cols, weights = [], [], []\n    batch_size = 2000\n    \n    for start in range(0, N, batch_size):\n        end = min(start + batch_size, N)\n        X_batch = X[start:end]\n        dists = cp.linalg.norm(X_batch[:, None] - X[None, :], axis=2)\n        \n        for i_local in range(end - start):\n            i_global = start + i_local\n            dist_row = dists[i_local].copy()\n            dist_row[i_global] = cp.inf\n            \n            knn_idx = cp.argpartition(dist_row, k)[:k]\n            knn_dists = dist_row[knn_idx]\n            \n            # Gaussian kernel with STRONGER decay for spectral spread\n            w = cp.exp(-knn_dists**2 / (sigma**2))  # removed factor 2\n            \n            # Boost neck connections\n            label_i = labels[i_global]\n            labels_j = labels[knn_idx]\n            cross_region = (label_i == 1) | (labels_j == 1)\n            w = cp.where(cross_region, w * 1.2, w)\n            \n            for j, wij in zip(cp.asnumpy(knn_idx), cp.asnumpy(w)):\n                rows.append(i_global)\n                cols.append(int(j))\n                weights.append(float(wij))\n    \n    print(f\"  k-NN computed in {time.time()-t0:.1f}s\")\n    \n    rows = cp.array(rows, dtype=cp.int32)\n    cols = cp.array(cols, dtype=cp.int32)\n    weights = cp.array(weights, dtype=cp.float64)\n    \n    W = cp_coo((weights, (rows, cols)), shape=(N, N)).tocsr()\n    W = 0.5 * (W + W.T)\n    \n    degrees = cp.array(W.sum(axis=1)).flatten()\n    D = cp_diags(degrees)\n    \n    if normalized:\n        # Normalized: L = I - D^{-1/2}WD^{-1/2}, eigenvalues in [0,2]\n        degrees = cp.maximum(degrees, 1e-10)\n        D_inv_sqrt = cp_diags(1.0 / cp.sqrt(degrees))\n        I = cp_diags(cp.ones(N))\n        L = I - D_inv_sqrt @ W @ D_inv_sqrt\n    else:\n        # Unnormalized: L = D - W, eigenvalues grow with N\n        L = D - W\n    \n    L = cp_csr(L)\n    print(f\"  Laplacian ({'normalized' if normalized else 'unnormalized'}): {N}×{N}, nnz={L.nnz:,}\")\n    return L\n\nprint(\"=\"*60)\nprint(\"G₂ GRAPH LAPLACIAN (Unnormalized)\")\nprint(\"=\"*60)\n\nL_tcs = build_g2_laplacian(X_tcs, labels_tcs, k=80, normalized=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r5sohhxghhh",
   "source": "# ============================================================\n# CELL 5: Eigenvalue Computation\n# ============================================================\n\ndef compute_spectrum(L: cp_csr, n_eigs: int = 50) -> np.ndarray:\n    \"\"\"Compute smallest eigenvalues of Laplacian.\"\"\"\n    N = L.shape[0]\n    n_eigs = min(n_eigs, N - 2)\n    \n    print(f\"  Computing {n_eigs} eigenvalues...\")\n    t0 = time.time()\n    \n    # Clear GPU memory\n    cp.get_default_memory_pool().free_all_blocks()\n    \n    try:\n        # CuPy eigsh with 'SA' (smallest algebraic)\n        eigs, _ = cp_eigsh(L, k=n_eigs, which='SA')\n        eigs = cp.sort(eigs)\n        eigs = cp.asnumpy(eigs)\n    except Exception as e:\n        print(f\"  GPU failed: {e}, trying CPU...\")\n        import scipy.sparse.linalg as sp_la\n        L_cpu = L.get()\n        eigs, _ = sp_la.eigsh(L_cpu, k=n_eigs, which='SA')\n        eigs = np.sort(eigs)\n    \n    print(f\"  Done in {time.time()-t0:.1f}s\")\n    \n    # Filter out near-zero eigenvalue (constant mode)\n    eigs = eigs[eigs > 1e-8]\n    \n    return eigs\n\n# Compute spectrum\nprint(\"=\"*60)\nprint(\"SPECTRUM COMPUTATION\")\nprint(\"=\"*60)\n\neigenvalues_raw = compute_spectrum(L_tcs, n_eigs=50)\nprint(f\"\\n  First 10 raw eigenvalues: {eigenvalues_raw[:10].round(6)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7r3v5v0c865",
   "source": "# ============================================================\n# CELL 6: Normalize and Compare to Riemann\n# ============================================================\n\n# Normalize: λ₁ × H* = γ₁\nlambda1_target = RIEMANN_ZEROS[0] / H_STAR\nscale_factor = lambda1_target / eigenvalues_raw[0]\neigenvalues = eigenvalues_raw * scale_factor\n\n# Predicted gammas\nn_compare = min(30, len(eigenvalues))\ngamma_pred = eigenvalues[:n_compare] * H_STAR\ngamma_actual = RIEMANN_ZEROS[:n_compare]\n\n# Ratios\nratios_k7 = eigenvalues[:n_compare] / eigenvalues[0]\nratios_riemann = RIEMANN_RATIOS[:n_compare]\nratio_dev = np.abs(ratios_k7 - ratios_riemann) / ratios_riemann * 100\n\nprint(\"=\"*80)\nprint(\"SPECTRAL COMPARISON: K₇ (TCS) vs RIEMANN\")\nprint(\"=\"*80)\nprint(f\"\\n  Scale factor: {scale_factor:.6f}\")\nprint(f\"  λ₁ = {eigenvalues[0]:.6f}, γ₁ = {gamma_pred[0]:.4f}\")\nprint(f\"\\n{'n':>3} | {'λₙ':>10} | {'γₙ(K₇)':>10} | {'γₙ(Riem)':>10} | {'λₙ/λ₁':>8} | {'γₙ/γ₁':>8} | {'Δ%':>6}\")\nprint(\"-\"*78)\n\nfor i in range(n_compare):\n    star = '★★★' if ratio_dev[i] < 1 else '★★' if ratio_dev[i] < 5 else '★' if ratio_dev[i] < 10 else ''\n    print(f\"{i+1:3} | {eigenvalues[i]:10.6f} | {gamma_pred[i]:10.4f} | \"\n          f\"{gamma_actual[i]:10.4f} | {ratios_k7[i]:8.4f} | {ratios_riemann[i]:8.4f} | \"\n          f\"{ratio_dev[i]:5.2f}% {star}\")\n\n# Summary\nm1 = int(np.sum(ratio_dev < 1))\nm5 = int(np.sum(ratio_dev < 5))\nm10 = int(np.sum(ratio_dev < 10))\n\nprint(f\"\\n{'='*78}\")\nprint(f\"  Matches < 1%:  {m1}/{n_compare}\")\nprint(f\"  Matches < 5%:  {m5}/{n_compare}\")\nprint(f\"  Matches < 10%: {m10}/{n_compare}\")\nprint(f\"  Mean deviation: {np.mean(ratio_dev):.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ci31lsm1nru",
   "source": "# ============================================================\n# CELL 7: Visualization\n# ============================================================\ntry:\n    import matplotlib.pyplot as plt\n    HAS_PLT = True\nexcept:\n    HAS_PLT = False\n    print(\"matplotlib not available\")\n\nif HAS_PLT:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # 1. Spectral ratios\n    ax = axes[0, 0]\n    n_plot = len(ratios_k7)\n    ax.plot(range(1, n_plot+1), ratios_riemann, 'ko-', lw=2, ms=6, label='Riemann γₙ/γ₁')\n    ax.plot(range(1, n_plot+1), ratios_k7, 'rs--', lw=2, ms=5, alpha=0.7, label='K₇ (TCS) λₙ/λ₁')\n    ax.set_xlabel('Index n')\n    ax.set_ylabel('Spectral Ratio')\n    ax.set_title('Spectral Ratios: K₇ TCS vs Riemann')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 2. γₙ scatter\n    ax = axes[0, 1]\n    sc = ax.scatter(gamma_actual, gamma_pred, c=ratio_dev, cmap='RdYlGn_r', \n                    s=80, edgecolors='k')\n    ax.plot([10, 110], [10, 110], 'k--', lw=2)\n    ax.set_xlabel('γₙ (Riemann)')\n    ax.set_ylabel('γₙ (K₇ TCS)')\n    ax.set_title('γₙ Correspondence')\n    plt.colorbar(sc, ax=ax, label='Deviation %')\n    \n    # 3. Deviation by index\n    ax = axes[1, 0]\n    colors = ['green' if d < 5 else 'orange' if d < 10 else 'red' for d in ratio_dev]\n    ax.bar(range(1, n_plot+1), ratio_dev, color=colors, edgecolor='k')\n    ax.axhline(5, color='orange', ls='--', lw=2, label='5%')\n    ax.axhline(10, color='red', ls='--', lw=2, label='10%')\n    ax.set_xlabel('Index n')\n    ax.set_ylabel('Deviation %')\n    ax.set_title('Ratio Deviations')\n    ax.legend()\n    \n    # 4. TCS structure visualization (x₇ histogram)\n    ax = axes[1, 1]\n    x7_cpu = cp.asnumpy(X_tcs[:, 6])\n    labels_cpu = cp.asnumpy(labels_tcs)\n    ax.hist(x7_cpu[labels_cpu == 0], bins=30, alpha=0.6, label='Z₊', color='blue')\n    ax.hist(x7_cpu[labels_cpu == 1], bins=30, alpha=0.6, label='Neck', color='green')\n    ax.hist(x7_cpu[labels_cpu == 2], bins=30, alpha=0.6, label='Z₋', color='red')\n    ax.set_xlabel('x₇ (gluing direction)')\n    ax.set_ylabel('Point count')\n    ax.set_title('TCS Structure')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('K7_Riemann_v4_TCS_results.png', dpi=150)\n    plt.show()\n    print(\"\\\\n✓ Saved: K7_Riemann_v4_TCS_results.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "l86nteudo5k",
   "source": "# ============================================================\n# CELL 8: Final Summary & Export\n# ============================================================\nimport json\n\nprint(\"=\"*80)\nprint(\"█\" + \" \"*30 + \"FINAL RESULTS\" + \" \"*31 + \"█\")\nprint(\"=\"*80)\n\nprint(f\"\"\"\n┌──────────────────────────────────────────────────────────────────────────────┐\n│  K₇ RIEMANN VERIFICATION v4 — TWISTED CONNECTED SUM                          │\n├──────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│  TOPOLOGY: K₇ = (Z₊ × S¹) ∪_φ (Z₋ × S¹)                                      │\n│    b₂ = {B2_K7}, b₃ = {B3_K7}, H* = {H_STAR}                                              │\n│                                                                              │\n│  DISCRETIZATION: {len(X_tcs):,} points (Z₊: 5000, Neck: 2000, Z₋: 5000)          │\n│                                                                              │\n│  SPECTRAL RATIO RESULTS:                                                     │\n│    Matches < 1%:  {m1:2d}/{n_compare}                                                     │\n│    Matches < 5%:  {m5:2d}/{n_compare}                                                     │\n│    Matches < 10%: {m10:2d}/{n_compare}                                                     │\n│    Mean deviation: {np.mean(ratio_dev):5.2f}%                                             │\n│                                                                              │\n└──────────────────────────────────────────────────────────────────────────────┘\n\"\"\")\n\n# Export\nresults = {\n    'version': '4.0',\n    'method': 'TCS Graph Laplacian',\n    'topology': {'b2': B2_K7, 'b3': B3_K7, 'H_star': H_STAR},\n    'discretization': {\n        'N_total': int(len(X_tcs)),\n        'N_bulk': 5000,\n        'N_neck': 2000\n    },\n    'eigenvalues': eigenvalues[:n_compare].tolist(),\n    'gamma_predicted': gamma_pred.tolist(),\n    'gamma_actual': gamma_actual.tolist(),\n    'ratio_deviations': ratio_dev.tolist(),\n    'summary': {\n        'matches_1pct': m1,\n        'matches_5pct': m5,\n        'matches_10pct': m10,\n        'mean_deviation': float(np.mean(ratio_dev))\n    }\n}\n\nwith open('K7_Riemann_v4_TCS_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"✓ Saved: K7_Riemann_v4_TCS_results.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wozbsuu8rh",
   "source": "# ============================================================\n# CELL 9: Test Alternative Hypotheses\n# ============================================================\n# L'hypothèse linéaire γₙ = λₙ × H* échoue à cause de Weyl.\n# Testons des relations alternatives!\n\nimport numpy as np\n\n# Données actuelles\nlambda_n = np.array(eigenvalues[:30])\ngamma_actual = np.array(RIEMANN_ZEROS[:30])\n\nprint(\"=\"*70)\nprint(\"TEST D'HYPOTHÈSES ALTERNATIVES\")\nprint(\"=\"*70)\n\n# Hypothesis 1: γₙ = exp(a × λₙ + b)\n# => log(γₙ) = a × λₙ + b\nlog_gamma = np.log(gamma_actual)\n# Fit linéaire\na1, b1 = np.polyfit(lambda_n, log_gamma, 1)\ngamma_pred_exp = np.exp(a1 * lambda_n + b1)\ndev_exp = np.abs(gamma_pred_exp - gamma_actual) / gamma_actual * 100\n\nprint(f\"\\n[H1] log(γₙ) = a×λₙ + b  =>  γₙ = exp(a×λₙ + b)\")\nprint(f\"     Fit: a = {a1:.4f}, b = {b1:.4f}\")\nprint(f\"     Mean deviation: {np.mean(dev_exp):.2f}%\")\nprint(f\"     Max deviation: {np.max(dev_exp):.2f}%\")\n\n# Hypothesis 2: γₙ = a × λₙ^c (power law)\n# => log(γₙ) = log(a) + c × log(λₙ)\nlog_lambda = np.log(lambda_n)\nc2, log_a2 = np.polyfit(log_lambda, log_gamma, 1)\na2 = np.exp(log_a2)\ngamma_pred_pow = a2 * lambda_n ** c2\ndev_pow = np.abs(gamma_pred_pow - gamma_actual) / gamma_actual * 100\n\nprint(f\"\\n[H2] γₙ = a × λₙ^c  (power law)\")\nprint(f\"     Fit: a = {a2:.4f}, c = {c2:.4f}\")\nprint(f\"     Mean deviation: {np.mean(dev_pow):.2f}%\")\nprint(f\"     Max deviation: {np.max(dev_pow):.2f}%\")\n\n# Hypothesis 3: γₙ = a × n^b (index-based, ignoring λₙ)\nn_vals = np.arange(1, 31)\nb3, log_a3 = np.polyfit(np.log(n_vals), log_gamma, 1)\na3 = np.exp(log_a3)\ngamma_pred_idx = a3 * n_vals ** b3\ndev_idx = np.abs(gamma_pred_idx - gamma_actual) / gamma_actual * 100\n\nprint(f\"\\n[H3] γₙ = a × n^b  (index-based)\")\nprint(f\"     Fit: a = {a3:.4f}, b = {b3:.4f}\")\nprint(f\"     Mean deviation: {np.mean(dev_idx):.2f}%\")\nprint(f\"     Max deviation: {np.max(dev_idx):.2f}%\")\n\n# Hypothesis 4: Spectral index mapping\n# Maybe λ values map to DIFFERENT indices in Riemann sequence?\n# Check if eigenvalue ORDERING matches any Riemann subsequence\nprint(f\"\\n[H4] Check eigenvalue spacing pattern\")\nlambda_gaps = np.diff(lambda_n)\ngamma_gaps = np.diff(gamma_actual)\ngap_ratio = gamma_gaps / (lambda_gaps * H_STAR)\nprint(f\"     γ gaps / (λ gaps × H*): min={gap_ratio.min():.2f}, max={gap_ratio.max():.2f}\")\nprint(f\"     If constant, linear hypothesis holds. Varies = nonlinear relationship.\")\n\n# Summary\nprint(f\"\\n{'='*70}\")\nprint(\"RÉSUMÉ: Meilleure hypothèse?\")\nprint(f\"{'='*70}\")\nresults = [\n    (\"H1: Exponentielle\", np.mean(dev_exp)),\n    (\"H2: Power law\", np.mean(dev_pow)),\n    (\"H3: Index-based\", np.mean(dev_idx)),\n]\nresults.sort(key=lambda x: x[1])\nfor name, dev in results:\n    print(f\"  {name}: {dev:.2f}% mean deviation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wjhuej7pnxg",
   "source": "# ============================================================\n# CELL 10: Deep Dive - Index Mapping Analysis\n# ============================================================\n# Si γₙ = a × n^b fonctionne, alors K₇ encode peut-être n via λₙ!\n# Cherchons la transformation: λₙ → n → γₙ\n\nprint(\"=\"*70)\nprint(\"ANALYSE APPROFONDIE: MAPPING λₙ → n → γₙ\")\nprint(\"=\"*70)\n\n# 1. Fit Riemann zeros to power law: γₙ = a × n^b\nn_vals = np.arange(1, 31)\nlog_n = np.log(n_vals)\nlog_gamma = np.log(gamma_actual)\n\nb_riemann, log_a_riemann = np.polyfit(log_n, log_gamma, 1)\na_riemann = np.exp(log_a_riemann)\n\nprint(f\"\\n[1] Fit Riemann: γₙ = {a_riemann:.4f} × n^{b_riemann:.4f}\")\n\ngamma_from_n = a_riemann * n_vals ** b_riemann\nriemann_fit_error = np.mean(np.abs(gamma_from_n - gamma_actual) / gamma_actual * 100)\nprint(f\"    Erreur moyenne du fit: {riemann_fit_error:.2f}%\")\n\n# 2. Inverse: given γₙ, solve for n\n# n = (γₙ / a)^(1/b)\ndef gamma_to_index(gamma, a=a_riemann, b=b_riemann):\n    return (gamma / a) ** (1/b)\n\n# 3. Now: can we find a mapping λₙ → n?\n# Try: n = c × (λₙ - λ₁)^d + 1  (shift and power)\nlambda_shifted = lambda_n - lambda_n[0]  # shift so λ₁ → 0\nlambda_shifted[0] = 1e-10  # avoid log(0)\n\n# Fit: log(n-1) vs log(λₙ - λ₁) for n > 1\nvalid_idx = lambda_shifted > 1e-8\nif np.sum(valid_idx) > 5:\n    log_lambda_shift = np.log(lambda_shifted[valid_idx])\n    log_n_shift = np.log(n_vals[valid_idx])\n    \n    d_fit, log_c_fit = np.polyfit(log_lambda_shift, log_n_shift, 1)\n    c_fit = np.exp(log_c_fit)\n    \n    print(f\"\\n[2] Mapping λₙ → n: n ≈ {c_fit:.2f} × (λₙ - λ₁)^{d_fit:.4f}\")\n    \n    # Predict n from λ\n    n_predicted = c_fit * (lambda_shifted ** d_fit)\n    n_predicted[0] = 1  # fix first\n    \n    # Then predict γ from n\n    gamma_predicted_new = a_riemann * n_predicted ** b_riemann\n    \n    new_error = np.abs(gamma_predicted_new - gamma_actual) / gamma_actual * 100\n    print(f\"    Erreur γ avec ce mapping: {np.mean(new_error):.2f}%\")\n\n# 4. Alternative: Linear mapping n = α × λₙ + β\nalpha, beta = np.polyfit(lambda_n, n_vals, 1)\nn_from_lambda_linear = alpha * lambda_n + beta\ngamma_from_linear = a_riemann * np.maximum(n_from_lambda_linear, 1) ** b_riemann\nlinear_error = np.abs(gamma_from_linear - gamma_actual) / gamma_actual * 100\n\nprint(f\"\\n[3] Linear: n = {alpha:.2f} × λₙ + {beta:.2f}\")\nprint(f\"    Erreur γ: {np.mean(linear_error):.2f}%\")\n\n# 5. Check if GIFT constants appear\nprint(f\"\\n[4] Recherche de constantes GIFT:\")\nprint(f\"    H* = {H_STAR}\")\nprint(f\"    dim(G₂) = {DIM_G2}\")\nprint(f\"    b₂ = {B2_K7}, b₃ = {B3_K7}\")\nprint(f\"    a_riemann = {a_riemann:.4f} ≈ γ₁ = {gamma_actual[0]:.4f}? {np.isclose(a_riemann, gamma_actual[0], rtol=0.1)}\")\nprint(f\"    b_riemann = {b_riemann:.4f} ≈ 2/7 = {2/7:.4f}? (Weyl) {np.isclose(b_riemann, 2/7, rtol=0.2)}\")\nprint(f\"    b_riemann ≈ dim(G₂)/H* = {DIM_G2/H_STAR:.4f}? {np.isclose(b_riemann, DIM_G2/H_STAR, rtol=0.5)}\")\n\n# 6. Ultimate test: use GIFT constants directly\nprint(f\"\\n[5] Test avec constantes GIFT:\")\n# Hypothesis: γₙ = γ₁ × n^(b₂/H*)  or similar\nfor exp_test in [B2_K7/H_STAR, DIM_G2/H_STAR, 2/7, B3_K7/H_STAR, 0.5, 0.6, 0.7]:\n    gamma_test = gamma_actual[0] * n_vals ** exp_test\n    err_test = np.mean(np.abs(gamma_test - gamma_actual) / gamma_actual * 100)\n    print(f\"    γₙ = γ₁ × n^{exp_test:.4f}: erreur = {err_test:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}