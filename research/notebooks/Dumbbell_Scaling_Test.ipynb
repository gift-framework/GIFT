{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumbbell Manifold: λ₁ ~ 1/L² Scaling Validation\n",
    "\n",
    "**Goal**: Verify that for a dumbbell manifold (two balls connected by a tube of length L),\n",
    "the first non-zero eigenvalue scales as λ₁ ~ 1/L².\n",
    "\n",
    "This validates **Tier 1** of the spectral bounds proof before moving to Tier 2.\n",
    "\n",
    "**Prediction from Cheeger analysis**:\n",
    "- h(dumbbell) ~ Area(cross-section) / Vol ~ 1/L\n",
    "- λ₁ ≥ h²/4 ~ 1/L²\n",
    "- Rayleigh upper bound also gives λ₁ ≤ c/L²\n",
    "\n",
    "**Test**: Compute λ₁(L) for L ∈ {1, 2, 4, 8, 16} and verify λ₁ × L² ≈ constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Detection and Setup\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    HAS_GPU = 'NVIDIA' in result.stdout\n",
    "    if HAS_GPU:\n",
    "        print(\"GPU detected!\")\n",
    "        print(result.stdout.split('\\n')[8])  # GPU name line\n",
    "except:\n",
    "    HAS_GPU = False\n",
    "    print(\"No GPU - using CPU\")\n",
    "\n",
    "if HAS_GPU:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    xp = cp\n",
    "    print(\"Using CuPy\")\n",
    "else:\n",
    "    import numpy as cp  # fallback\n",
    "    from scipy.sparse import csr_matrix as cp_csr\n",
    "    from scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    xp = cp\n",
    "    print(\"Using NumPy/SciPy\")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dumbbell Point Cloud Generator\n",
    "\n",
    "Generate points on a dumbbell: two 3-balls connected by a cylinder of length L and radius r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_3ball(n_points, radius=1.0, center=np.array([0,0,0])):\n",
    "    \"\"\"Sample uniformly from a 3-ball (solid sphere).\"\"\"\n",
    "    # Use rejection sampling for uniform distribution\n",
    "    points = []\n",
    "    while len(points) < n_points:\n",
    "        batch = np.random.uniform(-radius, radius, (n_points * 2, 3))\n",
    "        norms = np.linalg.norm(batch, axis=1)\n",
    "        valid = batch[norms <= radius]\n",
    "        points.extend(valid[:n_points - len(points)])\n",
    "    return np.array(points) + center\n",
    "\n",
    "def sample_cylinder(n_points, length, radius, center_start=np.array([0,0,0])):\n",
    "    \"\"\"Sample uniformly from a cylinder along x-axis.\"\"\"\n",
    "    # x uniform in [0, length], (y,z) uniform in disk of radius r\n",
    "    x = np.random.uniform(0, length, n_points)\n",
    "    theta = np.random.uniform(0, 2*np.pi, n_points)\n",
    "    r = radius * np.sqrt(np.random.uniform(0, 1, n_points))  # sqrt for uniform in disk\n",
    "    y = r * np.cos(theta)\n",
    "    z = r * np.sin(theta)\n",
    "    points = np.column_stack([x, y, z]) + center_start\n",
    "    return points\n",
    "\n",
    "def sample_dumbbell(n_total, L, ball_radius=1.0, tube_radius=0.3):\n",
    "    \"\"\"\n",
    "    Sample from a dumbbell manifold:\n",
    "    - Ball 1 centered at origin\n",
    "    - Tube from x=ball_radius to x=ball_radius+L\n",
    "    - Ball 2 centered at x=2*ball_radius+L\n",
    "    \n",
    "    Points distributed proportionally to volume.\n",
    "    \"\"\"\n",
    "    # Volumes\n",
    "    vol_ball = (4/3) * np.pi * ball_radius**3\n",
    "    vol_tube = np.pi * tube_radius**2 * L\n",
    "    vol_total = 2 * vol_ball + vol_tube\n",
    "    \n",
    "    # Number of points per region\n",
    "    n_ball = int(n_total * vol_ball / vol_total)\n",
    "    n_tube = n_total - 2 * n_ball\n",
    "    \n",
    "    # Sample each region\n",
    "    ball1 = sample_3ball(n_ball, ball_radius, center=np.array([0, 0, 0]))\n",
    "    ball2 = sample_3ball(n_ball, ball_radius, center=np.array([2*ball_radius + L, 0, 0]))\n",
    "    tube = sample_cylinder(n_tube, L, tube_radius, center_start=np.array([ball_radius, 0, 0]))\n",
    "    \n",
    "    # Combine\n",
    "    points = np.vstack([ball1, tube, ball2])\n",
    "    \n",
    "    return points, {'vol_total': vol_total, 'vol_tube': vol_tube, 'n_ball': n_ball, 'n_tube': n_tube}\n",
    "\n",
    "# Quick test\n",
    "pts, info = sample_dumbbell(1000, L=2.0)\n",
    "print(f\"Dumbbell with L=2: {len(pts)} points\")\n",
    "print(f\"Volume fractions: balls={(2*info['n_ball'])/len(pts):.1%}, tube={info['n_tube']/len(pts):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Laplacian with Fixed ε (not k-NN)\n",
    "\n",
    "To avoid k-dependence, we use **ε-neighborhood graph** with ε scaled to point density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian_eps(points, epsilon, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Build graph Laplacian with ε-neighborhood.\n",
    "    Edge weight = exp(-||x-y||² / ε²) if ||x-y|| < ε, else 0.\n",
    "    \n",
    "    Uses batch processing for memory efficiency.\n",
    "    \"\"\"\n",
    "    N = len(points)\n",
    "    batch_size = 2000  # Process in batches to save memory\n",
    "    \n",
    "    # Build sparse adjacency in COO format\n",
    "    rows, cols, data = [], [], []\n",
    "    \n",
    "    for i_start in range(0, N, batch_size):\n",
    "        i_end = min(i_start + batch_size, N)\n",
    "        batch = points[i_start:i_end]\n",
    "        \n",
    "        # Compute distances from batch to all points\n",
    "        # ||x-y||² = ||x||² + ||y||² - 2<x,y>\n",
    "        batch_sq = np.sum(batch**2, axis=1, keepdims=True)  # (batch, 1)\n",
    "        all_sq = np.sum(points**2, axis=1)  # (N,)\n",
    "        dist_sq = batch_sq + all_sq - 2 * batch @ points.T  # (batch, N)\n",
    "        dist_sq = np.maximum(dist_sq, 0)  # Numerical stability\n",
    "        \n",
    "        # Find neighbors within epsilon\n",
    "        mask = (dist_sq < epsilon**2) & (dist_sq > 0)  # Exclude self\n",
    "        \n",
    "        # Add edges\n",
    "        for local_i, global_i in enumerate(range(i_start, i_end)):\n",
    "            neighbors = np.where(mask[local_i])[0]\n",
    "            for j in neighbors:\n",
    "                weight = np.exp(-dist_sq[local_i, j] / epsilon**2)\n",
    "                rows.append(global_i)\n",
    "                cols.append(j)\n",
    "                data.append(weight)\n",
    "    \n",
    "    # Build sparse weight matrix\n",
    "    W = csr_matrix((data, (rows, cols)), shape=(N, N))\n",
    "    W = (W + W.T) / 2  # Symmetrize\n",
    "    \n",
    "    # Degree matrix\n",
    "    degrees = np.array(W.sum(axis=1)).flatten()\n",
    "    \n",
    "    # Laplacian L = D - W\n",
    "    D = csr_matrix((degrees, (range(N), range(N))), shape=(N, N))\n",
    "    L = D - W\n",
    "    \n",
    "    # Normalized Laplacian for better convergence\n",
    "    # L_norm = D^{-1/2} L D^{-1/2} = I - D^{-1/2} W D^{-1/2}\n",
    "    deg_inv_sqrt = np.where(degrees > 0, 1.0 / np.sqrt(degrees), 0)\n",
    "    D_inv_sqrt = csr_matrix((deg_inv_sqrt, (range(N), range(N))), shape=(N, N))\n",
    "    L_norm = D_inv_sqrt @ L @ D_inv_sqrt\n",
    "    \n",
    "    return L_norm, W, degrees\n",
    "\n",
    "# Test\n",
    "L_test, _, _ = compute_laplacian_eps(pts, epsilon=0.5)\n",
    "print(f\"Laplacian shape: {L_test.shape}, nnz: {L_test.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eigenvalue Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda1(L_sparse, k=6):\n",
    "    \"\"\"\n",
    "    Compute smallest k eigenvalues of Laplacian.\n",
    "    Returns λ₁ (first non-zero eigenvalue).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use 'SM' for smallest magnitude (SciPy)\n",
    "        # Note: For CuPy, use 'SA' instead\n",
    "        eigenvalues, _ = eigsh(L_sparse, k=k, which='SM', tol=1e-6)\n",
    "        eigenvalues = np.sort(np.real(eigenvalues))\n",
    "        \n",
    "        # λ₀ ≈ 0 (constant eigenfunction), λ₁ is the spectral gap\n",
    "        # Find first eigenvalue > threshold\n",
    "        threshold = 1e-8\n",
    "        for ev in eigenvalues:\n",
    "            if ev > threshold:\n",
    "                return ev\n",
    "        return eigenvalues[1] if len(eigenvalues) > 1 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Eigenvalue computation failed: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Test\n",
    "lam1 = compute_lambda1(L_test)\n",
    "print(f\"λ₁ = {lam1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Experiment: Scaling with L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scaling_experiment(L_values, N_points=10000, tube_radius=0.3, ball_radius=1.0, n_trials=3):\n",
    "    \"\"\"\n",
    "    For each L, compute λ₁ and check if λ₁ × L² is approximately constant.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for L in L_values:\n",
    "        print(f\"\\n=== L = {L} ===\")\n",
    "        \n",
    "        trial_lambdas = []\n",
    "        for trial in range(n_trials):\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # Sample dumbbell\n",
    "            points, info = sample_dumbbell(N_points, L, ball_radius, tube_radius)\n",
    "            \n",
    "            # Adaptive epsilon based on point density\n",
    "            # ε ~ (Vol / N)^(1/3) for 3D\n",
    "            vol = info['vol_total']\n",
    "            epsilon = 2.0 * (vol / N_points) ** (1/3)\n",
    "            \n",
    "            # Build Laplacian\n",
    "            L_sparse, _, degrees = compute_laplacian_eps(points, epsilon)\n",
    "            avg_degree = np.mean(degrees)\n",
    "            \n",
    "            # Compute λ₁\n",
    "            lam1 = compute_lambda1(L_sparse)\n",
    "            trial_lambdas.append(lam1)\n",
    "            \n",
    "            dt = time.time() - t0\n",
    "            print(f\"  Trial {trial+1}: λ₁={lam1:.6f}, ε={epsilon:.3f}, avg_deg={avg_degree:.1f}, time={dt:.1f}s\")\n",
    "        \n",
    "        # Average over trials\n",
    "        lam1_mean = np.mean(trial_lambdas)\n",
    "        lam1_std = np.std(trial_lambdas)\n",
    "        \n",
    "        results.append({\n",
    "            'L': L,\n",
    "            'lambda1': lam1_mean,\n",
    "            'lambda1_std': lam1_std,\n",
    "            'lambda1_L2': lam1_mean * L**2,\n",
    "            'vol': info['vol_total']\n",
    "        })\n",
    "        \n",
    "        print(f\"  → λ₁ = {lam1_mean:.6f} ± {lam1_std:.6f}\")\n",
    "        print(f\"  → λ₁ × L² = {lam1_mean * L**2:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "L_values = [1, 2, 4, 8, 16]\n",
    "N_points = 8000  # Adjust based on available memory\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DUMBBELL SCALING EXPERIMENT\")\n",
    "print(f\"Testing L ∈ {L_values}\")\n",
    "print(f\"N = {N_points} points per dumbbell\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = run_scaling_experiment(L_values, N_points=N_points, n_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis: Is λ₁ × L² constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Compute coefficient of variation for λ₁ × L²\n",
    "lam_L2_values = df['lambda1_L2'].values\n",
    "mean_lam_L2 = np.mean(lam_L2_values)\n",
    "std_lam_L2 = np.std(lam_L2_values)\n",
    "cv = std_lam_L2 / mean_lam_L2 * 100\n",
    "\n",
    "print(f\"\\nλ₁ × L² statistics:\")\n",
    "print(f\"  Mean: {mean_lam_L2:.4f}\")\n",
    "print(f\"  Std:  {std_lam_L2:.4f}\")\n",
    "print(f\"  CV:   {cv:.1f}%\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if cv < 20:\n",
    "    print(\"✓ VALIDATED: λ₁ × L² ≈ constant (CV < 20%)\")\n",
    "    print(f\"  → λ₁ ~ {mean_lam_L2:.2f} / L²\")\n",
    "    verdict = \"SCALING_CONFIRMED\"\n",
    "elif cv < 50:\n",
    "    print(\"~ PARTIAL: λ₁ × L² shows trend but with variance\")\n",
    "    verdict = \"PARTIAL\"\n",
    "else:\n",
    "    print(\"✗ NOT CONFIRMED: λ₁ does not scale as 1/L²\")\n",
    "    verdict = \"SCALING_NOT_CONFIRMED\"\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: λ₁ vs L (log-log)\n",
    "ax1 = axes[0]\n",
    "ax1.loglog(df['L'], df['lambda1'], 'bo-', markersize=8, label='Data')\n",
    "# Fit line for 1/L² scaling\n",
    "L_fit = np.array(df['L'])\n",
    "lam_fit = mean_lam_L2 / L_fit**2\n",
    "ax1.loglog(L_fit, lam_fit, 'r--', label=f'c/L² (c={mean_lam_L2:.2f})')\n",
    "ax1.set_xlabel('L (neck length)')\n",
    "ax1.set_ylabel('λ₁')\n",
    "ax1.set_title('λ₁ vs L (log-log)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: λ₁ × L² vs L (should be flat)\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df['L'], df['lambda1_L2'], 'go-', markersize=8)\n",
    "ax2.axhline(mean_lam_L2, color='r', linestyle='--', label=f'Mean = {mean_lam_L2:.2f}')\n",
    "ax2.fill_between(df['L'], mean_lam_L2 - std_lam_L2, mean_lam_L2 + std_lam_L2, \n",
    "                  alpha=0.2, color='r')\n",
    "ax2.set_xlabel('L (neck length)')\n",
    "ax2.set_ylabel('λ₁ × L²')\n",
    "ax2.set_title('λ₁ × L² vs L (should be constant)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Check other scalings\n",
    "ax3 = axes[2]\n",
    "ax3.plot(df['L'], df['lambda1'] * df['L'], 'b^-', label='λ₁ × L')\n",
    "ax3.plot(df['L'], df['lambda1_L2'], 'gs-', label='λ₁ × L²')\n",
    "ax3.plot(df['L'], df['lambda1'] * df['L']**3, 'rv-', label='λ₁ × L³')\n",
    "ax3.set_xlabel('L')\n",
    "ax3.set_ylabel('Scaled λ₁')\n",
    "ax3.set_title('Which scaling is constant?')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dumbbell_scaling.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to dumbbell_scaling.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "import json\n",
    "\n",
    "output = {\n",
    "    'experiment': 'Dumbbell Scaling Test',\n",
    "    'purpose': 'Validate λ₁ ~ 1/L² scaling law (Tier 1)',\n",
    "    'parameters': {\n",
    "        'L_values': L_values,\n",
    "        'N_points': N_points,\n",
    "        'ball_radius': 1.0,\n",
    "        'tube_radius': 0.3\n",
    "    },\n",
    "    'results': [\n",
    "        {\n",
    "            'L': float(r['L']),\n",
    "            'lambda1': float(r['lambda1']),\n",
    "            'lambda1_std': float(r['lambda1_std']),\n",
    "            'lambda1_L2': float(r['lambda1_L2'])\n",
    "        } for r in results\n",
    "    ],\n",
    "    'analysis': {\n",
    "        'mean_lambda1_L2': float(mean_lam_L2),\n",
    "        'std_lambda1_L2': float(std_lam_L2),\n",
    "        'coefficient_of_variation_percent': float(cv)\n",
    "    },\n",
    "    'verdict': verdict,\n",
    "    'interpretation': {\n",
    "        'SCALING_CONFIRMED': 'λ₁ scales as 1/L², consistent with Cheeger bound',\n",
    "        'PARTIAL': 'Scaling trend visible but noisy',\n",
    "        'SCALING_NOT_CONFIRMED': 'No clear 1/L² scaling observed'\n",
    "    }[verdict]\n",
    "}\n",
    "\n",
    "with open('dumbbell_scaling_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"Results saved to dumbbell_scaling_results.json\")\n",
    "print(\"\\n\" + json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "If `λ₁ × L² ≈ constant` (CV < 20%), this validates **Tier 1** of the spectral bounds proof:\n",
    "\n",
    "$$\\frac{c_1}{L^2} \\leq \\lambda_1 \\leq \\frac{c_2}{L^2}$$\n",
    "\n",
    "**Next step**: Proceed to **Tier 2** — derive L² ~ H* via Mayer-Vietoris harmonic form counting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
