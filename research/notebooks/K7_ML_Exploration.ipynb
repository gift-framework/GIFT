{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K₇ ML Exploration: Mapping the True Geometry\n",
    "\n",
    "## Objective\n",
    "\n",
    "Before expensive FEM computation, use ML to **explore** K₇ geometry:\n",
    "1. Learn metric corrections δg beyond TCS model\n",
    "2. Discover geometric features (curvature hotspots, geodesic structure)\n",
    "3. Identify where TCS fails and why\n",
    "4. Guide FEM mesh refinement\n",
    "\n",
    "## Constraints (Hard + Soft)\n",
    "\n",
    "| Constraint | Type | Formula |\n",
    "|------------|------|--------|\n",
    "| det(g) = 65/32 | HARD | Topological |\n",
    "| Torsion-free | HARD | ‖T‖ < 0.1 (Joyce) |\n",
    "| λ₁ × H* = 13 | SOFT | Spectral gap |\n",
    "| G₂ holonomy | HARD | φ ∧ *φ = Vol |\n",
    "\n",
    "## Method: Neural Metric with Physics Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU support\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU = True\n",
    "    print(\"✓ CuPy GPU available\")\n",
    "except ImportError:\n",
    "    cp = np\n",
    "    GPU = False\n",
    "    print(\"CPU mode\")\n",
    "\n",
    "# PyTorch for neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch device: {device}\")\n",
    "\n",
    "# Constants\n",
    "DIM = 7\n",
    "DET_G_TARGET = 65/32\n",
    "H_STAR = 99\n",
    "LAMBDA1_H_TARGET = 13\n",
    "TCS_RATIO = H_STAR / 84\n",
    "\n",
    "print(f\"\\nTargets:\")\n",
    "print(f\"  det(g) = {DET_G_TARGET}\")\n",
    "print(f\"  λ₁ × H* = {LAMBDA1_H_TARGET}\")\n",
    "print(f\"  TCS ratio = {TCS_RATIO:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: G₂ Structure - The 3-form φ₀\n",
    "\n",
    "# Harvey-Lawson canonical G₂ 3-form indices\n",
    "# φ₀ = e^{012} + e^{034} + e^{056} + e^{135} - e^{146} - e^{236} - e^{245}\n",
    "\n",
    "PHI_INDICES = [\n",
    "    (0, 1, 2, +1),\n",
    "    (0, 3, 4, +1),\n",
    "    (0, 5, 6, +1),\n",
    "    (1, 3, 5, +1),\n",
    "    (1, 4, 6, -1),\n",
    "    (2, 3, 6, -1),\n",
    "    (2, 4, 5, -1),\n",
    "]\n",
    "\n",
    "def phi_tensor():\n",
    "    \"\"\"Build the G₂ 3-form as a (7,7,7) tensor.\"\"\"\n",
    "    phi = np.zeros((7, 7, 7))\n",
    "    for i, j, k, sign in PHI_INDICES:\n",
    "        # Antisymmetric\n",
    "        phi[i, j, k] = sign\n",
    "        phi[j, k, i] = sign\n",
    "        phi[k, i, j] = sign\n",
    "        phi[j, i, k] = -sign\n",
    "        phi[i, k, j] = -sign\n",
    "        phi[k, j, i] = -sign\n",
    "    return phi\n",
    "\n",
    "def metric_from_phi(phi, scale=1.0):\n",
    "    \"\"\"Compute metric from G₂ 3-form: g_ij = (1/6) φ_ikl φ_jkl.\"\"\"\n",
    "    g = np.einsum('ikl,jkl->ij', phi, phi) / 6.0\n",
    "    return g * scale\n",
    "\n",
    "# Reference metric\n",
    "phi0 = phi_tensor()\n",
    "g_ref = metric_from_phi(phi0)\n",
    "\n",
    "# Scale to get det(g) = 65/32\n",
    "# det(c²I) = c^14, so c² = (65/32)^(1/7)\n",
    "scale_factor = (DET_G_TARGET) ** (1/7)\n",
    "g_scaled = g_ref * scale_factor\n",
    "\n",
    "print(\"Reference metric g_ref (unscaled):\")\n",
    "print(f\"  g_ii = {g_ref[0,0]:.4f} (should be 1)\")\n",
    "print(f\"  det(g_ref) = {np.linalg.det(g_ref):.4f}\")\n",
    "\n",
    "print(f\"\\nScaled metric g (det = 65/32):\")\n",
    "print(f\"  g_ii = {g_scaled[0,0]:.6f}\")\n",
    "print(f\"  det(g) = {np.linalg.det(g_scaled):.6f}\")\n",
    "print(f\"  Target = {DET_G_TARGET:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Neural Metric Network\n",
    "# Learn position-dependent corrections to the TCS metric\n",
    "\n",
    "class NeuralMetric(nn.Module):\n",
    "    \"\"\"Neural network that outputs a 7×7 SPD metric at each point.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 7D coordinates (on K₇)\n",
    "    - Hidden: 3 layers with residual connections\n",
    "    - Output: 28 components (symmetric 7×7 matrix)\n",
    "    \n",
    "    Constraints enforced:\n",
    "    - SPD: g = L @ L.T (Cholesky parametrization)\n",
    "    - det(g) = 65/32: normalize after construction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input embedding\n",
    "        self.embed = nn.Linear(7, hidden_dim)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # Output: lower triangular matrix (28 components for 7×7)\n",
    "        self.out = nn.Linear(hidden_dim, 28)\n",
    "        \n",
    "        # Initialize near identity\n",
    "        self._init_near_identity()\n",
    "        \n",
    "    def _init_near_identity(self):\n",
    "        \"\"\"Initialize to output near-identity metric.\"\"\"\n",
    "        nn.init.zeros_(self.out.weight)\n",
    "        # Bias: diagonal of Cholesky = sqrt(g_ii) ≈ 1.05\n",
    "        bias = torch.zeros(28)\n",
    "        diag_indices = [0, 2, 5, 9, 14, 20, 27]  # Triangular number indices\n",
    "        for idx in diag_indices:\n",
    "            bias[idx] = np.sqrt(scale_factor)  # ≈ 1.05\n",
    "        self.out.bias = nn.Parameter(bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: coordinates → metric tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, 7) coordinates on K₇\n",
    "            \n",
    "        Returns:\n",
    "            g: (batch, 7, 7) metric tensors with det = 65/32\n",
    "        \"\"\"\n",
    "        batch = x.shape[0]\n",
    "        \n",
    "        # Embed\n",
    "        h = torch.tanh(self.embed(x))\n",
    "        \n",
    "        # Residual blocks\n",
    "        h = h + self.block1(h)\n",
    "        h = h + self.block2(h)\n",
    "        h = h + self.block3(h)\n",
    "        \n",
    "        # Output lower triangular\n",
    "        L_flat = self.out(h)  # (batch, 28)\n",
    "        \n",
    "        # Build lower triangular matrix\n",
    "        L = torch.zeros(batch, 7, 7, device=x.device)\n",
    "        idx = 0\n",
    "        for i in range(7):\n",
    "            for j in range(i + 1):\n",
    "                L[:, i, j] = L_flat[:, idx]\n",
    "                idx += 1\n",
    "        \n",
    "        # Ensure positive diagonal (for SPD)\n",
    "        L_diag = torch.diagonal(L, dim1=1, dim2=2)\n",
    "        L_diag_pos = torch.abs(L_diag) + 0.1  # Ensure > 0\n",
    "        L = L - torch.diag_embed(torch.diagonal(L, dim1=1, dim2=2)) + torch.diag_embed(L_diag_pos)\n",
    "        \n",
    "        # Metric: g = L @ L.T (guaranteed SPD)\n",
    "        g = torch.bmm(L, L.transpose(1, 2))\n",
    "        \n",
    "        # Normalize to det(g) = 65/32\n",
    "        det_g = torch.linalg.det(g)\n",
    "        scale = (DET_G_TARGET / det_g) ** (1/7)\n",
    "        g = g * scale.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        return g\n",
    "\n",
    "# Test\n",
    "model = NeuralMetric().to(device)\n",
    "x_test = torch.randn(100, 7, device=device)\n",
    "g_test = model(x_test)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nTest output shape: {g_test.shape}\")\n",
    "print(f\"Mean det(g): {torch.linalg.det(g_test).mean().item():.6f}\")\n",
    "print(f\"Target det(g): {DET_G_TARGET:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: TCS Sampling with Local Coordinates\n",
    "\n",
    "def sample_K7_local(N, seed=42):\n",
    "    \"\"\"Sample points on K₇ with local 7D coordinates.\n",
    "    \n",
    "    TCS model: K₇ ≈ (S³ × S³ × S¹) / gluing\n",
    "    \n",
    "    Local coordinates:\n",
    "    - x[0:4]: First S³ (quaternion → 3 angles + radial)\n",
    "    - x[4:7]: Second S³ compressed + S¹\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # S³ × S³ × S¹ sampling\n",
    "    Q1 = np.random.randn(N, 4)\n",
    "    Q1 /= np.linalg.norm(Q1, axis=1, keepdims=True)\n",
    "    \n",
    "    Q2 = np.random.randn(N, 4)\n",
    "    Q2 /= np.linalg.norm(Q2, axis=1, keepdims=True)\n",
    "    \n",
    "    theta = np.random.uniform(0, 2*np.pi, N)\n",
    "    \n",
    "    # Build 7D local coordinates\n",
    "    # Use stereographic-like projection\n",
    "    x = np.zeros((N, 7))\n",
    "    \n",
    "    # First S³: use q[1:4] (3 coords), q[0] is implicit\n",
    "    x[:, 0:3] = Q1[:, 1:4]\n",
    "    \n",
    "    # Second S³ scaled by TCS ratio\n",
    "    x[:, 3:6] = Q2[:, 1:4] * TCS_RATIO\n",
    "    \n",
    "    # S¹ (neck)\n",
    "    x[:, 6] = theta / (2 * np.pi)  # Normalize to [0, 1]\n",
    "    \n",
    "    return x, Q1, Q2, theta\n",
    "\n",
    "# Sample training data\n",
    "N_TRAIN = 10000\n",
    "x_train, Q1_train, Q2_train, theta_train = sample_K7_local(N_TRAIN)\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"Training samples: {N_TRAIN}\")\n",
    "print(f\"Coordinate ranges:\")\n",
    "for i in range(7):\n",
    "    print(f\"  x[{i}]: [{x_train[:, i].min():.3f}, {x_train[:, i].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Physics Loss Functions\n",
    "\n",
    "def compute_torsion_proxy(g, x, eps=1e-4):\n",
    "    \"\"\"Compute torsion proxy via metric smoothness.\n",
    "    \n",
    "    True G₂ torsion requires covariant derivatives of φ.\n",
    "    Proxy: measure how much g varies with position.\n",
    "    \n",
    "    Low variation → more likely torsion-free.\n",
    "    \"\"\"\n",
    "    batch = x.shape[0]\n",
    "    \n",
    "    # Finite difference in each direction\n",
    "    torsion_proxy = 0.0\n",
    "    \n",
    "    for d in range(7):\n",
    "        x_plus = x.clone()\n",
    "        x_plus[:, d] += eps\n",
    "        \n",
    "        g_plus = model(x_plus)\n",
    "        \n",
    "        # Variation\n",
    "        dg = (g_plus - g) / eps\n",
    "        torsion_proxy += (dg ** 2).sum(dim=(1, 2)).mean()\n",
    "    \n",
    "    return torsion_proxy / 7\n",
    "\n",
    "def compute_spectral_loss(g, x, k=30):\n",
    "    \"\"\"Estimate spectral gap from metric.\n",
    "    \n",
    "    Build graph Laplacian with learned metric distances.\n",
    "    \"\"\"\n",
    "    batch = g.shape[0]\n",
    "    \n",
    "    # Subsample for efficiency\n",
    "    if batch > 2000:\n",
    "        idx = torch.randperm(batch)[:2000]\n",
    "        g_sub = g[idx]\n",
    "        x_sub = x[idx]\n",
    "    else:\n",
    "        g_sub = g\n",
    "        x_sub = x\n",
    "    \n",
    "    n = g_sub.shape[0]\n",
    "    \n",
    "    # Compute metric distances: d²(i,j) = (x_i - x_j)^T g (x_i - x_j)\n",
    "    # Use average metric for simplicity\n",
    "    g_mean = g_sub.mean(dim=0)  # (7, 7)\n",
    "    \n",
    "    diff = x_sub.unsqueeze(1) - x_sub.unsqueeze(0)  # (n, n, 7)\n",
    "    \n",
    "    # d² = diff @ g_mean @ diff.T\n",
    "    d_sq = torch.einsum('ijk,kl,ijl->ij', diff, g_mean, diff)\n",
    "    d_sq = torch.clamp(d_sq, min=0)\n",
    "    \n",
    "    # Gaussian kernel\n",
    "    sigma = torch.median(torch.sqrt(d_sq[d_sq > 0]))\n",
    "    W = torch.exp(-d_sq / (2 * sigma**2))\n",
    "    W.fill_diagonal_(0)\n",
    "    \n",
    "    # k-NN sparsification (approximate)\n",
    "    topk_vals, _ = torch.topk(W, k, dim=1)\n",
    "    threshold = topk_vals[:, -1:]\n",
    "    W = W * (W >= threshold).float()\n",
    "    W = (W + W.T) / 2\n",
    "    \n",
    "    # Normalized Laplacian\n",
    "    deg = W.sum(dim=1)\n",
    "    deg_inv_sqrt = 1.0 / torch.sqrt(deg + 1e-8)\n",
    "    D_inv_sqrt = torch.diag(deg_inv_sqrt)\n",
    "    L = torch.eye(n, device=x.device) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "    \n",
    "    # Eigenvalues (use power iteration for speed)\n",
    "    # Approximate λ₁ via Rayleigh quotient with random vector\n",
    "    v = torch.randn(n, device=x.device)\n",
    "    v = v - v.mean()  # Orthogonal to constant\n",
    "    v = v / v.norm()\n",
    "    \n",
    "    for _ in range(20):\n",
    "        v = L @ v\n",
    "        v = v - v.mean()\n",
    "        v = v / v.norm()\n",
    "    \n",
    "    lambda1_approx = (v @ L @ v).item()\n",
    "    \n",
    "    # Loss: push toward λ₁ × H* = 13\n",
    "    target_lambda1 = LAMBDA1_H_TARGET / H_STAR\n",
    "    spectral_loss = (lambda1_approx - target_lambda1) ** 2\n",
    "    \n",
    "    return spectral_loss, lambda1_approx\n",
    "\n",
    "def compute_g2_loss(g):\n",
    "    \"\"\"Loss for G₂ metric structure.\n",
    "    \n",
    "    For G₂ metric: g_ij = (1/6) φ_ikl φ_jkl\n",
    "    This implies certain trace/symmetry conditions.\n",
    "    \"\"\"\n",
    "    # Trace should be 7 × g_ii = 7 × (65/32)^(1/7)\n",
    "    target_trace = 7 * scale_factor\n",
    "    trace_loss = ((g.diagonal(dim1=1, dim2=2).sum(dim=1) - target_trace) ** 2).mean()\n",
    "    \n",
    "    # Off-diagonal should be small for reference metric\n",
    "    mask = 1 - torch.eye(7, device=g.device)\n",
    "    offdiag = g * mask.unsqueeze(0)\n",
    "    offdiag_loss = (offdiag ** 2).sum(dim=(1, 2)).mean()\n",
    "    \n",
    "    return trace_loss + 0.1 * offdiag_loss\n",
    "\n",
    "print(\"Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Loop - Exploration Phase\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ML EXPLORATION: Learning K₇ Metric Corrections\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = NeuralMetric(hidden_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "\n",
    "# Training config\n",
    "N_EPOCHS = 500\n",
    "BATCH_SIZE = 512\n",
    "LOG_EVERY = 50\n",
    "\n",
    "# Loss weights\n",
    "W_G2 = 1.0        # G₂ structure\n",
    "W_TORSION = 0.1   # Smoothness (torsion proxy)\n",
    "W_SPECTRAL = 0.5  # Spectral gap\n",
    "\n",
    "history = {'epoch': [], 'loss': [], 'g2_loss': [], 'torsion': [], 'spectral': [], 'lambda1': []}\n",
    "\n",
    "print(f\"\\nTraining for {N_EPOCHS} epochs...\")\n",
    "print(f\"Loss weights: G₂={W_G2}, Torsion={W_TORSION}, Spectral={W_SPECTRAL}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Random batch\n",
    "    idx = torch.randperm(N_TRAIN)[:BATCH_SIZE]\n",
    "    x_batch = x_train_torch[idx]\n",
    "    \n",
    "    # Forward\n",
    "    g_batch = model(x_batch)\n",
    "    \n",
    "    # Losses\n",
    "    g2_loss = compute_g2_loss(g_batch)\n",
    "    \n",
    "    # Torsion proxy (expensive, compute less frequently)\n",
    "    if epoch % 10 == 0:\n",
    "        torsion_loss = compute_torsion_proxy(g_batch[:100], x_batch[:100])\n",
    "    \n",
    "    # Spectral loss (expensive, compute less frequently)\n",
    "    if epoch % 20 == 0:\n",
    "        spectral_loss, lambda1 = compute_spectral_loss(g_batch, x_batch)\n",
    "    \n",
    "    # Total loss\n",
    "    loss = W_G2 * g2_loss + W_TORSION * torsion_loss + W_SPECTRAL * spectral_loss\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    if epoch % LOG_EVERY == 0:\n",
    "        history['epoch'].append(epoch)\n",
    "        history['loss'].append(loss.item())\n",
    "        history['g2_loss'].append(g2_loss.item())\n",
    "        history['torsion'].append(torsion_loss.item())\n",
    "        history['spectral'].append(spectral_loss.item())\n",
    "        history['lambda1'].append(lambda1)\n",
    "        \n",
    "        print(f\"Epoch {epoch:4d} | Loss: {loss.item():.4f} | \"\n",
    "              f\"G₂: {g2_loss.item():.4f} | Torsion: {torsion_loss.item():.4f} | \"\n",
    "              f\"λ₁×H*: {lambda1 * H_STAR:.2f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analysis - What Did ML Discover?\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS: Exploring Learned Geometry\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Sample test points\n",
    "N_TEST = 5000\n",
    "x_test, Q1_test, Q2_test, theta_test = sample_K7_local(N_TEST, seed=123)\n",
    "x_test_torch = torch.tensor(x_test, dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    g_learned = model(x_test_torch)\n",
    "\n",
    "# Convert to numpy\n",
    "g_np = g_learned.cpu().numpy()\n",
    "\n",
    "# 1. Metric statistics\n",
    "print(\"\\n1. METRIC STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "det_g = np.linalg.det(g_np)\n",
    "print(f\"det(g) mean: {det_g.mean():.6f} (target: {DET_G_TARGET:.6f})\")\n",
    "print(f\"det(g) std:  {det_g.std():.6f}\")\n",
    "\n",
    "# Diagonal components\n",
    "g_diag = np.array([g_np[:, i, i] for i in range(7)])\n",
    "print(f\"\\nDiagonal g_ii:\")\n",
    "for i in range(7):\n",
    "    print(f\"  g_{i}{i}: {g_diag[i].mean():.4f} ± {g_diag[i].std():.4f}\")\n",
    "\n",
    "# Off-diagonal (should be small)\n",
    "g_offdiag = []\n",
    "for i in range(7):\n",
    "    for j in range(i+1, 7):\n",
    "        g_offdiag.append(np.abs(g_np[:, i, j]).mean())\n",
    "print(f\"\\nMean |g_ij| (off-diag): {np.mean(g_offdiag):.6f}\")\n",
    "\n",
    "# 2. Anisotropy analysis\n",
    "print(\"\\n2. ANISOTROPY ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Eigenvalues of metric at each point\n",
    "eigs = np.linalg.eigvalsh(g_np)\n",
    "anisotropy = eigs[:, -1] / eigs[:, 0]  # Max/min eigenvalue ratio\n",
    "\n",
    "print(f\"Anisotropy (max/min eigenvalue):\")\n",
    "print(f\"  Mean: {anisotropy.mean():.4f}\")\n",
    "print(f\"  Max:  {anisotropy.max():.4f}\")\n",
    "print(f\"  Min:  {anisotropy.min():.4f}\")\n",
    "\n",
    "# TCS vs non-TCS regions\n",
    "# Near \"neck\" (theta ≈ 0 or π)\n",
    "neck_mask = (np.abs(theta_test - np.pi) < 0.5) | (theta_test < 0.5) | (theta_test > 2*np.pi - 0.5)\n",
    "bulk_mask = ~neck_mask\n",
    "\n",
    "print(f\"\\nAnisotropy by region:\")\n",
    "print(f\"  Neck:  {anisotropy[neck_mask].mean():.4f} ({neck_mask.sum()} points)\")\n",
    "print(f\"  Bulk:  {anisotropy[bulk_mask].mean():.4f} ({bulk_mask.sum()} points)\")\n",
    "\n",
    "# 3. Deviation from TCS model\n",
    "print(\"\\n3. DEVIATION FROM TCS MODEL\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "g_tcs = np.eye(7) * scale_factor  # TCS reference (diagonal)\n",
    "deviation = np.sqrt(((g_np - g_tcs) ** 2).sum(axis=(1, 2)))\n",
    "\n",
    "print(f\"Frobenius deviation from TCS:\")\n",
    "print(f\"  Mean: {deviation.mean():.4f}\")\n",
    "print(f\"  Max:  {deviation.max():.4f}\")\n",
    "\n",
    "# Where is deviation largest?\n",
    "high_dev_idx = deviation > np.percentile(deviation, 95)\n",
    "print(f\"\\nHigh-deviation regions (top 5%):\")\n",
    "print(f\"  x mean: {x_test[high_dev_idx].mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization and Save Results\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Training loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.semilogy(history['epoch'], history['loss'], 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Total Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. λ₁ × H* evolution\n",
    "ax2 = axes[0, 1]\n",
    "lambda1_H = [l * H_STAR for l in history['lambda1']]\n",
    "ax2.plot(history['epoch'], lambda1_H, 'g-', linewidth=2)\n",
    "ax2.axhline(y=13, color='r', linestyle='--', label='Target 13')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('λ₁ × H*')\n",
    "ax2.set_title('Spectral Gap Evolution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Anisotropy distribution\n",
    "ax3 = axes[0, 2]\n",
    "ax3.hist(anisotropy, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(x=1.0, color='r', linestyle='--', label='Isotropic')\n",
    "ax3.set_xlabel('Anisotropy (λ_max / λ_min)')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Metric Anisotropy Distribution')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Diagonal components\n",
    "ax4 = axes[1, 0]\n",
    "for i in range(7):\n",
    "    ax4.hist(g_diag[i], bins=30, alpha=0.5, label=f'g_{i}{i}')\n",
    "ax4.axvline(x=scale_factor, color='k', linestyle='--', label=f'TCS ({scale_factor:.3f})')\n",
    "ax4.set_xlabel('Metric component value')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('Diagonal Metric Components')\n",
    "ax4.legend(fontsize=8)\n",
    "\n",
    "# 5. Deviation vs position (theta)\n",
    "ax5 = axes[1, 1]\n",
    "ax5.scatter(theta_test, deviation, c=anisotropy, cmap='viridis', alpha=0.3, s=5)\n",
    "ax5.set_xlabel('θ (S¹ coordinate)')\n",
    "ax5.set_ylabel('Deviation from TCS')\n",
    "ax5.set_title('Deviation vs Neck Position')\n",
    "cbar = plt.colorbar(ax5.collections[0], ax=ax5)\n",
    "cbar.set_label('Anisotropy')\n",
    "\n",
    "# 6. Metric heatmap (average)\n",
    "ax6 = axes[1, 2]\n",
    "g_mean = g_np.mean(axis=0)\n",
    "im = ax6.imshow(g_mean, cmap='RdBu_r', vmin=0.9, vmax=1.3)\n",
    "ax6.set_title('Mean Metric g_ij')\n",
    "ax6.set_xlabel('j')\n",
    "ax6.set_ylabel('i')\n",
    "plt.colorbar(im, ax=ax6)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "plt.savefig('outputs/k7_ml_exploration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notebook': 'K7_ML_Exploration.ipynb'\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': N_EPOCHS,\n",
    "        'final_loss': float(history['loss'][-1]),\n",
    "        'final_lambda1_H': float(history['lambda1'][-1] * H_STAR)\n",
    "    },\n",
    "    'metric_stats': {\n",
    "        'det_g_mean': float(det_g.mean()),\n",
    "        'det_g_std': float(det_g.std()),\n",
    "        'anisotropy_mean': float(anisotropy.mean()),\n",
    "        'anisotropy_max': float(anisotropy.max()),\n",
    "        'deviation_mean': float(deviation.mean()),\n",
    "        'deviation_max': float(deviation.max())\n",
    "    },\n",
    "    'regional_analysis': {\n",
    "        'neck_anisotropy': float(anisotropy[neck_mask].mean()),\n",
    "        'bulk_anisotropy': float(anisotropy[bulk_mask].mean())\n",
    "    },\n",
    "    'insights': {\n",
    "        'tcs_adequate': bool(deviation.mean() < 0.1),\n",
    "        'mostly_isotropic': bool(anisotropy.mean() < 1.2),\n",
    "        'neck_differs': bool(abs(anisotropy[neck_mask].mean() - anisotropy[bulk_mask].mean()) > 0.05)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('outputs/k7_ml_exploration.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved: outputs/k7_ml_exploration.png\")\n",
    "print(\"Saved: outputs/k7_ml_exploration.json\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. TCS model adequacy: {'✓ Good' if results['insights']['tcs_adequate'] else '✗ Needs refinement'}\")\n",
    "print(f\"2. Metric isotropy: {'✓ Mostly isotropic' if results['insights']['mostly_isotropic'] else '✗ Anisotropic'}\")\n",
    "print(f\"3. Neck vs bulk: {'✓ Different' if results['insights']['neck_differs'] else '≈ Similar'}\")\n",
    "print(f\"\\n4. Final λ₁ × H* = {results['training']['final_lambda1_H']:.2f} (target: 13)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
