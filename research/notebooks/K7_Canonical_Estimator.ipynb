{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K₇ Canonical Spectral Estimator\n",
    "\n",
    "## Objective: First-Principles k-Scaling (No Tuning)\n",
    "\n",
    "**The Problem**: Previous work used empirical k ∝ √N scaling. This allows \"tuning\" the coefficient to get desired results.\n",
    "\n",
    "**The Solution**: Use Belkin-Niyogi (2008) canonical scaling:\n",
    "\n",
    "```\n",
    "For d-dimensional manifold:\n",
    "  k_canonical = c × N^(6/(d+6))\n",
    "  \n",
    "For d=7 (K₇):\n",
    "  k_canonical = c × N^(6/13) ≈ c × N^0.462\n",
    "  \n",
    "Convergence rate: O(N^(-2/(d+6))) = O(N^(-2/13)) ≈ O(N^(-0.154))\n",
    "```\n",
    "\n",
    "**Key Test**: If the theory is right, the N→∞ limit should be **independent of coefficient c**.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- Belkin & Niyogi, \"Towards a theoretical foundation for Laplacian-based manifold methods\" (2008)\n",
    "- Calder & Garcia Trillos, \"Improved spectral convergence rates\" (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Check for GPU\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"✓ GPU available (CuPy)\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"○ CPU mode (NumPy/SciPy)\")\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from scipy.sparse.linalg import eigsh\n",
    "\n",
    "print(f\"Started: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Framework\n",
    "\n",
    "### Belkin-Niyogi Optimal Scaling\n",
    "\n",
    "For k-NN graph Laplacian on d-dimensional manifold:\n",
    "\n",
    "| Quantity | Formula | d=7 value |\n",
    "|----------|---------|----------|\n",
    "| Optimal k exponent | 6/(d+6) | 6/13 ≈ 0.462 |\n",
    "| Convergence rate | N^(-2/(d+6)) | N^(-0.154) |\n",
    "| Bias-variance balance | ε ~ N^(-1/(d+6)) | N^(-0.077) |\n",
    "\n",
    "### Key Insight\n",
    "The **coefficient c** in k = c × N^0.462 affects:\n",
    "- Finite-N values (shifts the curve)\n",
    "- But NOT the N→∞ limit (if theory holds)\n",
    "\n",
    "We test c ∈ {1, 2, 4, 8} and verify limit convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIFT Constants\n",
    "b2, b3 = 21, 77\n",
    "H_STAR = b2 + b3 + 1  # = 99\n",
    "DIM_G2 = 14\n",
    "DIM_K7 = 7\n",
    "\n",
    "# Belkin-Niyogi theoretical scaling for d=7\n",
    "K_EXPONENT = 6 / (DIM_K7 + 6)  # = 6/13 ≈ 0.462\n",
    "CONVERGENCE_RATE = 2 / (DIM_K7 + 6)  # = 2/13 ≈ 0.154\n",
    "\n",
    "# TCS metric parameters\n",
    "DET_G = 65/32\n",
    "RATIO = H_STAR / 84  # ≈ 1.179\n",
    "\n",
    "print(f\"K₇ Canonical Scaling:\")\n",
    "print(f\"  k exponent (theoretical): {K_EXPONENT:.4f}\")\n",
    "print(f\"  k exponent (empirical √N): 0.5000\")\n",
    "print(f\"  Difference: {0.5 - K_EXPONENT:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Convergence rate: O(N^{-CONVERGENCE_RATE:.4f})\")\n",
    "print(f\"H* = {H_STAR}, target range: [13, 14]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_S3_quaternion(n, rng):\n",
    "    \"\"\"Sample uniformly on S³ using quaternion normalization.\"\"\"\n",
    "    x = rng.standard_normal((n, 4))\n",
    "    x /= np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "def sample_S1(n, rng):\n",
    "    \"\"\"Sample uniformly on S¹.\"\"\"\n",
    "    return rng.uniform(0, 2*np.pi, n)\n",
    "\n",
    "def sample_TCS_K7(N, rng, ratio=RATIO):\n",
    "    \"\"\"Sample TCS construction: S¹ × S³ × S³ with metric scaling.\"\"\"\n",
    "    theta = sample_S1(N, rng)\n",
    "    q1 = sample_S3_quaternion(N, rng)\n",
    "    q2 = sample_S3_quaternion(N, rng)\n",
    "    return theta, q1, q2, ratio\n",
    "\n",
    "def geodesic_distance_S3(q1, q2):\n",
    "    \"\"\"Geodesic distance on S³: d = 2·arccos(|q₁·q₂|).\"\"\"\n",
    "    dot = np.abs(np.sum(q1 * q2, axis=1))\n",
    "    dot = np.clip(dot, -1, 1)\n",
    "    return 2 * np.arccos(dot)\n",
    "\n",
    "def geodesic_distance_S1(t1, t2):\n",
    "    \"\"\"Geodesic distance on S¹: d = min(|Δθ|, 2π-|Δθ|).\"\"\"\n",
    "    diff = np.abs(t1 - t2)\n",
    "    return np.minimum(diff, 2*np.pi - diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix_chunked(theta, q1, q2, ratio, chunk_size=2000):\n",
    "    \"\"\"Compute TCS distance matrix with memory-efficient chunking.\"\"\"\n",
    "    N = len(theta)\n",
    "    alpha = DET_G / (ratio ** 3)\n",
    "    \n",
    "    # Use float32 to save memory\n",
    "    D = np.zeros((N, N), dtype=np.float32)\n",
    "    \n",
    "    for i in range(0, N, chunk_size):\n",
    "        i_end = min(i + chunk_size, N)\n",
    "        for j in range(0, N, chunk_size):\n",
    "            j_end = min(j + chunk_size, N)\n",
    "            \n",
    "            # S¹ distances\n",
    "            t1 = theta[i:i_end, None]\n",
    "            t2 = theta[None, j:j_end]\n",
    "            d_S1 = geodesic_distance_S1(t1, t2)\n",
    "            \n",
    "            # S³ distances (first factor)\n",
    "            d_S3_1 = np.zeros((i_end-i, j_end-j), dtype=np.float32)\n",
    "            for ii, qi in enumerate(q1[i:i_end]):\n",
    "                dot = np.abs(np.sum(qi * q2[j:j_end], axis=1))\n",
    "                d_S3_1[ii] = 2 * np.arccos(np.clip(dot, -1, 1))\n",
    "            \n",
    "            # S³ distances (second factor, scaled by ratio)\n",
    "            d_S3_2 = np.zeros((i_end-i, j_end-j), dtype=np.float32)\n",
    "            for ii, qi in enumerate(q2[i:i_end]):\n",
    "                # Note: reusing q2 for both S³ factors (simplification)\n",
    "                dot = np.abs(np.sum(qi * q2[j:j_end], axis=1))\n",
    "                d_S3_2[ii] = 2 * np.arccos(np.clip(dot, -1, 1))\n",
    "            \n",
    "            # TCS metric: d² = α·d²_S¹ + d²_S³₁ + r²·d²_S³₂\n",
    "            D[i:i_end, j:j_end] = np.sqrt(\n",
    "                alpha * d_S1**2 + d_S3_1**2 + ratio**2 * d_S3_2**2\n",
    "            )\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_laplacian_knn(D, k):\n",
    "    \"\"\"Compute symmetric normalized Laplacian using k-NN graph.\n",
    "    \n",
    "    L = I - D^(-1/2) W D^(-1/2)\n",
    "    \n",
    "    where W_ij = exp(-d²_ij / 2σ²) for k nearest neighbors.\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    k = min(k, N - 1)\n",
    "    \n",
    "    # Find k nearest neighbors and compute σ as median of k-NN distances\n",
    "    knn_distances = np.zeros(N)\n",
    "    neighbors = np.zeros((N, k), dtype=np.int32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        idx = np.argpartition(D[i], k+1)[:k+1]\n",
    "        idx = idx[idx != i][:k]  # exclude self\n",
    "        neighbors[i] = idx\n",
    "        knn_distances[i] = np.median(D[i, idx])\n",
    "    \n",
    "    sigma = np.median(knn_distances)\n",
    "    \n",
    "    # Build sparse weight matrix (COO format for CuPy compatibility)\n",
    "    rows, cols, data = [], [], []\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in neighbors[i]:\n",
    "            w = np.exp(-D[i, j]**2 / (2 * sigma**2))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(w)\n",
    "    \n",
    "    # Symmetrize\n",
    "    rows_sym = rows + cols\n",
    "    cols_sym = cols + rows\n",
    "    data_sym = data + data\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        W = cp_csr((cp.array(data_sym), (cp.array(rows_sym), cp.array(cols_sym))), shape=(N, N))\n",
    "        # Degree matrix\n",
    "        d = cp.array(W.sum(axis=1)).flatten()\n",
    "        d_inv_sqrt = 1.0 / cp.sqrt(d + 1e-10)\n",
    "        D_inv_sqrt = cp_csr((d_inv_sqrt, (cp.arange(N), cp.arange(N))), shape=(N, N))\n",
    "        # Normalized Laplacian: L = I - D^(-1/2) W D^(-1/2)\n",
    "        L = cp_csr((cp.ones(N), (cp.arange(N), cp.arange(N))), shape=(N, N)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "    else:\n",
    "        from scipy.sparse import csr_matrix as sp_csr\n",
    "        W = sp_csr((data_sym, (rows_sym, cols_sym)), shape=(N, N))\n",
    "        d = np.array(W.sum(axis=1)).flatten()\n",
    "        d_inv_sqrt = 1.0 / np.sqrt(d + 1e-10)\n",
    "        D_inv_sqrt = sp_csr((d_inv_sqrt, (np.arange(N), np.arange(N))), shape=(N, N))\n",
    "        from scipy.sparse import eye\n",
    "        L = eye(N) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "    \n",
    "    return L, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda1(L, n_eigs=6):\n",
    "    \"\"\"Compute first non-zero eigenvalue of Laplacian.\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        eigenvalues = cp_eigsh(L, k=n_eigs, which='SA', return_eigenvectors=False)\n",
    "        eigenvalues = cp.asnumpy(eigenvalues)\n",
    "    else:\n",
    "        eigenvalues = eigsh(L, k=n_eigs, which='SA', return_eigenvectors=False)\n",
    "    \n",
    "    eigenvalues = np.sort(eigenvalues)\n",
    "    # First non-zero eigenvalue (skip λ₀ ≈ 0)\n",
    "    lambda1 = eigenvalues[eigenvalues > 1e-8][0] if np.any(eigenvalues > 1e-8) else eigenvalues[1]\n",
    "    return lambda1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Canonical Estimator: Test c-Independence\n",
    "\n",
    "We test multiple coefficients c ∈ {1, 2, 4, 8} with the canonical exponent 6/13.\n",
    "\n",
    "**Hypothesis**: All should converge to the same limit as N→∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonical_k(N, c=1.0):\n",
    "    \"\"\"Belkin-Niyogi canonical k for d=7.\"\"\"\n",
    "    return max(10, int(c * N ** K_EXPONENT))\n",
    "\n",
    "# Test parameters\n",
    "N_VALUES = [3000, 5000, 8000, 12000]  # Limited by memory\n",
    "C_VALUES = [1.0, 2.0, 4.0, 8.0]\n",
    "N_SEEDS = 3\n",
    "\n",
    "print(\"Canonical k values (k = c × N^0.462):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'N':>8} | \" + \" | \".join([f\"c={c}\" for c in C_VALUES]))\n",
    "print(\"-\" * 50)\n",
    "for N in N_VALUES:\n",
    "    ks = [canonical_k(N, c) for c in C_VALUES]\n",
    "    print(f\"{N:>8} | \" + \" | \".join([f\"{k:>4}\" for k in ks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Main computation: test c-independence\n",
    "results = {c: [] for c in C_VALUES}\n",
    "\n",
    "for N in N_VALUES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"N = {N}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for seed in range(N_SEEDS):\n",
    "        rng = np.random.default_rng(42 + seed)\n",
    "        theta, q1, q2, ratio = sample_TCS_K7(N, rng)\n",
    "        \n",
    "        print(f\"\\n  Seed {seed}: Computing distance matrix...\", end=\" \")\n",
    "        D = compute_distance_matrix_chunked(theta, q1, q2, ratio)\n",
    "        print(\"done\")\n",
    "        \n",
    "        for c in C_VALUES:\n",
    "            k = canonical_k(N, c)\n",
    "            L, sigma = compute_normalized_laplacian_knn(D, k)\n",
    "            lambda1 = compute_lambda1(L)\n",
    "            product = float(lambda1 * H_STAR)\n",
    "            \n",
    "            results[c].append({\n",
    "                'N': N,\n",
    "                'k': k,\n",
    "                'c': c,\n",
    "                'seed': seed,\n",
    "                'lambda1': float(lambda1),\n",
    "                'product': product,\n",
    "                'sigma': float(sigma)\n",
    "            })\n",
    "            \n",
    "            print(f\"    c={c}: k={k:3d}, λ₁×H* = {product:.3f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del D\n",
    "        if GPU_AVAILABLE:\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(f\"\\n\\nCompleted: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis: Convergence and c-Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute means and stds for each (N, c)\n",
    "summary = {}\n",
    "for c in C_VALUES:\n",
    "    summary[c] = {}\n",
    "    for N in N_VALUES:\n",
    "        products = [r['product'] for r in results[c] if r['N'] == N]\n",
    "        summary[c][N] = {'mean': np.mean(products), 'std': np.std(products)}\n",
    "\n",
    "# Plot convergence curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: λ₁×H* vs N for each c\n",
    "ax = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(C_VALUES)))\n",
    "for i, c in enumerate(C_VALUES):\n",
    "    means = [summary[c][N]['mean'] for N in N_VALUES]\n",
    "    stds = [summary[c][N]['std'] for N in N_VALUES]\n",
    "    ax.errorbar(N_VALUES, means, yerr=stds, marker='o', label=f'c={c}', \n",
    "                color=colors[i], capsize=3, linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(y=14, color='red', linestyle='--', alpha=0.7, label='Pell (14)')\n",
    "ax.axhline(y=13, color='blue', linestyle='--', alpha=0.7, label='Spinor (13)')\n",
    "ax.set_xlabel('N (sample size)', fontsize=12)\n",
    "ax.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax.set_title('Convergence: Canonical k = c × N^0.462', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Richardson extrapolation with theoretical rate\n",
    "ax = axes[1]\n",
    "\n",
    "# Use N^(-2/13) as the convergence variable\n",
    "x_theory = [N ** (-CONVERGENCE_RATE) for N in N_VALUES]\n",
    "\n",
    "for i, c in enumerate(C_VALUES):\n",
    "    means = [summary[c][N]['mean'] for N in N_VALUES]\n",
    "    ax.plot(x_theory, means, 'o-', label=f'c={c}', color=colors[i], \n",
    "            linewidth=2, markersize=8)\n",
    "    \n",
    "    # Linear fit for extrapolation\n",
    "    coeffs = np.polyfit(x_theory, means, 1)\n",
    "    x_extrap = np.linspace(0, max(x_theory), 100)\n",
    "    y_extrap = np.polyval(coeffs, x_extrap)\n",
    "    ax.plot(x_extrap, y_extrap, '--', color=colors[i], alpha=0.5)\n",
    "    \n",
    "    # Print extrapolated limit\n",
    "    limit = coeffs[1]  # y-intercept = N→∞ limit\n",
    "    print(f\"c={c}: Extrapolated limit = {limit:.3f}\")\n",
    "\n",
    "ax.axhline(y=14, color='red', linestyle='--', alpha=0.7)\n",
    "ax.axhline(y=13, color='blue', linestyle='--', alpha=0.7)\n",
    "ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel(f'N^(-{CONVERGENCE_RATE:.3f}) → 0 as N→∞', fontsize=12)\n",
    "ax.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax.set_title('Richardson Extrapolation (Theoretical Rate)', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('canonical_estimator_convergence.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Critical Test: Do Different c Values Converge to Same Limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute extrapolated limits for each c\n",
    "x_theory = [N ** (-CONVERGENCE_RATE) for N in N_VALUES]\n",
    "limits = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CRITICAL TEST: c-Independence of N→∞ Limit\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTheoretical convergence rate: O(N^{-CONVERGENCE_RATE:.4f})\")\n",
    "print(\"\\nExtrapolated limits (linear fit in N^(-0.154)):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for c in C_VALUES:\n",
    "    means = [summary[c][N]['mean'] for N in N_VALUES]\n",
    "    coeffs = np.polyfit(x_theory, means, 1)\n",
    "    limits[c] = coeffs[1]\n",
    "    slope = coeffs[0]\n",
    "    \n",
    "    # Compute R²\n",
    "    y_pred = np.polyval(coeffs, x_theory)\n",
    "    ss_res = np.sum((np.array(means) - y_pred)**2)\n",
    "    ss_tot = np.sum((np.array(means) - np.mean(means))**2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "    \n",
    "    print(f\"  c = {c}: limit = {limits[c]:.3f}, slope = {slope:.2f}, R² = {r2:.3f}\")\n",
    "\n",
    "# Check spread of limits\n",
    "limit_values = list(limits.values())\n",
    "mean_limit = np.mean(limit_values)\n",
    "std_limit = np.std(limit_values)\n",
    "spread = max(limit_values) - min(limit_values)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(f\"Mean limit:  {mean_limit:.3f}\")\n",
    "print(f\"Std:         {std_limit:.3f}\")\n",
    "print(f\"Spread:      {spread:.3f}\")\n",
    "print(f\"Relative:    {100*spread/mean_limit:.1f}%\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if spread < 1.0:  # Less than 1 unit spread\n",
    "    print(\"✓ PASS: Limits are c-INDEPENDENT (spread < 1)\")\n",
    "    print(f\"  Canonical limit: λ₁×H* = {mean_limit:.2f} ± {std_limit:.2f}\")\n",
    "else:\n",
    "    print(\"✗ FAIL: Limits depend on c (spread ≥ 1)\")\n",
    "    print(\"  The estimator is NOT canonical.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Theoretical vs Empirical Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare k-scalings\n",
    "print(\"\\nk-Scaling Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'N':>8} | {'Theoretical':>12} | {'Empirical √N':>12} | {'Pell':>12}\")\n",
    "print(f\"{'':>8} | {'c×N^0.462':>12} | {'0.74×N^0.5':>12} | {'0.366×N^0.5':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for N in [5000, 10000, 20000, 50000]:\n",
    "    k_theory = 2.0 * N ** K_EXPONENT  # c=2 as middle value\n",
    "    k_empirical = 0.74 * np.sqrt(N)\n",
    "    k_pell = 0.366 * np.sqrt(N)\n",
    "    print(f\"{N:>8} | {k_theory:>12.1f} | {k_empirical:>12.1f} | {k_pell:>12.1f}\")\n",
    "\n",
    "print(\"\\nNote: Theoretical k grows SLOWER than √N\")\n",
    "print(f\"      N^0.462 / √N = N^(-0.038) → 0 as N→∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final results\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'method': 'Belkin-Niyogi canonical k-scaling',\n",
    "        'k_exponent': float(K_EXPONENT),\n",
    "        'convergence_rate': float(CONVERGENCE_RATE),\n",
    "        'H_star': int(H_STAR),\n",
    "        'N_values': N_VALUES,\n",
    "        'c_values': C_VALUES,\n",
    "        'n_seeds': N_SEEDS\n",
    "    },\n",
    "    'raw_results': results,\n",
    "    'summary': {\n",
    "        c: {\n",
    "            str(N): {'mean': float(summary[c][N]['mean']), \n",
    "                     'std': float(summary[c][N]['std'])}\n",
    "            for N in N_VALUES\n",
    "        }\n",
    "        for c in C_VALUES\n",
    "    },\n",
    "    'extrapolated_limits': {str(c): float(limits[c]) for c in C_VALUES},\n",
    "    'conclusion': {\n",
    "        'mean_limit': float(mean_limit),\n",
    "        'std_limit': float(std_limit),\n",
    "        'spread': float(spread),\n",
    "        'c_independent': bool(spread < 1.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('canonical_estimator_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to canonical_estimator_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Theoretical k-scaling**: k = c × N^0.462 (Belkin-Niyogi for d=7)\n",
    "2. **Convergence rate**: O(N^-0.154) (theoretical)\n",
    "3. **c-independence test**: [RESULT ABOVE]\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "If limits are **c-independent**:\n",
    "- The canonical estimator works\n",
    "- The limit is a true geometric invariant\n",
    "- No tuning required\n",
    "\n",
    "If limits **depend on c**:\n",
    "- The graph Laplacian approximation has systematic bias\n",
    "- More sophisticated methods needed (heat kernel, etc.)\n",
    "- Or the N range is insufficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
