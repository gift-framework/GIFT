{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K₇ Spectral Gap: High-Precision Extrapolation\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Use validated methodology + Richardson extrapolation to estimate λ₁×H* at N→∞.\n",
    "\n",
    "**Convergence theory** (7D manifold):\n",
    "```\n",
    "λ(N) = λ∞ + A × N^(-β)   where β ≈ 1/11 ≈ 0.091\n",
    "```\n",
    "\n",
    "**Method**:\n",
    "1. Compute λ₁ at multiple N values (memory-optimized)\n",
    "2. Fit power law to extract λ∞\n",
    "3. Statistical confidence intervals\n",
    "\n",
    "## Target: Determine if λ₁×H* = 13 exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup with Memory Management\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "\n",
    "# GPU support\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU = True\n",
    "    print(\"✓ CuPy GPU available\")\n",
    "    \n",
    "    # Memory info\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    print(f\"  GPU memory: {mempool.total_bytes() / 1e9:.1f} GB allocated\")\n",
    "except ImportError:\n",
    "    GPU = False\n",
    "    from scipy.sparse import csr_matrix as cp_csr\n",
    "    from scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    cp = np\n",
    "    print(\"CPU mode\")\n",
    "\n",
    "# Constants\n",
    "H_STAR = 99\n",
    "TCS_RATIO = H_STAR / 84\n",
    "TARGET = 13  # dim(G₂) - 1\n",
    "BETA_THEORY = 1/11  # Convergence exponent for 7D\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressive memory cleanup.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "print(f\"\\nConstants:\")\n",
    "print(f\"  H* = {H_STAR}\")\n",
    "print(f\"  TCS ratio = {TCS_RATIO:.4f}\")\n",
    "print(f\"  Target λ₁×H* = {TARGET}\")\n",
    "print(f\"  Convergence β = {BETA_THEORY:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Memory-Optimized Spectral Computation\n",
    "\n",
    "def compute_lambda1_chunked(N, k, seed=42, chunk_size=5000):\n",
    "    \"\"\"Compute λ₁ with chunked distance computation for memory efficiency.\n",
    "    \n",
    "    For N=50k, full distance matrix = 50k² × 8 bytes = 20 GB\n",
    "    With chunking, we compute k-NN distances in chunks.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Sample K₇ points\n",
    "    Q1 = np.random.randn(N, 4)\n",
    "    Q1 /= np.linalg.norm(Q1, axis=1, keepdims=True)\n",
    "    \n",
    "    Q2 = np.random.randn(N, 4)\n",
    "    Q2 /= np.linalg.norm(Q2, axis=1, keepdims=True)\n",
    "    \n",
    "    theta = np.random.uniform(0, 2*np.pi, N)\n",
    "    \n",
    "    # Build sparse W matrix via chunked k-NN\n",
    "    print(f\"    Building sparse graph (N={N}, k={k})...\", end=\" \", flush=True)\n",
    "    \n",
    "    # Storage for sparse matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    \n",
    "    # Global sigma estimation (sample)\n",
    "    sample_idx = np.random.choice(N, min(1000, N), replace=False)\n",
    "    D_sample = compute_distance_block(Q1, Q2, theta, sample_idx, sample_idx)\n",
    "    knn_sample = np.partition(D_sample, min(k, len(sample_idx)-1), axis=1)[:, min(k, len(sample_idx)-1)]\n",
    "    sigma = np.median(knn_sample)\n",
    "    \n",
    "    # Process in chunks\n",
    "    n_chunks = (N + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        i_start = i * chunk_size\n",
    "        i_end = min((i + 1) * chunk_size, N)\n",
    "        idx_i = np.arange(i_start, i_end)\n",
    "        \n",
    "        # Compute distances to all points\n",
    "        D_chunk = compute_distance_block(Q1, Q2, theta, idx_i, np.arange(N))\n",
    "        \n",
    "        # Find k nearest neighbors for each row\n",
    "        for local_i, global_i in enumerate(idx_i):\n",
    "            dists = D_chunk[local_i]\n",
    "            # Get k nearest (excluding self)\n",
    "            knn_idx = np.argpartition(dists, k+1)[:k+1]\n",
    "            knn_idx = knn_idx[knn_idx != global_i][:k]\n",
    "            \n",
    "            # Gaussian weights\n",
    "            weights = np.exp(-dists[knn_idx]**2 / (2 * sigma**2))\n",
    "            \n",
    "            rows.extend([global_i] * len(knn_idx))\n",
    "            cols.extend(knn_idx.tolist())\n",
    "            data.extend(weights.tolist())\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del D_chunk\n",
    "        if i % 5 == 0:\n",
    "            clear_memory()\n",
    "    \n",
    "    print(\"done.\")\n",
    "    \n",
    "    # Build sparse W\n",
    "    from scipy.sparse import coo_matrix, diags\n",
    "    W = coo_matrix((data, (rows, cols)), shape=(N, N))\n",
    "    W = W.tocsr()\n",
    "    \n",
    "    # Symmetrize\n",
    "    W = (W + W.T) / 2\n",
    "    \n",
    "    # Normalized Laplacian\n",
    "    degrees = np.array(W.sum(axis=1)).flatten()\n",
    "    degrees = np.maximum(degrees, 1e-10)\n",
    "    D_inv_sqrt = diags(1.0 / np.sqrt(degrees))\n",
    "    \n",
    "    L = diags(np.ones(N)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "    \n",
    "    # Compute eigenvalues\n",
    "    print(f\"    Computing eigenvalues...\", end=\" \", flush=True)\n",
    "    \n",
    "    if GPU:\n",
    "        L_gpu = cp_csr(cp.array(L.toarray()))\n",
    "        eigs, _ = cp_eigsh(L_gpu, k=5, which='SA')\n",
    "        eigs = cp.asnumpy(eigs)\n",
    "    else:\n",
    "        eigs, _ = cp_eigsh(L.tocsr(), k=5, which='SM')\n",
    "    \n",
    "    eigs = np.sort(eigs)\n",
    "    lambda1 = eigs[1]  # First non-zero\n",
    "    \n",
    "    print(f\"λ₁ = {lambda1:.6f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del W, L, rows, cols, data\n",
    "    clear_memory()\n",
    "    \n",
    "    return lambda1, sigma\n",
    "\n",
    "def compute_distance_block(Q1, Q2, theta, idx_i, idx_j):\n",
    "    \"\"\"Compute geodesic distances between points idx_i and idx_j.\"\"\"\n",
    "    # S³ distances\n",
    "    dot1 = np.abs(Q1[idx_i] @ Q1[idx_j].T)\n",
    "    np.clip(dot1, 0, 1, out=dot1)\n",
    "    d1 = 2.0 * np.arccos(dot1)\n",
    "    \n",
    "    dot2 = np.abs(Q2[idx_i] @ Q2[idx_j].T)\n",
    "    np.clip(dot2, 0, 1, out=dot2)\n",
    "    d2 = 2.0 * np.arccos(dot2)\n",
    "    \n",
    "    # S¹ distance\n",
    "    dtheta = np.abs(theta[idx_i, None] - theta[None, idx_j])\n",
    "    dtheta = np.minimum(dtheta, 2*np.pi - dtheta)\n",
    "    \n",
    "    # TCS metric\n",
    "    D = np.sqrt(d1**2 + (TCS_RATIO**2) * d2**2 + dtheta**2)\n",
    "    \n",
    "    return D\n",
    "\n",
    "print(\"Functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Multi-N Convergence Study\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HIGH-PRECISION CONVERGENCE STUDY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# N values for extrapolation\n",
    "# Memory: N² × 8 bytes (but chunked, so manageable)\n",
    "N_values = [15000, 25000, 40000, 60000]\n",
    "\n",
    "print(f\"\\nN values: {N_values}\")\n",
    "print(f\"k = 0.74 × √N (optimal from research)\")\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "for N in N_values:\n",
    "    k = int(0.74 * np.sqrt(N))\n",
    "    print(f\"\\n--- N = {N:,}, k = {k} ---\")\n",
    "    \n",
    "    # Multiple seeds for error estimation\n",
    "    lambdas = []\n",
    "    for seed in range(3):\n",
    "        print(f\"  Seed {seed}:\")\n",
    "        lambda1, sigma = compute_lambda1_chunked(N, k, seed=42+seed, chunk_size=3000)\n",
    "        lambdas.append(lambda1)\n",
    "    \n",
    "    mean_lambda = np.mean(lambdas)\n",
    "    std_lambda = np.std(lambdas)\n",
    "    product = mean_lambda * H_STAR\n",
    "    \n",
    "    print(f\"  → λ₁ = {mean_lambda:.6f} ± {std_lambda:.6f}\")\n",
    "    print(f\"  → λ₁×H* = {product:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'N': N,\n",
    "        'k': k,\n",
    "        'lambda1_mean': float(mean_lambda),\n",
    "        'lambda1_std': float(std_lambda),\n",
    "        'product': float(product),\n",
    "        'individual': [float(l) for l in lambdas]\n",
    "    })\n",
    "    \n",
    "    clear_memory()\n",
    "\n",
    "print(\"\\n✓ Convergence study complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Richardson Extrapolation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RICHARDSON EXTRAPOLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract data\n",
    "Ns = np.array([r['N'] for r in results])\n",
    "products = np.array([r['product'] for r in results])\n",
    "stds = np.array([r['lambda1_std'] * H_STAR for r in results])\n",
    "\n",
    "# Model: λ×H* = λ∞×H* + A × N^(-β)\n",
    "def convergence_model(N, lambda_inf, A, beta):\n",
    "    return lambda_inf + A * np.power(N, -beta)\n",
    "\n",
    "def convergence_model_fixed_beta(N, lambda_inf, A):\n",
    "    return lambda_inf + A * np.power(N, -BETA_THEORY)\n",
    "\n",
    "# Fit 1: Free β\n",
    "try:\n",
    "    popt_free, pcov_free = curve_fit(\n",
    "        convergence_model, Ns, products,\n",
    "        p0=[13, 100, 0.1],\n",
    "        bounds=([0, -np.inf, 0.01], [20, np.inf, 0.5]),\n",
    "        sigma=stds + 0.01,  # Add small constant for stability\n",
    "        absolute_sigma=True\n",
    "    )\n",
    "    lambda_inf_free, A_free, beta_free = popt_free\n",
    "    perr_free = np.sqrt(np.diag(pcov_free))\n",
    "    fit_free_ok = True\n",
    "except:\n",
    "    fit_free_ok = False\n",
    "    lambda_inf_free, beta_free = np.nan, np.nan\n",
    "\n",
    "# Fit 2: Fixed β = 1/11 (theoretical)\n",
    "try:\n",
    "    popt_fixed, pcov_fixed = curve_fit(\n",
    "        convergence_model_fixed_beta, Ns, products,\n",
    "        p0=[13, 100],\n",
    "        sigma=stds + 0.01,\n",
    "        absolute_sigma=True\n",
    "    )\n",
    "    lambda_inf_fixed, A_fixed = popt_fixed\n",
    "    perr_fixed = np.sqrt(np.diag(pcov_fixed))\n",
    "    fit_fixed_ok = True\n",
    "except:\n",
    "    fit_fixed_ok = False\n",
    "    lambda_inf_fixed = np.nan\n",
    "\n",
    "# Simple linear extrapolation as backup\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(1/Ns, products)\n",
    "lambda_inf_linear = intercept  # Value at 1/N → 0\n",
    "\n",
    "print(\"\\nExtrapolation Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if fit_free_ok:\n",
    "    print(f\"\\n1. Free β fit:\")\n",
    "    print(f\"   λ∞×H* = {lambda_inf_free:.4f} ± {perr_free[0]:.4f}\")\n",
    "    print(f\"   β = {beta_free:.4f} (theory: {BETA_THEORY:.4f})\")\n",
    "    print(f\"   Deviation from 13: {abs(lambda_inf_free-13)/13*100:.2f}%\")\n",
    "\n",
    "if fit_fixed_ok:\n",
    "    print(f\"\\n2. Fixed β={BETA_THEORY:.4f} fit:\")\n",
    "    print(f\"   λ∞×H* = {lambda_inf_fixed:.4f} ± {perr_fixed[0]:.4f}\")\n",
    "    print(f\"   Deviation from 13: {abs(lambda_inf_fixed-13)/13*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n3. Linear extrapolation (1/N → 0):\")\n",
    "print(f\"   λ∞×H* = {lambda_inf_linear:.4f}\")\n",
    "print(f\"   R² = {r_value**2:.4f}\")\n",
    "print(f\"   Deviation from 13: {abs(lambda_inf_linear-13)/13*100:.2f}%\")\n",
    "\n",
    "# Best estimate\n",
    "if fit_fixed_ok:\n",
    "    best_estimate = lambda_inf_fixed\n",
    "    best_error = perr_fixed[0]\n",
    "    best_method = \"Fixed β\"\n",
    "elif fit_free_ok:\n",
    "    best_estimate = lambda_inf_free\n",
    "    best_error = perr_free[0]\n",
    "    best_method = \"Free β\"\n",
    "else:\n",
    "    best_estimate = lambda_inf_linear\n",
    "    best_error = std_err * max(Ns)  # Rough estimate\n",
    "    best_method = \"Linear\"\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"BEST ESTIMATE ({best_method}):\")\n",
    "print(f\"  λ₁×H* (N→∞) = {best_estimate:.4f} ± {best_error:.4f}\")\n",
    "print(f\"  Target = 13\")\n",
    "print(f\"  Deviation = {abs(best_estimate-13)/13*100:.2f}%\")\n",
    "print(f\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualization and Save\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Raw convergence\n",
    "ax1 = axes[0, 0]\n",
    "ax1.errorbar(Ns, products, yerr=stds, fmt='bo-', linewidth=2, markersize=8, capsize=5, label='Measured')\n",
    "ax1.axhline(y=13, color='g', linestyle='--', linewidth=2, label='Target 13')\n",
    "ax1.axhline(y=14, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Pell 14')\n",
    "ax1.set_xlabel('N (sample size)', fontsize=12)\n",
    "ax1.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax1.set_title('Convergence with Sample Size', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Extrapolation fit\n",
    "ax2 = axes[0, 1]\n",
    "N_plot = np.linspace(min(Ns)*0.5, max(Ns)*2, 100)\n",
    "\n",
    "ax2.errorbar(Ns, products, yerr=stds, fmt='bo', markersize=10, capsize=5, label='Data')\n",
    "\n",
    "if fit_fixed_ok:\n",
    "    ax2.plot(N_plot, convergence_model_fixed_beta(N_plot, *popt_fixed), 'r-', \n",
    "             linewidth=2, label=f'Fit: λ∞×H* = {lambda_inf_fixed:.2f}')\n",
    "\n",
    "ax2.axhline(y=13, color='g', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax2.axhline(y=best_estimate, color='red', linestyle=':', linewidth=2, label=f'Extrapolated: {best_estimate:.2f}')\n",
    "\n",
    "ax2.set_xlabel('N', fontsize=12)\n",
    "ax2.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax2.set_title('Richardson Extrapolation', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: 1/N plot (linear extrapolation)\n",
    "ax3 = axes[1, 0]\n",
    "inv_N = 1 / Ns\n",
    "ax3.errorbar(inv_N, products, yerr=stds, fmt='bo', markersize=10, capsize=5)\n",
    "ax3.plot([0, max(inv_N)*1.2], [intercept, intercept + slope*max(inv_N)*1.2], 'r-', linewidth=2)\n",
    "ax3.axhline(y=13, color='g', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax3.scatter([0], [intercept], color='red', s=100, zorder=5, marker='*', label=f'N→∞: {intercept:.2f}')\n",
    "ax3.set_xlabel('1/N', fontsize=12)\n",
    "ax3.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax3.set_title(f'Linear Extrapolation (R² = {r_value**2:.3f})', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim(-0.00001, max(inv_N)*1.2)\n",
    "\n",
    "# Plot 4: Final result\n",
    "ax4 = axes[1, 1]\n",
    "categories = ['Measured\\n(best N)', 'Extrapolated\\n(N→∞)', 'Target\\n13', 'Pell\\n14']\n",
    "values = [products[-1], best_estimate, 13, 14]\n",
    "colors = ['steelblue', 'red', 'green', 'orange']\n",
    "bars = ax4.bar(categories, values, color=colors, alpha=0.7)\n",
    "\n",
    "# Error bar on extrapolated\n",
    "ax4.errorbar(1, best_estimate, yerr=best_error, fmt='none', color='black', capsize=10, linewidth=2)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax4.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax4.set_title('Final Comparison', fontsize=12)\n",
    "ax4.set_ylim(0, 16)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "plt.savefig('outputs/k7_spectral_precision.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notebook': 'K7_Spectral_Precision.ipynb',\n",
    "        'method': 'Richardson extrapolation with chunked graph Laplacian'\n",
    "    },\n",
    "    'convergence_data': results,\n",
    "    'extrapolation': {\n",
    "        'free_beta': {\n",
    "            'lambda_inf': float(lambda_inf_free) if fit_free_ok else None,\n",
    "            'beta': float(beta_free) if fit_free_ok else None\n",
    "        },\n",
    "        'fixed_beta': {\n",
    "            'lambda_inf': float(lambda_inf_fixed) if fit_fixed_ok else None,\n",
    "            'beta_theory': float(BETA_THEORY)\n",
    "        },\n",
    "        'linear': {\n",
    "            'lambda_inf': float(lambda_inf_linear),\n",
    "            'r_squared': float(r_value**2)\n",
    "        }\n",
    "    },\n",
    "    'final_result': {\n",
    "        'lambda1_times_Hstar': float(best_estimate),\n",
    "        'uncertainty': float(best_error),\n",
    "        'method': best_method,\n",
    "        'deviation_from_13_pct': float(abs(best_estimate-13)/13*100),\n",
    "        'consistent_with_13': bool(abs(best_estimate-13) < 2*best_error + 0.5)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('outputs/k7_spectral_precision.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved: outputs/k7_spectral_precision.png\")\n",
    "print(\"Saved: outputs/k7_spectral_precision.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL CONCLUSION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n  λ₁ × H* (N→∞) = {best_estimate:.3f} ± {best_error:.3f}\")\n",
    "print(f\"\\n  Target (dim G₂ - 1) = 13\")\n",
    "print(f\"  Deviation = {abs(best_estimate-13)/13*100:.2f}%\")\n",
    "print(f\"\\n  Consistent with 13: {final_results['final_result']['consistent_with_13']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
