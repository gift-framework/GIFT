{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFT Spectral Gap v3: Unbiased Test\n",
    "\n",
    "**Key improvements over v2:**\n",
    "1. **Neutral initialization**: lambda starts at 1.0, NOT at 14/H*\n",
    "2. **Track minimum**: Record the lowest eigenvalue found\n",
    "3. **Multiple runs**: Average over random seeds\n",
    "\n",
    "**The test**: Does lambda naturally converge to 14/H* without being told the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy scipy matplotlib tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurvedLaplacian:\n",
    "    def __init__(self, metric_fn):\n",
    "        self.metric_fn = metric_fn\n",
    "        \n",
    "    def __call__(self, f_net, x):\n",
    "        x = x.requires_grad_(True)\n",
    "        batch_size, dim = x.shape\n",
    "        \n",
    "        g_diag = self.metric_fn(x)\n",
    "        g_inv = 1.0 / (g_diag + 1e-10)\n",
    "        sqrt_det_g = torch.sqrt(torch.prod(g_diag, dim=1, keepdim=True))\n",
    "        \n",
    "        f = f_net(x).squeeze(-1)\n",
    "        grad_f = torch.autograd.grad(f.sum(), x, create_graph=True)[0]\n",
    "        \n",
    "        laplacian = torch.zeros(batch_size, device=x.device)\n",
    "        for i in range(dim):\n",
    "            coeff = sqrt_det_g.squeeze() * g_inv[:, i]\n",
    "            flux = coeff * grad_f[:, i]\n",
    "            div_flux = torch.autograd.grad(flux.sum(), x, create_graph=True)[0][:, i]\n",
    "            laplacian = laplacian + div_flux / (sqrt_det_g.squeeze() + 1e-10)\n",
    "        \n",
    "        return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2TCSMetric:\n",
    "    def __init__(self, b2, b3, vol_normalized=True):\n",
    "        self.b2, self.b3 = b2, b3\n",
    "        self.H_star = b2 + b3 + 1\n",
    "        self.dim = 7\n",
    "        self.T = np.sqrt(self.H_star)\n",
    "        self.T0 = self.T / 3\n",
    "        \n",
    "        if vol_normalized:\n",
    "            self._compute_vol_normalization()\n",
    "        else:\n",
    "            self.vol_factor = 1.0\n",
    "    \n",
    "    def _compute_vol_normalization(self, n_samples=100000):\n",
    "        x = torch.rand(n_samples, 7, device=device)\n",
    "        x[:, 0] *= self.T\n",
    "        t = x[:, 0]\n",
    "        h_val = torch.cosh((t - self.T/2) / self.T0)\n",
    "        sqrt_g = h_val**6\n",
    "        vol_raw = torch.mean(sqrt_g).item() * self.T\n",
    "        self.vol_factor = vol_raw**(-1/7)\n",
    "        \n",
    "    def h(self, t):\n",
    "        return torch.cosh((t - self.T/2) / self.T0)\n",
    "    \n",
    "    def metric(self, x):\n",
    "        t = x[:, 0]\n",
    "        h_val = self.h(t)\n",
    "        g = torch.ones(x.shape[0], 7, device=x.device)\n",
    "        g[:, 1:] = (h_val**2).unsqueeze(1).expand(-1, 6)\n",
    "        return g * self.vol_factor**2\n",
    "    \n",
    "    def sqrt_det_g(self, x):\n",
    "        t = x[:, 0]\n",
    "        return self.h(t)**6 * self.vol_factor**7\n",
    "    \n",
    "    def sample(self, n_points):\n",
    "        x = torch.rand(n_points, 7, device=device)\n",
    "        x[:, 0] *= self.T\n",
    "        return x\n",
    "    \n",
    "    def lambda_1_predicted(self):\n",
    "        return 14.0 / self.H_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EigenfunctionNet(nn.Module):\n",
    "    def __init__(self, input_dim=7, hidden_dim=256, n_layers=8, fourier_features=64):\n",
    "        super().__init__()\n",
    "        self.register_buffer('B', torch.randn(input_dim, fourier_features) * 2.0)\n",
    "        encoded_dim = input_dim + 2 * fourier_features\n",
    "        \n",
    "        self.input_layer = nn.Linear(encoded_dim, hidden_dim)\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.GELU(),\n",
    "                         nn.Linear(hidden_dim, hidden_dim))\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.log_lambda = nn.Parameter(torch.tensor(0.0))  # lambda starts at 1.0\n",
    "        \n",
    "    def fourier_encode(self, x):\n",
    "        proj = x @ self.B\n",
    "        return torch.cat([x, torch.sin(proj), torch.cos(proj)], dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.gelu(self.input_layer(self.fourier_encode(x)))\n",
    "        for block in self.res_blocks:\n",
    "            h = h + 0.1 * block(h)\n",
    "        return self.output_layer(h)\n",
    "    \n",
    "    def get_lambda(self):\n",
    "        return torch.exp(self.log_lambda).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unbiased(metric_obj, n_epochs=5000, n_points=10000, lr=5e-4, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    net = EigenfunctionNet().to(device)\n",
    "    laplacian = CurvedLaplacian(metric_obj.metric)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=500)\n",
    "    \n",
    "    history = {'lambda': [], 'loss_pde': []}\n",
    "    best_lambda = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    pbar = tqdm(range(n_epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        x = metric_obj.sample(n_points)\n",
    "        sqrt_g = metric_obj.sqrt_det_g(x)\n",
    "        \n",
    "        f = net(x).squeeze(-1)\n",
    "        lam = torch.exp(net.log_lambda)\n",
    "        lap_f = laplacian(net, x)\n",
    "        \n",
    "        loss_pde = torch.mean((lap_f + lam * f)**2 * sqrt_g) / torch.mean(sqrt_g)\n",
    "        f_mean = torch.sum(f * sqrt_g) / torch.sum(sqrt_g)\n",
    "        f_norm = torch.sum(f**2 * sqrt_g) / torch.sum(sqrt_g)\n",
    "        loss_orth = f_mean**2\n",
    "        loss_norm = (f_norm - 1.0)**2\n",
    "        \n",
    "        loss = loss_pde + 100*loss_orth + 10*loss_norm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lambda = net.get_lambda()\n",
    "        history['lambda'].append(current_lambda)\n",
    "        history['loss_pde'].append(loss_pde.item())\n",
    "        \n",
    "        if epoch > 200 and loss_norm.item() < 0.1:\n",
    "            if current_lambda < best_lambda:\n",
    "                best_lambda = current_lambda\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            pbar.set_postfix({'lam': f\"{current_lambda:.4f}\", \n",
    "                            'min': f\"{best_lambda:.4f}\" if best_lambda < float('inf') else \"...\"})\n",
    "    \n",
    "    return {\n",
    "        'lambda_final': history['lambda'][-1],\n",
    "        'lambda_min': best_lambda if best_lambda < float('inf') else history['lambda'][-1],\n",
    "        'best_epoch': best_epoch,\n",
    "        'history': history\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"UNBIASED TEST: lambda starts at 1.0, NOT at 14/H*\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "manifolds = [\n",
    "    {\"name\": \"K7 (GIFT)\", \"b2\": 21, \"b3\": 77},\n",
    "    {\"name\": \"Joyce J1\", \"b2\": 12, \"b3\": 43},\n",
    "    {\"name\": \"Joyce J4\", \"b2\": 0, \"b3\": 103},\n",
    "    {\"name\": \"Kovalev TCS\", \"b2\": 0, \"b3\": 71},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for m in manifolds:\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"{m['name']}: H* = {m['b2'] + m['b3'] + 1}\")\n",
    "    print(f\"GIFT prediction: 14/H* = {14/(m['b2']+m['b3']+1):.6f}\")\n",
    "    print(f\"Initial lambda = 1.0 (NEUTRAL)\")\n",
    "    \n",
    "    metric = G2TCSMetric(m['b2'], m['b3'])\n",
    "    \n",
    "    run_results = []\n",
    "    for seed in [42, 123, 456]:\n",
    "        print(f\"\\nRun seed {seed}:\")\n",
    "        r = train_unbiased(metric, n_epochs=4000, n_points=10000, seed=seed)\n",
    "        run_results.append(r)\n",
    "        print(f\"  Final: {r['lambda_final']:.6f}, Min: {r['lambda_min']:.6f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'name': m['name'],\n",
    "        'H_star': m['b2'] + m['b3'] + 1,\n",
    "        'lambda_pred': 14 / (m['b2'] + m['b3'] + 1),\n",
    "        'lambda_final': np.mean([r['lambda_final'] for r in run_results]),\n",
    "        'lambda_min': np.mean([r['lambda_min'] for r in run_results]),\n",
    "        'runs': run_results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Manifold':<15} {'H*':>5} {'14/H*':>10} {'lam_min':>10} {'lam_final':>10}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<15} {r['H_star']:>5} {r['lambda_pred']:>10.6f} \"\n",
    "          f\"{r['lambda_min']:>10.6f} {r['lambda_final']:>10.6f}\")\n",
    "\n",
    "print(\"\\nINVARIANT: lambda_min * H* = ?\")\n",
    "products = [r['lambda_min'] * r['H_star'] for r in results]\n",
    "for r, p in zip(results, products):\n",
    "    print(f\"  {r['name']}: {p:.4f}\")\n",
    "print(f\"\\nMean: {np.mean(products):.4f} (target: 14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, r in zip(axes.flat, results):\n",
    "    history = r['runs'][0]['history']\n",
    "    ax.plot(history['lambda'], 'b-', alpha=0.7, label='lambda(t)')\n",
    "    ax.axhline(y=r['lambda_pred'], color='r', linestyle='--', label=f'14/H*={r[\"lambda_pred\"]:.4f}')\n",
    "    ax.axhline(y=r['lambda_min'], color='g', linestyle=':', label=f'min={r[\"lambda_min\"]:.4f}')\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.set_title(f\"{r['name']} (H*={r['H_star']})\")\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('lambda')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('unbiased_convergence.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
