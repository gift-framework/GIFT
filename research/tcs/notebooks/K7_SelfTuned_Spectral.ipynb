{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K₇ Self-Tuned Spectral Gap\n",
    "\n",
    "## Fully Automatic — Zero Manual Parameters\n",
    "\n",
    "**Method**: Cheng & Wu (2022) self-tuned k-NN kernels\n",
    "\n",
    "**Key Innovation**: Local bandwidth σᵢ = distance to k-th neighbor\n",
    "- No global σ parameter\n",
    "- No manual coefficient tuning\n",
    "- Proven convergence to manifold Laplacian\n",
    "\n",
    "**Reference**: \"Convergence of Graph Laplacian with kNN Self-tuned Kernels\"\n",
    "(Information and Inference, 2022)\n",
    "\n",
    "---\n",
    "\n",
    "## The Test\n",
    "\n",
    "We use TWO independent k values (k=30, k=50) to verify that:\n",
    "1. The self-tuned approach gives consistent results\n",
    "2. The N→∞ limit is independent of k choice\n",
    "\n",
    "If both k values give the same limit, we have **canonical validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Check for GPU\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"✓ GPU available (CuPy)\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"○ CPU mode (NumPy/SciPy)\")\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from scipy.sparse.linalg import eigsh\n",
    "\n",
    "print(f\"Started: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIFT Constants\n",
    "b2, b3 = 21, 77\n",
    "H_STAR = b2 + b3 + 1  # = 99\n",
    "DIM_G2 = 14\n",
    "DIM_K7 = 7\n",
    "DET_G = 65/32\n",
    "RATIO = H_STAR / 84  # ≈ 1.179\n",
    "\n",
    "print(f\"K₇ Self-Tuned Spectral Gap\")\n",
    "print(f\"H* = {H_STAR}, target range: [13, 14]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_S3_quaternion(n, rng):\n",
    "    \"\"\"Sample uniformly on S³ using quaternion normalization.\"\"\"\n",
    "    x = rng.standard_normal((n, 4))\n",
    "    x /= np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "def sample_S1(n, rng):\n",
    "    \"\"\"Sample uniformly on S¹.\"\"\"\n",
    "    return rng.uniform(0, 2*np.pi, n)\n",
    "\n",
    "def sample_TCS_K7(N, rng, ratio=RATIO):\n",
    "    \"\"\"Sample TCS construction: S¹ × S³ × S³ with metric scaling.\"\"\"\n",
    "    theta = sample_S1(N, rng)\n",
    "    q1 = sample_S3_quaternion(N, rng)\n",
    "    q2 = sample_S3_quaternion(N, rng)\n",
    "    return theta, q1, q2, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix_chunked(theta, q1, q2, ratio, chunk_size=2000):\n",
    "    \"\"\"Compute TCS distance matrix with memory-efficient chunking.\"\"\"\n",
    "    N = len(theta)\n",
    "    alpha = DET_G / (ratio ** 3)\n",
    "    \n",
    "    D = np.zeros((N, N), dtype=np.float32)\n",
    "    \n",
    "    for i in range(0, N, chunk_size):\n",
    "        i_end = min(i + chunk_size, N)\n",
    "        for j in range(0, N, chunk_size):\n",
    "            j_end = min(j + chunk_size, N)\n",
    "            \n",
    "            # S¹ distances\n",
    "            t1 = theta[i:i_end, None]\n",
    "            t2 = theta[None, j:j_end]\n",
    "            diff = np.abs(t1 - t2)\n",
    "            d_S1 = np.minimum(diff, 2*np.pi - diff)\n",
    "            \n",
    "            # S³ distances (both factors)\n",
    "            d_S3_1 = np.zeros((i_end-i, j_end-j), dtype=np.float32)\n",
    "            d_S3_2 = np.zeros((i_end-i, j_end-j), dtype=np.float32)\n",
    "            \n",
    "            for ii, (qi1, qi2) in enumerate(zip(q1[i:i_end], q2[i:i_end])):\n",
    "                dot1 = np.abs(np.sum(qi1 * q1[j:j_end], axis=1))\n",
    "                d_S3_1[ii] = 2 * np.arccos(np.clip(dot1, -1, 1))\n",
    "                \n",
    "                dot2 = np.abs(np.sum(qi2 * q2[j:j_end], axis=1))\n",
    "                d_S3_2[ii] = 2 * np.arccos(np.clip(dot2, -1, 1))\n",
    "            \n",
    "            # TCS metric\n",
    "            D[i:i_end, j:j_end] = np.sqrt(\n",
    "                alpha * d_S1**2 + d_S3_1**2 + ratio**2 * d_S3_2**2\n",
    "            )\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Tuned k-NN Laplacian (Cheng-Wu Method)\n",
    "\n",
    "**Key difference from standard approach**:\n",
    "\n",
    "Standard: W_ij = exp(-d²_ij / 2σ²) with global σ\n",
    "\n",
    "Self-tuned: W_ij = exp(-d²_ij / (σᵢ × σⱼ)) with local σᵢ = d(i, k-th neighbor)\n",
    "\n",
    "This **removes the global bandwidth parameter entirely**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_self_tuned_laplacian(D, k):\n",
    "    \"\"\"Compute self-tuned k-NN Laplacian (Cheng-Wu 2022).\n",
    "    \n",
    "    W_ij = exp(-d²_ij / (σᵢ × σⱼ))\n",
    "    where σᵢ = distance to k-th neighbor of point i\n",
    "    \n",
    "    This is FULLY AUTOMATIC - no manual σ parameter.\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    k = min(k, N - 1)\n",
    "    \n",
    "    # Compute local bandwidth σᵢ = distance to k-th neighbor\n",
    "    sigma = np.zeros(N)\n",
    "    neighbors = np.zeros((N, k), dtype=np.int32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Find k+1 nearest neighbors (including self)\n",
    "        idx = np.argpartition(D[i], k+1)[:k+1]\n",
    "        idx = idx[idx != i][:k]  # exclude self\n",
    "        neighbors[i] = idx\n",
    "        \n",
    "        # σᵢ = distance to k-th nearest neighbor\n",
    "        dists = D[i, idx]\n",
    "        sigma[i] = np.max(dists)  # k-th neighbor distance\n",
    "    \n",
    "    # Prevent zero sigma\n",
    "    sigma = np.maximum(sigma, 1e-10)\n",
    "    \n",
    "    # Build self-tuned weight matrix\n",
    "    rows, cols, data = [], [], []\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in neighbors[i]:\n",
    "            # Self-tuned kernel: exp(-d²/(σᵢ×σⱼ))\n",
    "            w = np.exp(-D[i, j]**2 / (sigma[i] * sigma[j]))\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(w)\n",
    "    \n",
    "    # Symmetrize\n",
    "    rows_sym = rows + cols\n",
    "    cols_sym = cols + rows\n",
    "    data_sym = data + data\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        W = cp_csr((cp.array(data_sym), (cp.array(rows_sym), cp.array(cols_sym))), shape=(N, N))\n",
    "        d = cp.array(W.sum(axis=1)).flatten()\n",
    "        d_inv_sqrt = 1.0 / cp.sqrt(d + 1e-10)\n",
    "        D_inv_sqrt = cp_csr((d_inv_sqrt, (cp.arange(N), cp.arange(N))), shape=(N, N))\n",
    "        L = cp_csr((cp.ones(N), (cp.arange(N), cp.arange(N))), shape=(N, N)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "    else:\n",
    "        from scipy.sparse import csr_matrix as sp_csr, eye\n",
    "        W = sp_csr((data_sym, (rows_sym, cols_sym)), shape=(N, N))\n",
    "        d = np.array(W.sum(axis=1)).flatten()\n",
    "        d_inv_sqrt = 1.0 / np.sqrt(d + 1e-10)\n",
    "        D_inv_sqrt = sp_csr((d_inv_sqrt, (np.arange(N), np.arange(N))), shape=(N, N))\n",
    "        L = eye(N) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "    \n",
    "    return L, np.median(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambda1(L, n_eigs=6):\n",
    "    \"\"\"Compute first non-zero eigenvalue of Laplacian.\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        eigenvalues = cp_eigsh(L, k=n_eigs, which='SA', return_eigenvectors=False)\n",
    "        eigenvalues = cp.asnumpy(eigenvalues)\n",
    "    else:\n",
    "        eigenvalues = eigsh(L, k=n_eigs, which='SA', return_eigenvectors=False)\n",
    "    \n",
    "    eigenvalues = np.sort(eigenvalues)\n",
    "    lambda1 = eigenvalues[eigenvalues > 1e-8][0] if np.any(eigenvalues > 1e-8) else eigenvalues[1]\n",
    "    return lambda1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment: k-Independence Test\n",
    "\n",
    "We test two different k values to verify the limit is independent of k choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_VALUES = [3000, 5000, 8000, 12000]\n",
    "K_VALUES = [30, 50]  # Two independent k values\n",
    "N_SEEDS = 3\n",
    "\n",
    "print(\"Self-Tuned Spectral Gap Experiment\")\n",
    "print(\"=\"*50)\n",
    "print(f\"N values: {N_VALUES}\")\n",
    "print(f\"k values: {K_VALUES} (testing k-independence)\")\n",
    "print(f\"Seeds per config: {N_SEEDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Main computation\n",
    "results = {k: [] for k in K_VALUES}\n",
    "\n",
    "for N in N_VALUES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"N = {N}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for seed in range(N_SEEDS):\n",
    "        rng = np.random.default_rng(42 + seed)\n",
    "        theta, q1, q2, ratio = sample_TCS_K7(N, rng)\n",
    "        \n",
    "        print(f\"\\n  Seed {seed}: Computing distance matrix...\", end=\" \")\n",
    "        D = compute_distance_matrix_chunked(theta, q1, q2, ratio)\n",
    "        print(\"done\")\n",
    "        \n",
    "        for k in K_VALUES:\n",
    "            L, sigma_med = compute_self_tuned_laplacian(D, k)\n",
    "            lambda1 = compute_lambda1(L)\n",
    "            product = float(lambda1 * H_STAR)\n",
    "            \n",
    "            results[k].append({\n",
    "                'N': N,\n",
    "                'k': k,\n",
    "                'seed': seed,\n",
    "                'lambda1': float(lambda1),\n",
    "                'product': product,\n",
    "                'sigma_median': float(sigma_med)\n",
    "            })\n",
    "            \n",
    "            print(f\"    k={k}: σ_med={sigma_med:.4f}, λ₁×H* = {product:.3f}\")\n",
    "        \n",
    "        del D\n",
    "        if GPU_AVAILABLE:\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(f\"\\n\\nCompleted: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: k-Independence and Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute summary statistics\n",
    "summary = {}\n",
    "for k in K_VALUES:\n",
    "    summary[k] = {}\n",
    "    for N in N_VALUES:\n",
    "        products = [r['product'] for r in results[k] if r['N'] == N]\n",
    "        summary[k][N] = {'mean': np.mean(products), 'std': np.std(products)}\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: λ₁×H* vs N for each k\n",
    "ax = axes[0]\n",
    "colors = ['blue', 'orange']\n",
    "for i, k in enumerate(K_VALUES):\n",
    "    means = [summary[k][N]['mean'] for N in N_VALUES]\n",
    "    stds = [summary[k][N]['std'] for N in N_VALUES]\n",
    "    ax.errorbar(N_VALUES, means, yerr=stds, marker='o', label=f'k={k}', \n",
    "                color=colors[i], capsize=3, linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(y=14, color='red', linestyle='--', alpha=0.7, label='Pell (14)')\n",
    "ax.axhline(y=13, color='green', linestyle='--', alpha=0.7, label='Spinor (13)')\n",
    "ax.set_xlabel('N (sample size)', fontsize=12)\n",
    "ax.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax.set_title('Self-Tuned Laplacian (NO manual σ)', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Extrapolation\n",
    "ax = axes[1]\n",
    "RATE = 2/13  # Theoretical convergence rate\n",
    "x_theory = [N ** (-RATE) for N in N_VALUES]\n",
    "\n",
    "limits = {}\n",
    "for i, k in enumerate(K_VALUES):\n",
    "    means = [summary[k][N]['mean'] for N in N_VALUES]\n",
    "    ax.plot(x_theory, means, 'o-', label=f'k={k}', color=colors[i], \n",
    "            linewidth=2, markersize=8)\n",
    "    \n",
    "    # Linear extrapolation\n",
    "    coeffs = np.polyfit(x_theory, means, 1)\n",
    "    limits[k] = coeffs[1]\n",
    "    \n",
    "    x_ext = np.linspace(0, max(x_theory), 100)\n",
    "    ax.plot(x_ext, np.polyval(coeffs, x_ext), '--', color=colors[i], alpha=0.5)\n",
    "    print(f\"k={k}: Extrapolated limit = {limits[k]:.3f}\")\n",
    "\n",
    "ax.axhline(y=14, color='red', linestyle='--', alpha=0.7)\n",
    "ax.axhline(y=13, color='green', linestyle='--', alpha=0.7)\n",
    "ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel(f'N^(-{RATE:.3f}) → 0 as N→∞', fontsize=12)\n",
    "ax.set_ylabel('λ₁ × H*', fontsize=12)\n",
    "ax.set_title('Richardson Extrapolation to N→∞', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('self_tuned_spectral.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Independence test\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"k-INDEPENDENCE TEST (Self-Tuned Method)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "limit_values = list(limits.values())\n",
    "mean_limit = np.mean(limit_values)\n",
    "spread = max(limit_values) - min(limit_values)\n",
    "\n",
    "print(f\"\\nExtrapolated limits:\")\n",
    "for k in K_VALUES:\n",
    "    print(f\"  k={k}: {limits[k]:.3f}\")\n",
    "\n",
    "print(f\"\\nMean limit:  {mean_limit:.3f}\")\n",
    "print(f\"Spread:      {spread:.3f}\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if spread < 1.0:\n",
    "    print(\"✓ PASS: Limits are k-INDEPENDENT (spread < 1)\")\n",
    "    print(f\"  CANONICAL LIMIT: λ₁×H* = {mean_limit:.2f}\")\n",
    "    \n",
    "    # Interpret\n",
    "    if abs(mean_limit - 14) < 1:\n",
    "        print(f\"  → Consistent with Pell prediction (14)\")\n",
    "    elif abs(mean_limit - 13) < 1:\n",
    "        print(f\"  → Consistent with spinor correction (13)\")\n",
    "    else:\n",
    "        print(f\"  → Novel value (not 13 or 14)\")\n",
    "else:\n",
    "    print(\"✗ FAIL: Limits depend on k (spread ≥ 1)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'method': 'Cheng-Wu self-tuned k-NN kernel',\n",
    "        'H_star': int(H_STAR),\n",
    "        'N_values': N_VALUES,\n",
    "        'k_values': K_VALUES,\n",
    "        'n_seeds': N_SEEDS\n",
    "    },\n",
    "    'raw_results': results,\n",
    "    'extrapolated_limits': {str(k): float(limits[k]) for k in K_VALUES},\n",
    "    'conclusion': {\n",
    "        'mean_limit': float(mean_limit),\n",
    "        'spread': float(spread),\n",
    "        'k_independent': bool(spread < 1.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('self_tuned_spectral_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to self_tuned_spectral_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Self-Tuned Method Advantages\n",
    "\n",
    "1. **Zero manual parameters**: σᵢ = k-th neighbor distance (automatic)\n",
    "2. **Density-adaptive**: Works in high and low density regions\n",
    "3. **Proven convergence**: Cheng & Wu (2022) guarantee manifold Laplacian limit\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "If k-independence holds and limit ≈ 14:\n",
    "- **Pell equation confirmed**: 99² − 50×14² = 1\n",
    "- **No tuning required**: Result is geometric invariant\n",
    "\n",
    "If k-independence holds and limit ≈ 13:\n",
    "- **Spinor correction confirmed**: dim(G₂) − h = 14 − 1\n",
    "- **Parallel spinor effect real**\n",
    "\n",
    "If limits depend on k:\n",
    "- Need larger N for convergence\n",
    "- Or: true limit is moduli-dependent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
