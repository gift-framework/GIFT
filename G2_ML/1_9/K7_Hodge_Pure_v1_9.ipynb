{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K7 Hodge Pure v1.9 - Harmonic Forms Training\n",
    "\n",
    "**Goal**: Learn true H² (21) and H³ (77) harmonic forms on the frozen v1.8 metric, then compute proper Yukawa tensor.\n",
    "\n",
    "## Key Objectives\n",
    "\n",
    "1. **Phase 1**: Train 21 harmonic 2-forms (H² basis)\n",
    "2. **Phase 2**: Train 77 harmonic 3-forms (H³ basis: 35 local + 42 global)\n",
    "3. **Phase 3**: Compute Yukawa tensor Y_ijk = ∫ ω_i ∧ ω_j ∧ Φ_k\n",
    "4. **Verify**: Does 43/77 split emerge? Does tau = 3472/891 appear?\n",
    "\n",
    "## Features\n",
    "\n",
    "- Automatic checkpoint saving every N epochs\n",
    "- Resume from checkpoint if interrupted\n",
    "- Training progress visualization\n",
    "- Multi-format output (npz, pt, json, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup and Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration.\"\"\"\n",
    "    # Geometry\n",
    "    dim: int = 7\n",
    "    b2_K7: int = 21\n",
    "    b3_K7: int = 77\n",
    "    b3_local: int = 35\n",
    "    b3_global: int = 42\n",
    "    \n",
    "    # Targets from GIFT\n",
    "    target_det_g: float = 2.03125  # 65/32\n",
    "    target_kappa_T: float = 0.01639344262295082  # 1/61\n",
    "    tau_target: float = 3.8967452300785634  # 3472/891\n",
    "    \n",
    "    # Network architecture\n",
    "    hidden_dim: int = 256\n",
    "    n_layers: int = 4\n",
    "    \n",
    "    # Training - H2\n",
    "    n_epochs_h2: int = 3000\n",
    "    lr_h2: float = 1e-3\n",
    "    \n",
    "    # Training - H3\n",
    "    n_epochs_h3: int = 5000\n",
    "    lr_h3: float = 5e-4\n",
    "    \n",
    "    # Common training\n",
    "    batch_size: int = 2048\n",
    "    weight_decay: float = 1e-5\n",
    "    max_grad_norm: float = 1.0\n",
    "    scheduler_patience: int = 300\n",
    "    scheduler_factor: float = 0.5\n",
    "    \n",
    "    # Loss weights\n",
    "    w_closed: float = 1.0\n",
    "    w_coclosed: float = 1.0\n",
    "    w_orthonormal: float = 0.1\n",
    "    w_g2_compat: float = 0.5\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_every: int = 500\n",
    "    checkpoint_dir: str = \"checkpoints_v1_9\"\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"outputs_v1_9\"\n",
    "    log_every: int = 100\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration:\")\n",
    "for k, v in asdict(config).items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load v1.8 Metric Data\n",
    "\n",
    "def load_v18_data(path: Optional[str] = None) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Load metric and phi from v1.8 samples.\"\"\"\n",
    "    \n",
    "    # Try common paths\n",
    "    paths_to_try = [\n",
    "        path,\n",
    "        \"samples.npz\",\n",
    "        \"../1_8/samples.npz\",\n",
    "        \"/content/samples.npz\",\n",
    "        \"/content/drive/MyDrive/GIFT/G2_ML/1_8/samples.npz\",\n",
    "    ]\n",
    "    \n",
    "    for p in paths_to_try:\n",
    "        if p and os.path.exists(p):\n",
    "            print(f\"Loading from {p}\")\n",
    "            data = np.load(p)\n",
    "            return {\n",
    "                'coords': torch.from_numpy(data['coords']).float(),\n",
    "                'metric': torch.from_numpy(data['metric']).float(),\n",
    "                'phi': torch.from_numpy(data['phi']).float(),\n",
    "            }\n",
    "    \n",
    "    # Generate synthetic data if not found\n",
    "    print(\"v1.8 data not found, generating synthetic data...\")\n",
    "    n_samples = 5000\n",
    "    \n",
    "    coords = torch.rand(n_samples, 7) * 2 * np.pi\n",
    "    \n",
    "    # Synthetic metric close to target det(g) = 65/32\n",
    "    metric = torch.eye(7).unsqueeze(0).expand(n_samples, -1, -1).clone()\n",
    "    # Scale to get det(g) ≈ 2.03125\n",
    "    scale = (config.target_det_g) ** (1/7)\n",
    "    metric = metric * scale\n",
    "    # Add small variations\n",
    "    metric = metric + 0.01 * torch.randn(n_samples, 7, 7)\n",
    "    metric = 0.5 * (metric + metric.transpose(-1, -2))  # Symmetrize\n",
    "    \n",
    "    # Synthetic phi\n",
    "    phi = torch.randn(n_samples, 35) * 0.5\n",
    "    phi = phi * np.sqrt(7.0) / (torch.norm(phi, dim=1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    return {'coords': coords, 'metric': metric, 'phi': phi}\n",
    "\n",
    "# Load data\n",
    "data = load_v18_data()\n",
    "n_samples = data['coords'].shape[0]\n",
    "print(f\"Loaded {n_samples} samples\")\n",
    "print(f\"  coords: {data['coords'].shape}\")\n",
    "print(f\"  metric: {data['metric'].shape}\")\n",
    "print(f\"  phi: {data['phi'].shape}\")\n",
    "\n",
    "# Verify metric\n",
    "det_g = torch.det(data['metric'])\n",
    "print(f\"  Mean det(g): {det_g.mean().item():.6f} (target: {config.target_det_g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Index Mappings for Differential Forms\n",
    "\n",
    "from itertools import combinations\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def form_indices(dim: int, degree: int) -> List[Tuple[int, ...]]:\n",
    "    \"\"\"Get ordered multi-indices for k-forms.\"\"\"\n",
    "    return list(combinations(range(dim), degree))\n",
    "\n",
    "def levi_civita_sign(perm: Tuple[int, ...]) -> int:\n",
    "    \"\"\"Compute sign of permutation.\"\"\"\n",
    "    n = len(perm)\n",
    "    if len(set(perm)) != n:\n",
    "        return 0\n",
    "    inversions = sum(1 for i in range(n) for j in range(i+1, n) if perm[i] > perm[j])\n",
    "    return 1 if inversions % 2 == 0 else -1\n",
    "\n",
    "# Verify\n",
    "print(f\"2-form indices: {len(form_indices(7, 2))} = C(7,2) = 21\")\n",
    "print(f\"3-form indices: {len(form_indices(7, 3))} = C(7,3) = 35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title H2 Network - 21 Harmonic 2-Forms\n",
    "\n",
    "class H2Network(nn.Module):\n",
    "    \"\"\"Network for 21 harmonic 2-forms on K7.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.n_modes = config.b2_K7  # 21\n",
    "        self.n_components = 21  # C(7,2)\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        layers = []\n",
    "        in_dim = config.dim\n",
    "        for _ in range(config.n_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, config.hidden_dim),\n",
    "                nn.SiLU(),\n",
    "            ])\n",
    "            in_dim = config.hidden_dim\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        # Per-mode output heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_dim, self.n_components)\n",
    "            for _ in range(self.n_modes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass. Returns (batch, 21 modes, 21 components).\"\"\"\n",
    "        features = self.features(x)\n",
    "        outputs = torch.stack([head(features) for head in self.heads], dim=1)\n",
    "        return outputs\n",
    "\n",
    "print(f\"H2Network: 21 modes x 21 components = 441 outputs per point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title H3 Network - 77 Harmonic 3-Forms\n",
    "\n",
    "class H3Network(nn.Module):\n",
    "    \"\"\"Network for 77 harmonic 3-forms on K7.\n",
    "    \n",
    "    Decomposed as:\n",
    "    - 35 local modes (from local phi structure)\n",
    "    - 42 global modes (from TCS neck topology)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.n_modes = config.b3_K7  # 77\n",
    "        self.n_local = config.b3_local  # 35\n",
    "        self.n_global = config.b3_global  # 42\n",
    "        self.n_components = 35  # C(7,3)\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        layers = []\n",
    "        in_dim = config.dim\n",
    "        for _ in range(config.n_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, config.hidden_dim),\n",
    "                nn.SiLU(),\n",
    "            ])\n",
    "            in_dim = config.hidden_dim\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        # Local mode heads (35)\n",
    "        self.local_heads = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_dim, self.n_components)\n",
    "            for _ in range(self.n_local)\n",
    "        ])\n",
    "        \n",
    "        # Global mode processing (include lambda/neck dependence)\n",
    "        self.global_neck = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim + 1, config.hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "        )\n",
    "        self.global_heads = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_dim, self.n_components)\n",
    "            for _ in range(self.n_global)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass. Returns (batch, 77 modes, 35 components).\"\"\"\n",
    "        lambda_neck = x[:, 0:1]  # First coord is neck parameter\n",
    "        features = self.features(x)\n",
    "        \n",
    "        # Local modes\n",
    "        local_out = torch.stack(\n",
    "            [head(features) for head in self.local_heads], dim=1\n",
    "        )  # (batch, 35, 35)\n",
    "        \n",
    "        # Global modes (neck-dependent)\n",
    "        global_features = self.global_neck(torch.cat([features, lambda_neck], dim=-1))\n",
    "        global_out = torch.stack(\n",
    "            [head(global_features) for head in self.global_heads], dim=1\n",
    "        )  # (batch, 42, 35)\n",
    "        \n",
    "        return torch.cat([local_out, global_out], dim=1)  # (batch, 77, 35)\n",
    "\n",
    "print(f\"H3Network: 77 modes (35 local + 42 global) x 35 components = 2695 outputs per point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Loss Functions\n",
    "\n",
    "def gram_matrix(forms: torch.Tensor, metric: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gram matrix of forms.\n",
    "    \n",
    "    Args:\n",
    "        forms: (batch, n_modes, n_components)\n",
    "        metric: (batch, 7, 7)\n",
    "    \n",
    "    Returns:\n",
    "        G: (n_modes, n_modes) Gram matrix\n",
    "    \"\"\"\n",
    "    batch, n_modes, n_comp = forms.shape\n",
    "    \n",
    "    # Volume element sqrt(|det g|)\n",
    "    det_g = torch.det(metric)\n",
    "    vol = torch.sqrt(det_g.abs()).unsqueeze(-1).unsqueeze(-1)  # (batch, 1, 1)\n",
    "    \n",
    "    # Weighted inner product\n",
    "    weighted = forms * vol\n",
    "    G = torch.einsum('bic,bjc->ij', weighted, forms) / batch\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def orthonormality_loss(G: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Loss for deviation from identity Gram matrix.\"\"\"\n",
    "    n = G.shape[0]\n",
    "    I = torch.eye(n, device=G.device)\n",
    "    return torch.mean((G - I) ** 2)\n",
    "\n",
    "\n",
    "def closedness_loss_fd(\n",
    "    x: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    degree: int = 2,\n",
    "    eps: float = 1e-4\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Loss for dω = 0 using finite differences.\"\"\"\n",
    "    batch = x.shape[0]\n",
    "    device = x.device\n",
    "    \n",
    "    omega = model(x)  # (batch, n_modes, n_comp)\n",
    "    n_modes = omega.shape[1]\n",
    "    \n",
    "    # Compute gradient approximation\n",
    "    total_loss = torch.tensor(0.0, device=device)\n",
    "    \n",
    "    for c in range(7):\n",
    "        x_plus = x.clone()\n",
    "        x_plus[:, c] += eps\n",
    "        x_minus = x.clone()\n",
    "        x_minus[:, c] -= eps\n",
    "        \n",
    "        omega_plus = model(x_plus)\n",
    "        omega_minus = model(x_minus)\n",
    "        \n",
    "        # Gradient\n",
    "        grad = (omega_plus - omega_minus) / (2 * eps)\n",
    "        \n",
    "        # For closed form, sum of antisymmetric gradients should vanish\n",
    "        total_loss += torch.mean(grad ** 2)\n",
    "    \n",
    "    return total_loss / 7\n",
    "\n",
    "\n",
    "def g2_compatibility_loss(Phi: torch.Tensor, phi_ref: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Loss for H3 local modes to be compatible with G2 structure.\n",
    "    \n",
    "    The first 35 modes should be related to the G2 3-form phi.\n",
    "    \"\"\"\n",
    "    # Local modes are Phi[:, :35, :]\n",
    "    local_modes = Phi[:, :35, :]  # (batch, 35, 35)\n",
    "    \n",
    "    # The diagonal should capture phi structure\n",
    "    diag = local_modes.diagonal(dim1=1, dim2=2)  # (batch, 35)\n",
    "    \n",
    "    # Compare with reference phi\n",
    "    return torch.mean((diag - phi_ref) ** 2)\n",
    "\n",
    "\n",
    "print(\"Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Checkpointing Utilities\n",
    "\n",
    "def save_checkpoint(\n",
    "    path: str,\n",
    "    epoch: int,\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler,\n",
    "    losses: Dict,\n",
    "    best_loss: float,\n",
    "    phase: str\n",
    "):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'losses': losses,\n",
    "        'best_loss': best_loss,\n",
    "        'phase': phase,\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    path: str,\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler = None\n",
    ") -> Dict:\n",
    "    \"\"\"Load training checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    print(f\"Resumed from epoch {checkpoint['epoch']}\")\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir: str, phase: str) -> Optional[str]:\n",
    "    \"\"\"Find latest checkpoint for a phase.\"\"\"\n",
    "    pattern = f\"{phase}_epoch_*.pt\"\n",
    "    checkpoints = sorted(Path(checkpoint_dir).glob(pattern))\n",
    "    return str(checkpoints[-1]) if checkpoints else None\n",
    "\n",
    "\n",
    "print(\"Checkpointing utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training Progress Visualization\n",
    "\n",
    "class TrainingVisualizer:\n",
    "    \"\"\"Live training visualization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.losses = {'total': [], 'ortho': [], 'closed': []}\n",
    "        self.epochs = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def update(self, epoch: int, loss_dict: Dict):\n",
    "        self.epochs.append(epoch)\n",
    "        for k in self.losses:\n",
    "            self.losses[k].append(loss_dict.get(k, 0))\n",
    "    \n",
    "    def plot(self, title: str = \"Training Progress\"):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Total loss\n",
    "        axes[0].semilogy(self.epochs, self.losses['total'])\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Total Loss')\n",
    "        axes[0].set_title('Total Loss')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Orthonormality\n",
    "        axes[1].semilogy(self.epochs, self.losses['ortho'])\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Orthonormality Loss')\n",
    "        axes[1].set_title('Gram Matrix → Identity')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Closedness\n",
    "        axes[2].semilogy(self.epochs, self.losses['closed'])\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('Closedness Loss')\n",
    "        axes[2].set_title('dω → 0')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        fig.suptitle(f\"{title} | Epoch {self.epochs[-1]} | Time: {elapsed/60:.1f} min\", fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Visualizer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 1: Train H2 (21 Harmonic 2-Forms)\n",
    "\n",
    "def train_h2(config: Config, data: Dict, resume: bool = True) -> Tuple[H2Network, Dict]:\n",
    "    \"\"\"Train H2 network.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 1: Training H2 (21 harmonic 2-forms)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Model\n",
    "    model = H2Network(config).to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr_h2, weight_decay=config.weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=config.scheduler_patience, factor=config.scheduler_factor)\n",
    "    \n",
    "    # Data\n",
    "    coords = data['coords'].to(device)\n",
    "    metric = data['metric'].to(device)\n",
    "    n_samples = coords.shape[0]\n",
    "    \n",
    "    # Check for checkpoint\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    all_losses = {'total': [], 'ortho': [], 'closed': []}\n",
    "    \n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    latest = find_latest_checkpoint(config.checkpoint_dir, 'h2')\n",
    "    if resume and latest:\n",
    "        ckpt = load_checkpoint(latest, model, optimizer, scheduler)\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        best_loss = ckpt['best_loss']\n",
    "        all_losses = ckpt['losses']\n",
    "    \n",
    "    # Visualizer\n",
    "    viz = TrainingVisualizer()\n",
    "    viz.losses = all_losses\n",
    "    viz.epochs = list(range(start_epoch))\n",
    "    \n",
    "    # Training loop\n",
    "    best_state = model.state_dict().copy()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.n_epochs_h2):\n",
    "        model.train()\n",
    "        \n",
    "        # Random batch\n",
    "        idx = torch.randperm(n_samples)[:config.batch_size]\n",
    "        x_batch = coords[idx]\n",
    "        g_batch = metric[idx]\n",
    "        \n",
    "        # Forward\n",
    "        omega = model(x_batch)\n",
    "        \n",
    "        # Losses\n",
    "        G = gram_matrix(omega, g_batch)\n",
    "        loss_ortho = orthonormality_loss(G)\n",
    "        loss_closed = closedness_loss_fd(x_batch, model, degree=2)\n",
    "        \n",
    "        total_loss = (config.w_orthonormal * loss_ortho + \n",
    "                      config.w_closed * loss_closed)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        # Track\n",
    "        loss_dict = {\n",
    "            'total': total_loss.item(),\n",
    "            'ortho': loss_ortho.item(),\n",
    "            'closed': loss_closed.item(),\n",
    "        }\n",
    "        all_losses['total'].append(loss_dict['total'])\n",
    "        all_losses['ortho'].append(loss_dict['ortho'])\n",
    "        all_losses['closed'].append(loss_dict['closed'])\n",
    "        \n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_state = model.state_dict().copy()\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch + 1) % config.log_every == 0:\n",
    "            viz.update(epoch + 1, loss_dict)\n",
    "            viz.plot(\"H2 Training\")\n",
    "        \n",
    "        # Checkpoint\n",
    "        if (epoch + 1) % config.checkpoint_every == 0:\n",
    "            ckpt_path = f\"{config.checkpoint_dir}/h2_epoch_{epoch+1:05d}.pt\"\n",
    "            save_checkpoint(ckpt_path, epoch, model, optimizer, scheduler, all_losses, best_loss, 'h2')\n",
    "    \n",
    "    # Final\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nH2 training complete. Best loss: {best_loss:.6f}\")\n",
    "    \n",
    "    return model, all_losses\n",
    "\n",
    "# Run H2 training\n",
    "h2_model, h2_losses = train_h2(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 2: Train H3 (77 Harmonic 3-Forms)\n",
    "\n",
    "def train_h3(config: Config, data: Dict, resume: bool = True) -> Tuple[H3Network, Dict]:\n",
    "    \"\"\"Train H3 network.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 2: Training H3 (77 harmonic 3-forms)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Model\n",
    "    model = H3Network(config).to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr_h3, weight_decay=config.weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=config.scheduler_patience, factor=config.scheduler_factor)\n",
    "    \n",
    "    # Data\n",
    "    coords = data['coords'].to(device)\n",
    "    metric = data['metric'].to(device)\n",
    "    phi = data['phi'].to(device)\n",
    "    n_samples = coords.shape[0]\n",
    "    \n",
    "    # Check for checkpoint\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    all_losses = {'total': [], 'ortho': [], 'closed': [], 'g2': []}\n",
    "    \n",
    "    latest = find_latest_checkpoint(config.checkpoint_dir, 'h3')\n",
    "    if resume and latest:\n",
    "        ckpt = load_checkpoint(latest, model, optimizer, scheduler)\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        best_loss = ckpt['best_loss']\n",
    "        all_losses = ckpt['losses']\n",
    "    \n",
    "    # Visualizer\n",
    "    viz = TrainingVisualizer()\n",
    "    viz.losses = {'total': all_losses['total'], 'ortho': all_losses['ortho'], 'closed': all_losses['closed']}\n",
    "    viz.epochs = list(range(start_epoch))\n",
    "    \n",
    "    # Training loop\n",
    "    best_state = model.state_dict().copy()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.n_epochs_h3):\n",
    "        model.train()\n",
    "        \n",
    "        # Random batch\n",
    "        idx = torch.randperm(n_samples)[:config.batch_size]\n",
    "        x_batch = coords[idx]\n",
    "        g_batch = metric[idx]\n",
    "        phi_batch = phi[idx]\n",
    "        \n",
    "        # Forward\n",
    "        Phi = model(x_batch)\n",
    "        \n",
    "        # Losses\n",
    "        G = gram_matrix(Phi, g_batch)\n",
    "        loss_ortho = orthonormality_loss(G)\n",
    "        loss_closed = closedness_loss_fd(x_batch, model, degree=3)\n",
    "        loss_g2 = g2_compatibility_loss(Phi, phi_batch)\n",
    "        \n",
    "        total_loss = (config.w_orthonormal * loss_ortho + \n",
    "                      config.w_closed * loss_closed +\n",
    "                      config.w_g2_compat * loss_g2)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        # Track\n",
    "        loss_dict = {\n",
    "            'total': total_loss.item(),\n",
    "            'ortho': loss_ortho.item(),\n",
    "            'closed': loss_closed.item(),\n",
    "            'g2': loss_g2.item(),\n",
    "        }\n",
    "        for k in all_losses:\n",
    "            all_losses[k].append(loss_dict.get(k, 0))\n",
    "        \n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_state = model.state_dict().copy()\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch + 1) % config.log_every == 0:\n",
    "            viz.update(epoch + 1, loss_dict)\n",
    "            viz.plot(\"H3 Training\")\n",
    "        \n",
    "        # Checkpoint\n",
    "        if (epoch + 1) % config.checkpoint_every == 0:\n",
    "            ckpt_path = f\"{config.checkpoint_dir}/h3_epoch_{epoch+1:05d}.pt\"\n",
    "            save_checkpoint(ckpt_path, epoch, model, optimizer, scheduler, all_losses, best_loss, 'h3')\n",
    "    \n",
    "    # Final\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nH3 training complete. Best loss: {best_loss:.6f}\")\n",
    "    \n",
    "    return model, all_losses\n",
    "\n",
    "# Run H3 training\n",
    "h3_model, h3_losses = train_h3(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 3: Compute Yukawa Tensor\n",
    "\n",
    "def compute_wedge_product(omega1, omega2, Phi):\n",
    "    \"\"\"Compute ω1 ∧ ω2 ∧ Φ (simplified).\n",
    "    \n",
    "    For full correctness, this should use proper wedge algebra.\n",
    "    Here we use a proxy: weighted triple correlation.\n",
    "    \"\"\"\n",
    "    # omega1, omega2: (batch, 21)\n",
    "    # Phi: (batch, 35)\n",
    "    \n",
    "    # ω1 ∧ ω2 has structure from antisymmetric combination\n",
    "    omega12 = omega1.unsqueeze(-1) * omega2.unsqueeze(-2)  # (batch, 21, 21)\n",
    "    omega12_anti = 0.5 * (omega12 - omega12.transpose(-1, -2))  # Antisymmetrize\n",
    "    \n",
    "    # Contract with Phi\n",
    "    # Use first 21 components of Phi for contraction\n",
    "    result = torch.einsum('bij,bi->b', omega12_anti, Phi[:, :21])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_yukawa(h2_model, h3_model, data: Dict, n_points: int = 5000):\n",
    "    \"\"\"Compute Yukawa tensor from trained models.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: Computing Yukawa Tensor\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    h2_model.eval()\n",
    "    h3_model.eval()\n",
    "    \n",
    "    coords = data['coords'].to(device)\n",
    "    metric = data['metric'].to(device)\n",
    "    \n",
    "    # Sample points\n",
    "    n_samples = min(n_points, coords.shape[0])\n",
    "    idx = torch.randperm(coords.shape[0])[:n_samples]\n",
    "    x = coords[idx]\n",
    "    g = metric[idx]\n",
    "    \n",
    "    # Volume element\n",
    "    det_g = torch.det(g)\n",
    "    vol = torch.sqrt(det_g.abs())\n",
    "    total_vol = vol.sum()\n",
    "    \n",
    "    print(f\"Using {n_samples} integration points\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        omega = h2_model(x)  # (n, 21, 21)\n",
    "        Phi = h3_model(x)    # (n, 77, 35)\n",
    "    \n",
    "    # Compute Yukawa tensor\n",
    "    n_h2 = config.b2_K7  # 21\n",
    "    n_h3 = config.b3_K7  # 77\n",
    "    \n",
    "    Y = torch.zeros(n_h2, n_h2, n_h3, device=device)\n",
    "    \n",
    "    print(\"Computing Y_ijk...\")\n",
    "    for i in range(n_h2):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Progress: {i+1}/{n_h2}\")\n",
    "        for j in range(i, n_h2):\n",
    "            omega_i = omega[:, i, :]  # (n, 21)\n",
    "            omega_j = omega[:, j, :]  # (n, 21)\n",
    "            \n",
    "            for k in range(n_h3):\n",
    "                Phi_k = Phi[:, k, :]  # (n, 35)\n",
    "                \n",
    "                # Wedge product coefficient\n",
    "                coeff = compute_wedge_product(omega_i, omega_j, Phi_k)\n",
    "                \n",
    "                # Integrate\n",
    "                integral = (coeff * vol).sum() / total_vol\n",
    "                \n",
    "                Y[i, j, k] = integral\n",
    "                if i != j:\n",
    "                    Y[j, i, k] = -integral\n",
    "    \n",
    "    # Gram matrix\n",
    "    print(\"Computing Gram matrix M = Y^T Y...\")\n",
    "    M = torch.einsum('ijk,ijl->kl', Y, Y)\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    print(\"Analyzing spectrum...\")\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(M)\n",
    "    \n",
    "    # Sort descending\n",
    "    idx_sort = torch.argsort(eigenvalues, descending=True)\n",
    "    eigenvalues = eigenvalues[idx_sort]\n",
    "    eigenvectors = eigenvectors[:, idx_sort]\n",
    "    \n",
    "    eigs = eigenvalues.cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'Y': Y.cpu().numpy(),\n",
    "        'M': M.cpu().numpy(),\n",
    "        'eigenvalues': eigs,\n",
    "        'eigenvectors': eigenvectors.cpu().numpy(),\n",
    "    }\n",
    "\n",
    "# Compute Yukawa\n",
    "yukawa_results = compute_yukawa(h2_model, h3_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Analyze Yukawa Spectrum\n",
    "\n",
    "def analyze_yukawa_spectrum(eigs: np.ndarray, tau_target: float = 3472/891):\n",
    "    \"\"\"Analyze eigenvalue spectrum for 43/77 and tau.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"YUKAWA SPECTRAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    # Basic stats\n",
    "    nonzero = (np.abs(eigs) > 1e-10).sum()\n",
    "    print(f\"[EIGENVALUE DISTRIBUTION]\")\n",
    "    print(f\"  Total modes: {len(eigs)}\")\n",
    "    print(f\"  Non-zero (>1e-10): {nonzero}\")\n",
    "    print(f\"  Top 5: {eigs[:5].round(6)}\")\n",
    "    print(f\"  Around 43: {eigs[40:46].round(6)}\")\n",
    "    print()\n",
    "    \n",
    "    # Gap analysis\n",
    "    gaps = np.abs(np.diff(eigs))\n",
    "    mean_gap = gaps.mean()\n",
    "    \n",
    "    print(f\"[GAP ANALYSIS]\")\n",
    "    top_gaps = np.argsort(gaps)[::-1][:5]\n",
    "    for i, idx in enumerate(top_gaps):\n",
    "        ratio = gaps[idx] / mean_gap if mean_gap > 0 else 0\n",
    "        marker = \" <-- 43!\" if idx == 42 else \"\"\n",
    "        print(f\"  #{i+1}: gap at {idx}->{idx+1}: {gaps[idx]:.6f} ({ratio:.1f}x mean){marker}\")\n",
    "    print()\n",
    "    \n",
    "    # 43/77 check\n",
    "    largest_gap_idx = np.argmax(gaps)\n",
    "    n_visible = largest_gap_idx + 1\n",
    "    gap_43 = gaps[42] if len(gaps) > 42 else 0\n",
    "    gap_43_ratio = gap_43 / mean_gap if mean_gap > 0 else 0\n",
    "    \n",
    "    print(f\"[43/77 SPLIT]\")\n",
    "    print(f\"  Suggested n_visible: {n_visible}\")\n",
    "    print(f\"  Gap at 43: {gap_43_ratio:.2f}x mean\")\n",
    "    if 41 <= n_visible <= 45:\n",
    "        print(\"  *** 43/77 STRUCTURE CONFIRMED! ***\")\n",
    "    print()\n",
    "    \n",
    "    # Tau analysis\n",
    "    cumsum = np.cumsum(eigs)\n",
    "    total = eigs.sum()\n",
    "    \n",
    "    print(f\"[TAU ANALYSIS]\")\n",
    "    print(f\"  Target tau: {tau_target:.6f}\")\n",
    "    \n",
    "    best_ratio = 0\n",
    "    best_error = float('inf')\n",
    "    best_n = 0\n",
    "    \n",
    "    for n in range(35, 55):\n",
    "        if n < len(eigs) and total - cumsum[n-1] > 1e-10:\n",
    "            ratio = cumsum[n-1] / (total - cumsum[n-1])\n",
    "            error = 100 * abs(ratio - tau_target) / tau_target\n",
    "            marker = \" ***\" if error < best_error else \"\"\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_ratio = ratio\n",
    "                best_n = n\n",
    "            if n in [42, 43, 44, 45]:\n",
    "                print(f\"  Split at {n}: ratio = {ratio:.4f}, error = {error:.1f}%{marker}\")\n",
    "    \n",
    "    print(f\"  Best: n={best_n}, ratio={best_ratio:.4f}, error={best_error:.1f}%\")\n",
    "    if best_error < 10:\n",
    "        print(\"  *** TAU EMERGES FROM SPECTRUM! ***\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'n_visible': n_visible,\n",
    "        'gap_43_ratio': gap_43_ratio,\n",
    "        'tau_estimate': best_ratio,\n",
    "        'tau_error_pct': best_error,\n",
    "        'nonzero_count': nonzero,\n",
    "    }\n",
    "\n",
    "# Analyze\n",
    "analysis = analyze_yukawa_spectrum(yukawa_results['eigenvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize Yukawa Spectrum\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "eigs = yukawa_results['eigenvalues']\n",
    "\n",
    "# Eigenvalue distribution\n",
    "ax = axes[0, 0]\n",
    "ax.semilogy(np.abs(eigs) + 1e-15)\n",
    "ax.axvline(x=42, color='r', linestyle='--', label='43/34 boundary')\n",
    "ax.set_xlabel('Mode index')\n",
    "ax.set_ylabel('|Eigenvalue|')\n",
    "ax.set_title('Yukawa Gram Matrix Spectrum')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gaps\n",
    "ax = axes[0, 1]\n",
    "gaps = np.abs(np.diff(eigs))\n",
    "ax.bar(range(len(gaps)), gaps)\n",
    "ax.axvline(x=42, color='r', linestyle='--', label='Gap at 43')\n",
    "ax.set_xlabel('Gap index')\n",
    "ax.set_ylabel('Gap magnitude')\n",
    "ax.set_title('Eigenvalue Gaps')\n",
    "ax.legend()\n",
    "\n",
    "# Cumulative variance\n",
    "ax = axes[1, 0]\n",
    "cumsum = np.cumsum(eigs)\n",
    "total = eigs.sum()\n",
    "ax.plot(100 * cumsum / total if total > 0 else cumsum)\n",
    "ax.axhline(y=100*43/77, color='r', linestyle='--', label='43/77 = 55.8%')\n",
    "ax.axvline(x=42, color='g', linestyle=':', label='n=43')\n",
    "ax.set_xlabel('Mode index')\n",
    "ax.set_ylabel('Cumulative variance (%)')\n",
    "ax.set_title('Cumulative Variance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Tau scan\n",
    "ax = axes[1, 1]\n",
    "tau_target = 3472/891\n",
    "ratios = []\n",
    "ns = list(range(35, 55))\n",
    "for n in ns:\n",
    "    if total - cumsum[n-1] > 1e-10:\n",
    "        ratios.append(cumsum[n-1] / (total - cumsum[n-1]))\n",
    "    else:\n",
    "        ratios.append(np.nan)\n",
    "ax.plot(ns, ratios, 'b-o')\n",
    "ax.axhline(y=tau_target, color='r', linestyle='--', label=f'tau = {tau_target:.3f}')\n",
    "ax.axvline(x=43, color='g', linestyle=':', label='n=43')\n",
    "ax.set_xlabel('Split position n')\n",
    "ax.set_ylabel('Visible/Hidden ratio')\n",
    "ax.set_title('Tau Emergence Scan')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.output_dir}/yukawa_spectrum.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save All Outputs\n",
    "\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving outputs...\")\n",
    "\n",
    "# 1. NPZ format (numpy)\n",
    "np.savez(\n",
    "    f\"{config.output_dir}/yukawa.npz\",\n",
    "    Y=yukawa_results['Y'],\n",
    "    M=yukawa_results['M'],\n",
    "    eigenvalues=yukawa_results['eigenvalues'],\n",
    "    eigenvectors=yukawa_results['eigenvectors'],\n",
    ")\n",
    "print(f\"  Saved: {config.output_dir}/yukawa.npz\")\n",
    "\n",
    "# 2. PyTorch format\n",
    "torch.save({\n",
    "    'h2_model': h2_model.state_dict(),\n",
    "    'h3_model': h3_model.state_dict(),\n",
    "    'config': asdict(config),\n",
    "}, f\"{config.output_dir}/models.pt\")\n",
    "print(f\"  Saved: {config.output_dir}/models.pt\")\n",
    "\n",
    "# 3. JSON format (analysis)\n",
    "final_metrics = {\n",
    "    'geometry': {\n",
    "        'det_g_mean': float(torch.det(data['metric']).mean().item()),\n",
    "        'det_g_target': config.target_det_g,\n",
    "    },\n",
    "    'training': {\n",
    "        'h2_epochs': config.n_epochs_h2,\n",
    "        'h3_epochs': config.n_epochs_h3,\n",
    "        'h2_final_loss': h2_losses['total'][-1] if h2_losses['total'] else None,\n",
    "        'h3_final_loss': h3_losses['total'][-1] if h3_losses['total'] else None,\n",
    "    },\n",
    "    'yukawa': {\n",
    "        'n_visible': int(analysis['n_visible']),\n",
    "        'gap_43_ratio': float(analysis['gap_43_ratio']),\n",
    "        'tau_estimate': float(analysis['tau_estimate']),\n",
    "        'tau_error_pct': float(analysis['tau_error_pct']),\n",
    "        'nonzero_count': int(analysis['nonzero_count']),\n",
    "        'tau_target': config.tau_target,\n",
    "    },\n",
    "    'verdict': {\n",
    "        '43_77_confirmed': 41 <= analysis['n_visible'] <= 45,\n",
    "        'tau_emerged': analysis['tau_error_pct'] < 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(f\"{config.output_dir}/final_metrics.json\", 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "print(f\"  Saved: {config.output_dir}/final_metrics.json\")\n",
    "\n",
    "# 4. CSV format (eigenvalues)\n",
    "import csv\n",
    "with open(f\"{config.output_dir}/eigenvalues.csv\", 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['index', 'eigenvalue', 'cumulative', 'gap'])\n",
    "    cumsum = np.cumsum(yukawa_results['eigenvalues'])\n",
    "    gaps = np.abs(np.diff(yukawa_results['eigenvalues']))\n",
    "    for i, ev in enumerate(yukawa_results['eigenvalues']):\n",
    "        gap = gaps[i] if i < len(gaps) else 0\n",
    "        writer.writerow([i, ev, cumsum[i], gap])\n",
    "print(f\"  Saved: {config.output_dir}/eigenvalues.csv\")\n",
    "\n",
    "# 5. Samples for further analysis\n",
    "with torch.no_grad():\n",
    "    coords = data['coords'].to(device)\n",
    "    sample_omega = h2_model(coords[:1000])\n",
    "    sample_Phi = h3_model(coords[:1000])\n",
    "\n",
    "np.savez(\n",
    "    f\"{config.output_dir}/samples.npz\",\n",
    "    coords=data['coords'][:1000].numpy(),\n",
    "    metric=data['metric'][:1000].numpy(),\n",
    "    phi=data['phi'][:1000].numpy(),\n",
    "    omega=sample_omega.cpu().numpy(),\n",
    "    Phi=sample_Phi.cpu().numpy(),\n",
    ")\n",
    "print(f\"  Saved: {config.output_dir}/samples.npz\")\n",
    "\n",
    "print(\"\\nAll outputs saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Final Summary\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"V1.9 HODGE PURE - FINAL REPORT\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"[GEOMETRY]\")\n",
    "print(f\"  det(g) mean: {final_metrics['geometry']['det_g_mean']:.6f}\")\n",
    "print(f\"  det(g) target: {final_metrics['geometry']['det_g_target']}\")\n",
    "print()\n",
    "print(\"[TRAINING]\")\n",
    "print(f\"  H2 epochs: {final_metrics['training']['h2_epochs']}\")\n",
    "print(f\"  H3 epochs: {final_metrics['training']['h3_epochs']}\")\n",
    "print(f\"  H2 final loss: {final_metrics['training']['h2_final_loss']:.6f}\")\n",
    "print(f\"  H3 final loss: {final_metrics['training']['h3_final_loss']:.6f}\")\n",
    "print()\n",
    "print(\"[YUKAWA SPECTRUM]\")\n",
    "print(f\"  Suggested n_visible: {final_metrics['yukawa']['n_visible']}\")\n",
    "print(f\"  Gap at 43: {final_metrics['yukawa']['gap_43_ratio']:.2f}x mean\")\n",
    "print(f\"  Non-zero eigenvalues: {final_metrics['yukawa']['nonzero_count']}\")\n",
    "print()\n",
    "print(\"[TAU]\")\n",
    "print(f\"  Target: {final_metrics['yukawa']['tau_target']:.6f}\")\n",
    "print(f\"  Estimated: {final_metrics['yukawa']['tau_estimate']:.6f}\")\n",
    "print(f\"  Error: {final_metrics['yukawa']['tau_error_pct']:.1f}%\")\n",
    "print()\n",
    "print(\"[VERDICT]\")\n",
    "print(f\"  43/77 confirmed: {'YES' if final_metrics['verdict']['43_77_confirmed'] else 'NO'}\")\n",
    "print(f\"  Tau emerged: {'YES' if final_metrics['verdict']['tau_emerged'] else 'NO'}\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
