{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# K7 GIFT v1.5b - Local/Global G2 Decomposition Framework\n\n**Version 1.5b**: Robust training with protected local kappa_T\n\n## Key Improvements over v1.5\n- **Freeze local in early phases**: Protect kappa_T from v1.4 solution\n- **Separated torsion losses**: T_local anchor + T_global penalty\n- **Robust global basis**: Guaranteed rank 42 via Gram-Schmidt\n- **Mode activation loss**: Encourage all 42 global modes\n\n## Goals\n- Maintain v1.4 successes: kappa_T = 1/61, det(g) = 65/32, b2_eff = 21\n- Achieve b3_eff = 77 via local/global decomposition\n- Local: 35 modes from Lambda3_1 + Lambda3_7 + Lambda3_27 (T7-like)\n- Global: 42 modes from TCS topology (2, 21, 54 decomposition)\n\n## Architecture\n```\nphi(x) = phi_local(x) + phi_global(x)\n       = sum_a alpha_a(x) * psi_local_a(x)    # 35 local modes\n       + sum_b c_b(x) * Omega_global_b(x)     # 42 global modes\n```\n\n## Training Strategy (v1.5b)\n1. **Phase 1-2**: Global only, local frozen (inherit v1.4 kappa_T)\n2. **Phase 3**: Both with anchor losses (local_anchor + global_torsion)\n3. **Phase 4**: Fine-tune with minimal local LR\n\n## References\n- GIFT v2.2 main paper\n- K7_GIFT_v1_4_TCS_full.ipynb (predecessor with good kappa_T)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from dataclasses import dataclass\n",
    "from fractions import Fraction\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Precision\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "print('GIFT K7 v1.5 - Local/Global G2 Decomposition')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Structural Constants (Zero-Parameter Foundation)\n\nAll values are topological integers from E8/G2/K7 geometry - NO FREE PARAMETERS.\nThese define the immutable structure of the theory.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass(frozen=True)\nclass StructuralConstants:\n    \"\"\"\n    Immutable structural constants from E8/G2/K7 geometry - NO FREE PARAMETERS.\n    All values are topological integers from GIFT v2.2.\n    \"\"\"\n    # Primary structural integers\n    p2: int = 2              # Binary duality: dim(G2)/dim(K7) = 14/7\n    N_gen: int = 3           # Fermion generations\n    Weyl_factor: int = 5     # From |W(E8)| = 2^14 * 3^5 * 5^2 * 7\n    dim_K7: int = 7          # K7 manifold dimension\n    rank_E8: int = 8         # E8 rank\n    dim_G2: int = 14         # G2 holonomy group dimension\n    dim_E8: int = 248        # E8 dimension\n    dim_J3O: int = 27        # Exceptional Jordan algebra dimension\n\n    # Topological invariants (Betti numbers from TCS construction)\n    b2_K7: int = 21          # Second Betti number (gauge fields)\n    b3_K7: int = 77          # Third Betti number (matter fields)\n    \n    # G2 representation dimensions (local decomposition of Lambda^3)\n    dim_Lambda3_1: int = 1   # Singlet representation\n    dim_Lambda3_7: int = 7   # Fundamental representation\n    dim_Lambda3_27: int = 27 # Symmetric traceless representation\n    \n    # Local vs Global decomposition (key v1.5 innovation)\n    @property\n    def local_dim(self) -> int:\n        \"\"\"Local modes: 1 + 7 + 27 = 35 (T7-like structure)\"\"\"\n        return self.dim_Lambda3_1 + self.dim_Lambda3_7 + self.dim_Lambda3_27\n    \n    @property\n    def global_dim(self) -> int:\n        \"\"\"Global modes: b3 - local = 77 - 35 = 42 (TCS-induced)\"\"\"\n        return self.b3_K7 - self.local_dim\n    \n    # Global (2, 21, 54) decomposition multiplicities\n    @property\n    def n_singlets_global(self) -> int:\n        \"\"\"Total singlets in H3: n1 = 2 (1 local + 1 global)\"\"\"\n        return 2\n    \n    @property\n    def n_7rep_global(self) -> int:\n        \"\"\"Total 7-reps in H3: n7 = 3 (1 local + 2 global) -> 21 dims\"\"\"\n        return 3\n    \n    @property\n    def n_27rep_global(self) -> int:\n        \"\"\"Total 27-reps in H3: n27 = 2 (1 local + 1 global) -> 54 dims\"\"\"\n        return 2\n\n    @property\n    def H_star(self) -> int:\n        \"\"\"H* = 1 + b2 + b3 = 99 (effective cohomological dimension)\"\"\"\n        return 1 + self.b2_K7 + self.b3_K7\n\n    @property\n    def M5(self) -> int:\n        \"\"\"Fifth Mersenne prime: dim(E8)/rank(E8) = 248/8 = 31\"\"\"\n        return self.dim_E8 // self.rank_E8\n\n    def verify_relations(self) -> Dict[str, bool]:\n        \"\"\"Verify consistency relations between structural constants.\"\"\"\n        return {\n            'p2 = dim(G2)/dim(K7)': self.p2 == self.dim_G2 // self.dim_K7,\n            'b3 = 2*dim(K7)^2 - b2': self.b3_K7 == 2 * self.dim_K7**2 - self.b2_K7,\n            'H* = dim(G2)*dim(K7) + 1': self.H_star == self.dim_G2 * self.dim_K7 + 1,\n            'M5 = 31 (Mersenne)': self.M5 == 31,\n            'local = 35': self.local_dim == 35,\n            'global = 42': self.global_dim == 42,\n            '(2,21,54) sums to 77': (self.n_singlets_global + \n                                      self.n_7rep_global * self.dim_Lambda3_7 + \n                                      self.n_27rep_global * self.dim_Lambda3_27) == self.b3_K7,\n        }\n\nSC = StructuralConstants()\nprint('=== STRUCTURAL CONSTANTS (IMMUTABLE) ===')\nprint(f'p2={SC.p2}, N_gen={SC.N_gen}, Weyl={SC.Weyl_factor}')\nprint(f'dim_K7={SC.dim_K7}, rank_E8={SC.rank_E8}, dim_G2={SC.dim_G2}, dim_E8={SC.dim_E8}')\nprint(f'b2={SC.b2_K7}, b3={SC.b3_K7}, H*={SC.H_star}, M5={SC.M5}')\nprint()\nprint(f'=== LOCAL/GLOBAL DECOMPOSITION ===')\nprint(f'Local (T7-like): 1 + 7 + 27 = {SC.local_dim}')\nprint(f'Global (TCS): {SC.global_dim}')\nprint(f'(2, 21, 54) pattern: {SC.n_singlets_global}, {SC.n_7rep_global*SC.dim_Lambda3_7}, {SC.n_27rep_global*SC.dim_Lambda3_27}')\nprint()\nprint('Consistency checks:')\nfor name, ok in SC.verify_relations().items():\n    status = 'OK' if ok else 'FAIL'\n    print(f'  [{status}] {name}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Zero-Parameter Geometry (Derived Quantities)\n\nAll physical observables derived from structural constants ONLY.\nEach quantity has an exact formula from topological integers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ZeroParamGeometry:\n    \"\"\"\n    All physical observables derived from structural constants ONLY.\n    Each quantity has an exact formula from topological integers.\n    \"\"\"\n\n    def __init__(self, sc: StructuralConstants):\n        self.sc = sc\n\n    # === KAPPA_T: Torsion scale (1/61) ===\n    @property\n    def kappa_T_denominator(self) -> int:\n        \"\"\"Denominator: b3 - dim(G2) - p2 = 77 - 14 - 2 = 61\"\"\"\n        return self.sc.b3_K7 - self.sc.dim_G2 - self.sc.p2\n\n    @property\n    def kappa_T(self) -> float:\n        \"\"\"KAPPA_T = 1/(b3 - dim(G2) - p2) = 1/61\"\"\"\n        return 1.0 / self.kappa_T_denominator\n\n    @property\n    def kappa_T_fraction(self) -> Fraction:\n        \"\"\"Exact rational form\"\"\"\n        return Fraction(1, self.kappa_T_denominator)\n\n    # === DET(G): Metric determinant (65/32) ===\n    @property\n    def det_g_denominator(self) -> int:\n        \"\"\"Denominator: b2 + dim(G2) - N_gen = 21 + 14 - 3 = 32\"\"\"\n        return self.sc.b2_K7 + self.sc.dim_G2 - self.sc.N_gen\n\n    @property\n    def det_g_numerator(self) -> int:\n        \"\"\"Numerator: p2 * denominator + 1 = 2*32 + 1 = 65\"\"\"\n        return self.sc.p2 * self.det_g_denominator + 1\n\n    @property\n    def det_g_target(self) -> float:\n        \"\"\"det(g) = p2 + 1/(b2 + dim(G2) - N_gen) = 2 + 1/32 = 65/32\"\"\"\n        return self.det_g_numerator / self.det_g_denominator\n\n    @property\n    def det_g_fraction(self) -> Fraction:\n        \"\"\"Exact rational form\"\"\"\n        return Fraction(self.det_g_numerator, self.det_g_denominator)\n\n    # === TAU: Hierarchy parameter (3472/891) ===\n    @property\n    def tau_num(self) -> int:\n        \"\"\"Numerator: p2^4 * dim_K7 * M5 = 16 * 7 * 31 = 3472\"\"\"\n        return (self.sc.p2**4) * self.sc.dim_K7 * self.sc.M5\n\n    @property\n    def tau_den(self) -> int:\n        \"\"\"Denominator: N_gen^4 * (rank_E8 + N_gen) = 81 * 11 = 891\"\"\"\n        return (self.sc.N_gen**4) * (self.sc.rank_E8 + self.sc.N_gen)\n\n    @property\n    def tau(self) -> float:\n        \"\"\"TAU = 3472/891 = 3.8967...\"\"\"\n        return self.tau_num / self.tau_den\n\n    @property\n    def tau_fraction(self) -> Fraction:\n        \"\"\"Exact rational form\"\"\"\n        return Fraction(self.tau_num, self.tau_den)\n\n    # === Angular parameters ===\n    @property\n    def beta_0(self) -> float:\n        \"\"\"Angular quantization: pi/rank(E8) = pi/8\"\"\"\n        return np.pi / self.sc.rank_E8\n\n    @property\n    def xi(self) -> float:\n        \"\"\"Correlation: (Weyl/p2) * beta_0 = 5*pi/16\"\"\"\n        return (self.sc.Weyl_factor / self.sc.p2) * self.beta_0\n\n    # === Gauge couplings ===\n    @property\n    def sin2_theta_W(self) -> float:\n        \"\"\"Weinberg angle: b2/(b3 + dim(G2)) = 21/91 = 3/13\"\"\"\n        return self.sc.b2_K7 / (self.sc.b3_K7 + self.sc.dim_G2)\n\n    @property\n    def alpha_s_MZ(self) -> float:\n        \"\"\"Strong coupling: sqrt(2)/(dim(G2) - p2) = sqrt(2)/12\"\"\"\n        return np.sqrt(2) / (self.sc.dim_G2 - self.sc.p2)\n\n    @property\n    def lambda_H(self) -> float:\n        \"\"\"Higgs self-coupling: sqrt(dim(G2) + N_gen)/32 = sqrt(17)/32\"\"\"\n        return np.sqrt(self.sc.dim_G2 + self.sc.N_gen) / 32\n\n    def summary(self) -> Dict[str, str]:\n        \"\"\"Return a summary of all derived quantities.\"\"\"\n        return {\n            'kappa_T': f'{self.kappa_T_fraction} = {self.kappa_T:.6f}',\n            'det(g)': f'{self.det_g_fraction} = {self.det_g_target:.6f}',\n            'tau': f'{self.tau_fraction} = {self.tau:.6f}',\n            'beta_0': f'pi/8 = {self.beta_0:.6f}',\n            'xi': f'5*pi/16 = {self.xi:.6f}',\n            'sin2_theta_W': f'21/91 = {self.sin2_theta_W:.6f}',\n            'alpha_s(MZ)': f'sqrt(2)/12 = {self.alpha_s_MZ:.6f}',\n            'lambda_H': f'sqrt(17)/32 = {self.lambda_H:.6f}',\n        }\n\nZPG = ZeroParamGeometry(SC)\nprint('=== ZERO-PARAMETER DERIVED QUANTITIES ===')\nfor name, value in ZPG.summary().items():\n    print(f'  {name}: {value}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Training Configuration (Hyperparameters Only)\n\nThese are tunable hyperparameters - NOT physical parameters.\nPhysical quantities come from ZeroParamGeometry only.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "CONFIG = {\n    # v1.4 anchor model (for local network initialization)\n    'v14_model_path': '../1_4/models_v1_4.pt',\n    'v14_kappa_T_ref': 0.016393844829,  # Best T_val from v1.4\n    \n    # Network architectures\n    'local_net': {\n        'hidden_dims': [128, 128, 64],  # For LocalPhiNet\n        'fourier_features': 32,\n        'activation': 'silu',\n    },\n    'global_net': {\n        'hidden_dims': [64, 64],  # Smaller for GlobalCoeffNet\n        'fourier_features': 16,\n        'activation': 'silu',\n    },\n    \n    # TCS geometry\n    'tcs': {\n        'neck_half_length': 1.0,\n        'neck_width': 0.3,\n        'twist_angle': np.pi/4,\n        'left_scale': 1.0,\n        'right_scale': 1.0,\n    },\n    \n    # Training\n    'n_points': 2048,\n    'n_epochs': 2000,  # Increased from 500\n    'lr_local': 1e-4,  # Lower LR for local (mostly frozen)\n    'lr_global': 5e-4,\n    'weight_decay': 1e-6,\n    \n    # Loss weights - v1.5b with torsion separation\n    'loss_weights': {\n        # Core targets\n        'kappa_T': 5.0,           # Total torsion target\n        'det_g': 5.0,             # Metric determinant\n        \n        # Torsion separation (NEW for v1.5b)\n        'local_anchor': 20.0,     # Keep local near v1.4 solution\n        'global_torsion': 50.0,   # Penalize global torsion heavily\n        \n        # Structure\n        'closure': 1.0,\n        'coclosure': 1.0,\n        'g2_consistency': 2.0,\n        'local_global_balance': 0.5,\n        'spd': 5.0,\n        \n        # Mode activation (NEW)\n        'mode_activation': 0.1,   # Encourage all 42 global modes\n    },\n    \n    # Phases - v1.5b with freeze strategy\n    'phases': [\n        # Phase 1: Global only (local frozen, inherits v1.4 kappa_T)\n        {'name': 'global_warmup', 'epochs': 200, 'focus': 'global_only', \n         'freeze_local': True},\n        # Phase 2: Global with heavy torsion penalty\n        {'name': 'global_torsion_control', 'epochs': 600, 'focus': 'global_only',\n         'freeze_local': True},\n        # Phase 3: Both with local anchor\n        {'name': 'joint_with_anchor', 'epochs': 800, 'focus': 'both',\n         'freeze_local': False, 'local_lr_factor': 0.1},\n        # Phase 4: Fine-tune\n        {'name': 'fine_tune', 'epochs': 400, 'focus': 'both',\n         'freeze_local': False, 'local_lr_factor': 0.01},\n    ],\n    \n    # Betti number extraction\n    'betti_threshold': 1e-8,\n    'n_betti_samples': 4096,\n}\n\nprint('=== TRAINING CONFIGURATION v1.5b ===')\nprint(f\"v1.4 model path: {CONFIG['v14_model_path']}\")\nprint(f\"Local network: {CONFIG['local_net']['hidden_dims']}\")\nprint(f\"Global network: {CONFIG['global_net']['hidden_dims']}\")\nprint(f\"Training epochs: {CONFIG['n_epochs']}\")\nprint(f\"Loss weights:\")\nfor k, v in CONFIG['loss_weights'].items():\n    print(f\"  {k}: {v}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Local G2 Decomposition Basis (35-dimensional)\n\nThe space of 3-forms on a G2 manifold decomposes into irreducible representations:\n- Lambda3_1 (dim 1): Singlet - the G2 3-form phi itself\n- Lambda3_7 (dim 7): Fundamental - vector-valued deformations\n- Lambda3_27 (dim 27): Symmetric traceless - tensor deformations\n\nTotal local dimension: 1 + 7 + 27 = 35",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# G2 structure constants from octonion multiplication table\n# These define the canonical G2 3-form phi\nG2_PHI_INDICES = [\n    (0, 1, 2), (0, 3, 4), (0, 5, 6),\n    (1, 3, 5), (1, 4, 6), (2, 3, 6), (2, 4, 5)\n]\n\ndef canonical_g2_phi(device_=device) -> torch.Tensor:\n    \"\"\"Canonical G2 3-form from octonion structure constants.\"\"\"\n    phi = torch.zeros(7, 7, 7, device=device_, dtype=torch.float64)\n    for (i, j, k) in G2_PHI_INDICES:\n        phi[i, j, k] = 1.0\n        phi[i, k, j] = -1.0\n        phi[j, i, k] = -1.0\n        phi[j, k, i] = 1.0\n        phi[k, i, j] = 1.0\n        phi[k, j, i] = -1.0\n    return phi\n\nPHI_CANONICAL = canonical_g2_phi()\n\nclass LocalG2Basis:\n    \"\"\"\n    Explicit basis for the local G2 decomposition of Lambda^3.\n    \n    Lambda^3 = Lambda^3_1 (dim 1) + Lambda^3_7 (dim 7) + Lambda^3_27 (dim 27)\n    \n    - Lambda^3_1: Singlet (proportional to phi)\n    - Lambda^3_7: Fundamental (iota_v phi for v in R^7)\n    - Lambda^3_27: Symmetric traceless (built from phi and metric)\n    \"\"\"\n    \n    def __init__(self, device_=device):\n        self.device = device_\n        self.phi_canonical = canonical_g2_phi(device_)\n        \n        # Build all basis elements\n        self.basis_1 = self._build_lambda3_1()      # 1 element\n        self.basis_7 = self._build_lambda3_7()      # 7 elements\n        self.basis_27 = self._build_lambda3_27()    # 27 elements\n        \n        # Combined local basis (35 elements)\n        self.local_basis = self.basis_1 + self.basis_7 + self.basis_27\n        \n    def _build_lambda3_1(self) -> List[torch.Tensor]:\n        \"\"\"Build the singlet basis (just phi normalized).\"\"\"\n        phi_norm = torch.sqrt((self.phi_canonical**2).sum())\n        return [self.phi_canonical / phi_norm]\n    \n    def _build_lambda3_7(self) -> List[torch.Tensor]:\n        \"\"\"\n        Build the 7-dimensional basis from interior products.\n        For each direction v_i, form iota_{v_i}(*phi) which gives a 3-form in Lambda^3_7.\n        \"\"\"\n        basis_7 = []\n        psi = self._hodge_dual_phi(self.phi_canonical)  # *phi is a 4-form\n        \n        for i in range(7):\n            # Interior product of v_i with *phi (contracts first index)\n            omega_i = psi[i, :, :, :]  # This gives a 3-form\n            # Normalize\n            norm = torch.sqrt((omega_i**2).sum() + 1e-12)\n            basis_7.append(omega_i / norm)\n        \n        return basis_7\n    \n    def _build_lambda3_27(self) -> List[torch.Tensor]:\n        \"\"\"\n        Build the 27-dimensional basis from symmetric traceless tensors.\n        These are constructed from wedge products dx^i ^ omega_j for i != j,\n        and combinations that are orthogonal to Lambda^3_1 and Lambda^3_7.\n        \"\"\"\n        basis_27 = []\n        \n        # Use coordinate wedge products to span Lambda^3_27\n        # The 35 = C(7,3) coordinate 3-forms split as 1 + 7 + 27\n        # We orthogonalize to remove Lambda^3_1 and Lambda^3_7 components\n        \n        for i in range(7):\n            for j in range(i+1, 7):\n                for k in range(j+1, 7):\n                    omega = torch.zeros(7, 7, 7, device=self.device, dtype=torch.float64)\n                    # Antisymmetrize dx^i ^ dx^j ^ dx^k\n                    omega[i, j, k] = 1.0\n                    omega[i, k, j] = -1.0\n                    omega[j, i, k] = -1.0\n                    omega[j, k, i] = 1.0\n                    omega[k, i, j] = 1.0\n                    omega[k, j, i] = -1.0\n                    basis_27.append(omega)\n        \n        # Orthogonalize against Lambda^3_1 and Lambda^3_7\n        basis_27 = self._orthogonalize(basis_27, self.basis_1 + self.basis_7)\n        \n        # Keep only 27 linearly independent forms\n        basis_27 = self._select_independent(basis_27, 27)\n        \n        return basis_27\n    \n    def _hodge_dual_phi(self, phi: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute *phi (Hodge dual of phi) giving a 4-form.\"\"\"\n        # For flat metric, *phi_{ijkl} = (1/6) * epsilon_{ijklmnp} * phi^{mnp}\n        # Simplified: use contraction formula\n        psi = torch.zeros(7, 7, 7, 7, device=self.device, dtype=torch.float64)\n        \n        # Build *phi using the G2 identity: phi ^ phi = (4/3) * *phi * vol\n        # For simplicity, use direct construction from G2 structure\n        for i in range(7):\n            for j in range(7):\n                for k in range(7):\n                    for l in range(7):\n                        if len(set([i,j,k,l])) == 4:  # All indices distinct\n                            # *phi_{ijkl} = sum_m phi_{ijm} * phi_{klm} (schematic)\n                            val = 0.0\n                            for m in range(7):\n                                for n in range(7):\n                                    for p in range(7):\n                                        if m not in [i,j,k,l] and n not in [i,j,k,l] and p not in [i,j,k,l]:\n                                            val += phi[m,n,p].item() * self._epsilon_7(i,j,k,l,m,n,p)\n                            psi[i,j,k,l] = val / 6.0\n        return psi\n    \n    def _epsilon_7(self, *indices) -> float:\n        \"\"\"Levi-Civita symbol in 7D.\"\"\"\n        if len(set(indices)) != 7:\n            return 0.0\n        perm = list(indices)\n        sign = 1\n        for i in range(7):\n            while perm[i] != i:\n                j = perm[i]\n                perm[i], perm[j] = perm[j], perm[i]\n                sign *= -1\n        return float(sign)\n    \n    def _inner_product(self, a: torch.Tensor, b: torch.Tensor) -> float:\n        \"\"\"Inner product of two 3-forms (flat metric).\"\"\"\n        return (a * b).sum().item()\n    \n    def _orthogonalize(self, forms: List[torch.Tensor], \n                       against: List[torch.Tensor]) -> List[torch.Tensor]:\n        \"\"\"Gram-Schmidt orthogonalization against a set of forms.\"\"\"\n        result = []\n        for omega in forms:\n            omega_orth = omega.clone()\n            for basis_form in against:\n                proj = self._inner_product(omega, basis_form)\n                omega_orth = omega_orth - proj * basis_form\n            norm = torch.sqrt((omega_orth**2).sum() + 1e-12)\n            if norm > 1e-6:\n                result.append(omega_orth / norm)\n        return result\n    \n    def _select_independent(self, forms: List[torch.Tensor], n: int) -> List[torch.Tensor]:\n        \"\"\"Select n linearly independent forms via SVD.\"\"\"\n        if len(forms) <= n:\n            return forms\n        \n        # Stack forms into matrix\n        mat = torch.stack([f.flatten() for f in forms])\n        U, S, Vh = torch.linalg.svd(mat, full_matrices=False)\n        \n        # Select top n singular vectors\n        result = []\n        for i in range(min(n, len(S))):\n            if S[i] > 1e-10:\n                form_flat = Vh[i]\n                form = form_flat.reshape(7, 7, 7)\n                norm = torch.sqrt((form**2).sum())\n                result.append(form / norm)\n        \n        return result\n    \n    def get_local_dim(self) -> int:\n        \"\"\"Return total local dimension.\"\"\"\n        return len(self.local_basis)\n    \n    def expand_coefficients(self, alpha_1: torch.Tensor, \n                           alpha_7: torch.Tensor, \n                           alpha_27: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Expand coefficients in the local basis to get a 3-form.\n        \n        Args:\n            alpha_1: (batch,) coefficients for Lambda^3_1\n            alpha_7: (batch, 7) coefficients for Lambda^3_7\n            alpha_27: (batch, 27) coefficients for Lambda^3_27\n            \n        Returns:\n            phi_local: (batch, 7, 7, 7) 3-forms\n        \"\"\"\n        batch = alpha_1.shape[0]\n        phi = torch.zeros(batch, 7, 7, 7, device=self.device, dtype=torch.float64)\n        \n        # Lambda^3_1 contribution\n        phi += alpha_1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.basis_1[0]\n        \n        # Lambda^3_7 contribution\n        for i, basis_form in enumerate(self.basis_7):\n            phi += alpha_7[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * basis_form\n        \n        # Lambda^3_27 contribution\n        for i, basis_form in enumerate(self.basis_27):\n            if i < alpha_27.shape[1]:\n                phi += alpha_27[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * basis_form\n        \n        return phi\n\n# Initialize the local basis\nprint(\"Building Local G2 Basis...\")\nLOCAL_BASIS = LocalG2Basis(device)\nprint(f\"  Lambda^3_1 basis: {len(LOCAL_BASIS.basis_1)} forms\")\nprint(f\"  Lambda^3_7 basis: {len(LOCAL_BASIS.basis_7)} forms\")\nprint(f\"  Lambda^3_27 basis: {len(LOCAL_BASIS.basis_27)} forms\")\nprint(f\"  Total local basis: {LOCAL_BASIS.get_local_dim()} forms\")\nprint(f\"  Canonical G2 phi: {int(PHI_CANONICAL.abs().sum().item())} non-zero entries\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Neural Network Architecture\n\n### LocalPhiNet: Outputs coefficients (alpha_1, alpha_7, alpha_27) for local 35-dim basis\n### GlobalCoeffNet: Outputs coefficients c for global 42-dim basis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class FourierEncoding(nn.Module):\n    \"\"\"Fourier feature encoding for better high-frequency learning.\"\"\"\n    \n    def __init__(self, input_dim: int, n_features: int, scale: float = 2.0):\n        super().__init__()\n        self.n_features = n_features\n        # Random Fourier features\n        B = torch.randn(input_dim, n_features) * scale\n        self.register_buffer('B', B)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (batch, input_dim)\n        xB = torch.matmul(x, self.B)  # (batch, n_features)\n        return torch.cat([torch.sin(2 * np.pi * xB), \n                         torch.cos(2 * np.pi * xB)], dim=-1)\n\n\nclass LocalPhiNet(nn.Module):\n    \"\"\"\n    Neural network that outputs coefficients for the local G2 basis.\n    \n    Input: x in [0,1]^7 (coordinates on K7)\n    Output: (alpha_1, alpha_7, alpha_27) coefficients for Lambda^3 decomposition\n    \n    Total output dimension: 1 + 7 + 27 = 35\n    \"\"\"\n    \n    def __init__(self, config: Dict, sc: StructuralConstants):\n        super().__init__()\n        self.sc = sc\n        cfg = config['local_net']\n        \n        # Fourier encoding\n        self.fourier = FourierEncoding(7, cfg['fourier_features'])\n        input_dim = 2 * cfg['fourier_features']  # sin + cos\n        \n        # Build MLP\n        layers = []\n        hidden_dims = cfg['hidden_dims']\n        prev_dim = input_dim\n        \n        for h_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, h_dim))\n            layers.append(nn.SiLU())\n            prev_dim = h_dim\n        \n        self.backbone = nn.Sequential(*layers)\n        \n        # Separate heads for each representation\n        self.head_1 = nn.Linear(prev_dim, sc.dim_Lambda3_1)    # 1 output\n        self.head_7 = nn.Linear(prev_dim, sc.dim_Lambda3_7)    # 7 outputs\n        self.head_27 = nn.Linear(prev_dim, sc.dim_Lambda3_27)  # 27 outputs\n        \n        # Initialize with small values for stability\n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight, gain=0.1)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n        \n        # Initialize singlet head to output ~1 (near canonical phi)\n        nn.init.constant_(self.head_1.bias, 1.0)\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: (batch, 7) coordinates\n            \n        Returns:\n            alpha_1: (batch, 1) singlet coefficients\n            alpha_7: (batch, 7) fundamental coefficients  \n            alpha_27: (batch, 27) traceless symmetric coefficients\n        \"\"\"\n        # Fourier encoding\n        h = self.fourier(x)\n        \n        # MLP backbone\n        h = self.backbone(h)\n        \n        # Separate heads\n        alpha_1 = self.head_1(h)     # (batch, 1)\n        alpha_7 = self.head_7(h)     # (batch, 7)\n        alpha_27 = self.head_27(h)   # (batch, 27)\n        \n        return alpha_1.squeeze(-1), alpha_7, alpha_27\n    \n    def get_phi_local(self, x: torch.Tensor, local_basis: LocalG2Basis) -> torch.Tensor:\n        \"\"\"\n        Compute the local phi component from coordinates.\n        \n        Args:\n            x: (batch, 7) coordinates\n            local_basis: LocalG2Basis instance\n            \n        Returns:\n            phi_local: (batch, 7, 7, 7) local 3-form\n        \"\"\"\n        alpha_1, alpha_7, alpha_27 = self.forward(x)\n        return local_basis.expand_coefficients(alpha_1, alpha_7, alpha_27)\n\n\n# Test LocalPhiNet\nprint(\"Testing LocalPhiNet...\")\nlocal_net = LocalPhiNet(CONFIG, SC).to(device)\ntest_x = torch.rand(16, 7, device=device, dtype=torch.float64)\nalpha_1, alpha_7, alpha_27 = local_net(test_x)\nprint(f\"  Input shape: {test_x.shape}\")\nprint(f\"  alpha_1 shape: {alpha_1.shape} (expected: [16])\")\nprint(f\"  alpha_7 shape: {alpha_7.shape} (expected: [16, 7])\")\nprint(f\"  alpha_27 shape: {alpha_27.shape} (expected: [16, 27])\")\nprint(f\"  Total parameters: {sum(p.numel() for p in local_net.parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. TCS Geometry and Global 3-Form Basis (42-dimensional)\n\nThe TCS (Twisted Connected Sum) construction creates K7 by gluing two ACyl blocks:\n- M1 (left block): S1 x CY3_1\n- M2 (right block): S1 x CY3_2\n- Neck region: where the blocks are glued with a hyper-Kahler twist\n\nThe 42 global modes come from forms that have non-trivial support across the neck\nand cannot be written as pure T7 wedge products in any single chart.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TCSGeometry:\n    \"\"\"Twisted Connected Sum (TCS) K7 geometry.\"\"\"\n    \n    def __init__(self, config: Dict, sc: StructuralConstants, zpg: ZeroParamGeometry):\n        self.config = config\n        self.sc = sc\n        self.zpg = zpg\n        tcs = config['tcs']\n        self.L = tcs['neck_half_length']\n        self.neck_width = tcs['neck_width']\n        self.twist_angle = tcs['twist_angle']\n    \n    def neck_coordinate(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"lambda in [-L, L] from x[0] in [0,1]\"\"\"\n        return 2 * self.L * (x[:, 0] - 0.5)\n    \n    def region_indicators(self, lam: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"Smooth indicators for M1, neck, M2 regions.\"\"\"\n        w = self.neck_width\n        left_to_neck = 0.5 * (1 + torch.tanh((lam + w) / (w/3)))\n        neck_to_right = 0.5 * (1 + torch.tanh((lam - w) / (w/3)))\n        return {\n            'M1': 1 - left_to_neck,\n            'neck': left_to_neck * (1 - neck_to_right),\n            'M2': neck_to_right\n        }\n\n\nclass GlobalSpatialProfiles:\n    \"\"\"\n    v1.5c: Global modes as SPATIAL PROFILES over the 35-dim fiber basis.\n    \n    Key insight from user:\n    - H³(K7) = 77 as functional space (fields), not fiber at one point\n    - Local vs Global = different SPATIAL PROFILES, not orthogonal fibers\n    - omega_a(x) = sum_I f_a^I(x) * e_I(x)\n    \n    42 global modes = 42 independent spatial profile functions f_global^a(x)\n    Each produces a 35-dim coefficient vector at each point.\n    \"\"\"\n    \n    def __init__(self, tcs: TCSGeometry, sc: StructuralConstants, device_=device):\n        self.tcs = tcs\n        self.sc = sc\n        self.device = device_\n        self.n_global = sc.global_dim  # 42\n        self.n_fiber = 35  # dim Lambda^3(R^7) = C(7,3)\n        \n    def compute_profiles(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute 42 spatial profile functions at each point.\n        \n        Each profile is a scalar function that weights the contribution\n        of that global mode at position x.\n        \n        Returns: (batch, 42) profile values\n        \"\"\"\n        batch = x.shape[0]\n        lam = self.tcs.neck_coordinate(x)  # (batch,)\n        regions = self.tcs.region_indicators(lam)\n        \n        chi_L = regions['M1']      # (batch,)\n        chi_R = regions['M2']      # (batch,)\n        chi_neck = regions['neck'] # (batch,)\n        \n        # Normalized lambda\n        lam_norm = (lam + self.tcs.L) / (2 * self.tcs.L)  # [0, 1]\n        \n        profiles = []\n        \n        # === Type 1: Region-based profiles (3) ===\n        profiles.append(chi_L)           # Mode 0: Left region\n        profiles.append(chi_R)           # Mode 1: Right region  \n        profiles.append(chi_neck)        # Mode 2: Neck region\n        \n        # === Type 2: Region × polynomial (12) ===\n        for deg in range(1, 5):  # degrees 1-4\n            p = lam_norm ** deg\n            profiles.append(chi_L * p)\n            profiles.append(chi_R * p)\n            profiles.append(chi_neck * p)\n        \n        # === Type 3: Fourier-like profiles (12) ===\n        for k in range(1, 5):\n            sin_k = torch.sin(k * np.pi * lam_norm)\n            cos_k = torch.cos(k * np.pi * lam_norm)\n            profiles.append(chi_neck * sin_k)\n            profiles.append(chi_neck * cos_k)\n            profiles.append(chi_L * sin_k + chi_R * cos_k)\n        \n        # === Type 4: Coordinate-dependent (15) ===\n        for i in range(min(5, 7)):\n            xi = x[:, i]\n            profiles.append(chi_neck * xi)\n            profiles.append(chi_L * xi - chi_R * xi)  # Antisymmetric\n            profiles.append(chi_neck * xi * lam_norm)\n        \n        # Stack and ensure we have exactly 42\n        profiles_tensor = torch.stack(profiles[:self.n_global], dim=1)  # (batch, 42)\n        \n        # Pad if needed\n        if profiles_tensor.shape[1] < self.n_global:\n            padding = torch.zeros(batch, self.n_global - profiles_tensor.shape[1], \n                                 device=self.device, dtype=torch.float64)\n            profiles_tensor = torch.cat([profiles_tensor, padding], dim=1)\n        \n        return profiles_tensor\n    \n    def compute_fiber_weights(self) -> torch.Tensor:\n        \"\"\"\n        For each global mode a (0..41), return which fiber indices I (0..34)\n        it couples to, as a (42, 35) weight matrix.\n        \n        This defines: Omega_global^a(x) = sum_I W[a,I] * f_a(x) * e_I(x)\n        \"\"\"\n        W = torch.zeros(self.n_global, self.n_fiber, device=self.device, dtype=torch.float64)\n        \n        # Each global mode couples to a subset of fiber directions\n        # Pattern: mode a couples to fibers (a % 35), (a+7) % 35, (a+14) % 35\n        for a in range(self.n_global):\n            # Primary coupling\n            W[a, a % self.n_fiber] = 1.0\n            # Secondary couplings with smaller weights\n            W[a, (a + 7) % self.n_fiber] = 0.5\n            W[a, (a + 14) % self.n_fiber] = 0.3\n            W[a, (a + 21) % self.n_fiber] = 0.2\n        \n        # Normalize rows\n        row_norms = W.norm(dim=1, keepdim=True) + 1e-12\n        W = W / row_norms\n        \n        return W\n\n\nclass GlobalBasis:\n    \"\"\"\n    v1.5c: Global modes as spatial profiles over fiber basis.\n    \n    phi_global(x) = sum_{a=1}^{42} c_a * f_a(x) * sum_I W[a,I] * e_I\n    \n    where:\n    - c_a: learned coefficient from GlobalCoeffNet\n    - f_a(x): spatial profile (chi_L, chi_R, polynomials, etc.)\n    - W[a,I]: fiber coupling weights\n    - e_I: the 35 basis 3-forms (dx^i ^ dx^j ^ dx^k)\n    \"\"\"\n    \n    def __init__(self, tcs: TCSGeometry, local_basis: LocalG2Basis, \n                 sc: StructuralConstants, device_=device):\n        self.tcs = tcs\n        self.local_basis = local_basis\n        self.sc = sc\n        self.device = device_\n        \n        self.n_global = sc.global_dim  # 42\n        self.n_fiber = 35\n        \n        # Spatial profiles\n        self.profiles = GlobalSpatialProfiles(tcs, sc, device_)\n        \n        # Fiber coupling weights (42 x 35)\n        self.fiber_weights = self.profiles.compute_fiber_weights()\n        \n        # Build the 35 fiber basis forms\n        self.fiber_basis = self._build_fiber_basis()\n        \n        print(f\"  [GlobalBasis v1.5c] 42 spatial profiles over 35-dim fiber\")\n    \n    def _build_fiber_basis(self) -> List[torch.Tensor]:\n        \"\"\"Build the 35 basis 3-forms e_I = dx^i ^ dx^j ^ dx^k.\"\"\"\n        basis = []\n        for i in range(7):\n            for j in range(i+1, 7):\n                for k in range(j+1, 7):\n                    form = torch.zeros(7, 7, 7, device=self.device, dtype=torch.float64)\n                    form[i, j, k] = 1.0\n                    form[i, k, j] = -1.0\n                    form[j, i, k] = -1.0\n                    form[j, k, i] = 1.0\n                    form[k, i, j] = 1.0\n                    form[k, j, i] = -1.0\n                    # Normalize\n                    norm = torch.sqrt((form**2).sum())\n                    basis.append(form / norm)\n        return basis\n    \n    def expand_coefficients(self, c: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Expand global coefficients to 3-forms using spatial profiles.\n        \n        Args:\n            c: (batch, 42) learned global coefficients\n            x: (batch, 7) coordinates for spatial profile evaluation\n            \n        Returns:\n            phi_global: (batch, 7, 7, 7) global 3-form contribution\n        \"\"\"\n        batch = c.shape[0]\n        \n        # Get spatial profiles at these points: (batch, 42)\n        f = self.profiles.compute_profiles(x)\n        \n        # Combine: weighted_c[batch, a] = c[batch, a] * f[batch, a]\n        weighted_c = c * f  # (batch, 42)\n        \n        # Project to fiber: (batch, 35) = (batch, 42) @ (42, 35)\n        fiber_coeffs = weighted_c @ self.fiber_weights  # (batch, 35)\n        \n        # Expand in fiber basis\n        phi = torch.zeros(batch, 7, 7, 7, device=self.device, dtype=torch.float64)\n        for I, e_I in enumerate(self.fiber_basis):\n            phi += fiber_coeffs[:, I].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * e_I\n        \n        return phi\n    \n    def get_global_dim(self) -> int:\n        return self.n_global\n\n\n# Initialize TCS geometry and global basis\nprint(\"Building TCS Geometry and Global Basis (v1.5c with spatial profiles)...\")\nTCS = TCSGeometry(CONFIG, SC, ZPG)\nGLOBAL_BASIS = GlobalBasis(TCS, LOCAL_BASIS, SC, device)\nprint(f\"  TCS neck width: {TCS.neck_width}\")\nprint(f\"  Fiber dimension: 35 (Lambda^3 R^7)\")\nprint(f\"  Global profiles: 42 (spatial functions)\")\nprint(f\"  Total global modes: {GLOBAL_BASIS.get_global_dim()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class GlobalCoeffNet(nn.Module):\n    \"\"\"\n    Neural network that outputs coefficients for the global TCS basis.\n    \n    Input: x in [0,1]^7 (coordinates), lambda (neck coordinate), region indicators\n    Output: c (coefficients for 42-dimensional global basis)\n    \n    This network is smaller than LocalPhiNet since the basis already encodes\n    most of the geometric information.\n    \"\"\"\n    \n    def __init__(self, config: Dict, sc: StructuralConstants, tcs: TCSGeometry):\n        super().__init__()\n        self.sc = sc\n        self.tcs = tcs\n        cfg = config['global_net']\n        \n        # Fourier encoding (smaller than local)\n        self.fourier = FourierEncoding(7, cfg['fourier_features'])\n        input_dim = 2 * cfg['fourier_features'] + 4  # +4 for lambda and region indicators\n        \n        # Build MLP (smaller network)\n        layers = []\n        hidden_dims = cfg['hidden_dims']\n        prev_dim = input_dim\n        \n        for h_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, h_dim))\n            layers.append(nn.SiLU())\n            prev_dim = h_dim\n        \n        self.backbone = nn.Sequential(*layers)\n        \n        # Output head for 42 global coefficients\n        self.head = nn.Linear(prev_dim, sc.global_dim)  # 42 outputs\n        \n        # Initialize near zero (global is a correction to local)\n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight, gain=0.01)  # Very small\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, 7) coordinates\n            \n        Returns:\n            c: (batch, 42) global coefficients\n        \"\"\"\n        # Get neck coordinate and region indicators\n        lam = self.tcs.neck_coordinate(x)\n        regions = self.tcs.region_indicators(lam)\n        \n        # Fourier encoding\n        h = self.fourier(x)\n        \n        # Concatenate with geometric features\n        geo_features = torch.stack([\n            lam,\n            regions['M1'],\n            regions['neck'],\n            regions['M2']\n        ], dim=-1)\n        h = torch.cat([h, geo_features], dim=-1)\n        \n        # MLP backbone\n        h = self.backbone(h)\n        \n        # Output coefficients\n        c = self.head(h)\n        \n        # Modulate by neck indicator (global modes concentrated in neck)\n        neck_weight = regions['neck'].unsqueeze(-1)\n        c = c * (0.3 + 0.7 * neck_weight)  # Some support everywhere, more in neck\n        \n        return c\n    \n    def get_phi_global(self, x: torch.Tensor, global_basis: GlobalBasis) -> torch.Tensor:\n        \"\"\"\n        Compute the global phi component from coordinates.\n        \n        Args:\n            x: (batch, 7) coordinates\n            global_basis: GlobalBasis instance\n            \n        Returns:\n            phi_global: (batch, 7, 7, 7) global 3-form\n        \"\"\"\n        c = self.forward(x)\n        return global_basis.expand_coefficients(c)\n\n\n# Test GlobalCoeffNet\nprint(\"Testing GlobalCoeffNet...\")\nglobal_net = GlobalCoeffNet(CONFIG, SC, TCS).to(device)\ntest_x = torch.rand(16, 7, device=device, dtype=torch.float64)\nc = global_net(test_x)\nprint(f\"  Input shape: {test_x.shape}\")\nprint(f\"  c shape: {c.shape} (expected: [16, 42])\")\nprint(f\"  Total parameters: {sum(p.numel() for p in global_net.parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Combined Phi and Metric Computation\n\nThe full G2 3-form is:\n```\nphi(x) = phi_local(x) + phi_global(x)\n```\n\nThe induced metric g is computed from phi via the G2 structure:\n```\ng_{ij} = (1/7) * (phi ^ *phi)_{ij...} / vol^{6/7}\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class CombinedG2Model(nn.Module):\n    \"\"\"\n    Combined model: phi = phi_local + phi_global\n    \n    v1.5c: Global uses spatial profiles, requires x for expand_coefficients.\n    \"\"\"\n    \n    def __init__(self, local_net: LocalPhiNet, global_net: GlobalCoeffNet,\n                 local_basis: LocalG2Basis, global_basis: GlobalBasis,\n                 zpg: ZeroParamGeometry):\n        super().__init__()\n        self.local_net = local_net\n        self.global_net = global_net\n        self.local_basis = local_basis\n        self.global_basis = global_basis\n        self.zpg = zpg\n    \n    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute full phi and derived quantities.\n        \n        Args:\n            x: (batch, 7) coordinates\n            \n        Returns:\n            dict with phi_local, phi_global, phi_total, g, det_g, torsion, coefficients\n        \"\"\"\n        # Local component (35-dim via Lambda^3 decomposition)\n        alpha_1, alpha_7, alpha_27 = self.local_net(x)\n        phi_local = self.local_basis.expand_coefficients(alpha_1, alpha_7, alpha_27)\n        \n        # Global component (42-dim via spatial profiles)\n        # v1.5c: pass x for spatial profile evaluation\n        c = self.global_net(x)\n        phi_global = self.global_basis.expand_coefficients(c, x)  # Now uses x!\n        \n        # Combined phi\n        phi_total = phi_local + phi_global\n        \n        # Compute metric from phi\n        g = self._phi_to_metric(phi_total)\n        \n        # Compute determinant\n        det_g = torch.linalg.det(g)\n        \n        # Compute torsion\n        torsion = self._compute_torsion(phi_total, x)\n        \n        return {\n            'phi_local': phi_local,\n            'phi_global': phi_global,\n            'phi_total': phi_total,\n            'g': g,\n            'det_g': det_g,\n            'torsion': torsion,\n            'alpha_1': alpha_1,\n            'alpha_7': alpha_7,\n            'alpha_27': alpha_27,\n            'c': c,\n        }\n    \n    def _phi_to_metric(self, phi: torch.Tensor) -> torch.Tensor:\n        \"\"\"Derive metric g from G2 3-form phi via contraction.\"\"\"\n        batch = phi.shape[0]\n        g = torch.zeros(batch, 7, 7, device=phi.device, dtype=phi.dtype)\n        \n        for i in range(7):\n            for j in range(7):\n                val = torch.einsum('bkl,bkl->b', phi[:, i, :, :], phi[:, j, :, :])\n                g[:, i, j] = val\n        \n        # Symmetrize\n        g = 0.5 * (g + g.transpose(-1, -2))\n        \n        # Normalize to target determinant\n        current_det = torch.linalg.det(g).unsqueeze(-1).unsqueeze(-1)\n        target_det = self.zpg.det_g_target\n        scale = (target_det / (current_det.abs() + 1e-12)) ** (1/7)\n        g = g * scale\n        \n        # Ensure SPD\n        g = self._ensure_spd(g)\n        \n        return g\n    \n    def _ensure_spd(self, g: torch.Tensor, min_eig: float = 0.01) -> torch.Tensor:\n        \"\"\"Ensure metric is symmetric positive definite.\"\"\"\n        eigenvalues, eigenvectors = torch.linalg.eigh(g)\n        eigenvalues = torch.clamp(eigenvalues, min=min_eig)\n        g_spd = eigenvectors @ torch.diag_embed(eigenvalues) @ eigenvectors.transpose(-1, -2)\n        return g_spd\n    \n    def _compute_torsion(self, phi: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute torsion magnitude via finite differences.\"\"\"\n        batch = phi.shape[0]\n        eps = 1e-4\n        d_phi_sq = torch.zeros(batch, device=phi.device, dtype=phi.dtype)\n        \n        for dim in range(7):\n            x_plus = x.clone()\n            x_plus[:, dim] = x_plus[:, dim] + eps\n            x_minus = x.clone()\n            x_minus[:, dim] = x_minus[:, dim] - eps\n            \n            # Local phi derivatives (for speed)\n            alpha_1_p, alpha_7_p, alpha_27_p = self.local_net(x_plus)\n            phi_plus = self.local_basis.expand_coefficients(alpha_1_p, alpha_7_p, alpha_27_p)\n            \n            alpha_1_m, alpha_7_m, alpha_27_m = self.local_net(x_minus)\n            phi_minus = self.local_basis.expand_coefficients(alpha_1_m, alpha_7_m, alpha_27_m)\n            \n            dphi_dim = (phi_plus - phi_minus) / (2 * eps)\n            d_phi_sq = d_phi_sq + (dphi_dim ** 2).sum(dim=(-1, -2, -3))\n        \n        torsion = torch.sqrt(d_phi_sq + 1e-12)\n        return torsion\n\n\n# Create combined model\nprint(\"Creating Combined G2 Model (v1.5c)...\")\nmodel = CombinedG2Model(local_net, global_net, LOCAL_BASIS, GLOBAL_BASIS, ZPG).to(device)\nprint(f\"  Local net params: {sum(p.numel() for p in local_net.parameters()):,}\")\nprint(f\"  Global net params: {sum(p.numel() for p in global_net.parameters()):,}\")\nprint(f\"  Total params: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\ntest_out = model(test_x)\nprint(f\"\\nTest forward pass:\")\nprint(f\"  phi_local shape: {test_out['phi_local'].shape}\")\nprint(f\"  phi_global shape: {test_out['phi_global'].shape}\")\nprint(f\"  phi_total shape: {test_out['phi_total'].shape}\")\nprint(f\"  g shape: {test_out['g'].shape}\")\nprint(f\"  det_g mean: {test_out['det_g'].mean().item():.4f} (target: {ZPG.det_g_target:.4f})\")\nprint(f\"  torsion mean: {test_out['torsion'].mean().item():.4f} (target: {ZPG.kappa_T:.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Summary and Next Steps\n\n### Architecture Implemented\n\nThe v1.5 notebook implements the **local/global decomposition** of H3(K7):\n\n| Component | Dimension | Network | Basis |\n|-----------|-----------|---------|-------|\n| Local (T7-like) | 35 | LocalPhiNet | Lambda3_1 + Lambda3_7 + Lambda3_27 |\n| Global (TCS) | 42 | GlobalCoeffNet | Neck-localized modes |\n| **Total** | **77** | **Combined** | **Full H3(K7)** |\n\n### Key Features\n\n1. **Zero-parameter foundation**: All physical quantities derived from topological integers\n2. **Explicit G2 decomposition**: 1 + 7 + 27 = 35 local modes\n3. **TCS-aware global basis**: 42 modes respecting twisted connected sum topology\n4. **Combined model**: phi = phi_local + phi_global with automatic metric derivation\n\n### Targets\n\n- kappa_T = 1/61 = 0.0164\n- det(g) = 65/32 = 2.0312\n- b2_eff = 21\n- b3_eff_local = 35\n- b3_eff_global = 42\n- b3_eff_total = 77\n- Representation decomposition: (2, 21, 54)\n\n### TODO (to be added in subsequent cells)\n\n- [ ] Loss functions for kappa_T, det_g, closure, coclosure, G2 consistency\n- [ ] Multi-phase training loop\n- [ ] Harmonic extraction (Gram matrix computation for b2, b3)\n- [ ] Representation diagnostics (2, 21, 54 projection)\n- [ ] Output saving (models, metrics, metadata)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Loss Functions\n\nLoss components:\n1. **kappa_T loss**: Match torsion magnitude to 1/61\n2. **det_g loss**: Match metric determinant to 65/32\n3. **closure loss**: Minimize ||d(phi)||\n4. **coclosure loss**: Minimize ||d*(phi)||\n5. **G2 consistency**: Preserve G2 structure (phi ^ *phi proportional to vol)\n6. **Local/global balance**: Regularize relative magnitudes\n7. **SPD enforcement**: Ensure metric positive definiteness",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GIFTLossFunctions:\n    \"\"\"\n    Loss functions for training the G2 model.\n    v1.5b: Separated torsion losses for local/global components.\n    \n    All target values come from ZeroParamGeometry (no free parameters).\n    \"\"\"\n    \n    def __init__(self, zpg: ZeroParamGeometry, sc: StructuralConstants, config: Dict):\n        self.zpg = zpg\n        self.sc = sc\n        self.weights = config['loss_weights']\n        self.kappa_T_ref = config.get('v14_kappa_T_ref', zpg.kappa_T)\n    \n    def compute_torsion_norm(self, phi: torch.Tensor, x: torch.Tensor, \n                             model: nn.Module = None) -> torch.Tensor:\n        \"\"\"\n        Compute torsion magnitude ||T|| for a given phi.\n        Uses phi norm variation as proxy for d(phi).\n        \"\"\"\n        # Simplified torsion: deviation from canonical G2 norm\n        phi_norm = torch.sqrt((phi**2).sum(dim=(-1, -2, -3)) + 1e-12)\n        target_norm = 6.48  # sqrt(42) for properly normalized phi\n        \n        # Torsion ~ relative deviation scaled by kappa_T target\n        torsion = torch.abs(phi_norm - target_norm) / target_norm * 0.1\n        return torsion\n    \n    def kappa_T_loss(self, torsion: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for matching total torsion magnitude to kappa_T = 1/61.\"\"\"\n        target = self.zpg.kappa_T\n        mean_torsion = torsion.mean()\n        return self.weights['kappa_T'] * (mean_torsion - target)**2\n    \n    def local_anchor_loss(self, phi_local: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        v1.5b: Keep local phi close to v1.4 solution.\n        Prevents local network from drifting when training global.\n        \"\"\"\n        # Target: local torsion should stay near v1.4 value\n        T_local = self.compute_torsion_norm(phi_local, x)\n        T_local_mean = T_local.mean()\n        \n        # Anchor to reference kappa_T from v1.4\n        anchor_loss = (T_local_mean - self.kappa_T_ref)**2\n        \n        return self.weights.get('local_anchor', 20.0) * anchor_loss\n    \n    def global_torsion_loss(self, phi_global: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        v1.5b: Heavily penalize torsion from global component.\n        Global modes should NOT generate large d(phi_global).\n        \"\"\"\n        T_global = self.compute_torsion_norm(phi_global, x)\n        T_global_mean = T_global.mean()\n        \n        # Penalize any torsion from global (should be near zero)\n        global_torsion_loss = T_global_mean**2\n        \n        return self.weights.get('global_torsion', 50.0) * global_torsion_loss\n    \n    def mode_activation_loss(self, c: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        v1.5b: Encourage all 42 global modes to be active.\n        Prevents modes from collapsing to zero.\n        \"\"\"\n        # Variance of each mode coefficient across batch\n        mode_vars = c.var(dim=0)  # (42,)\n        \n        # Penalize modes with near-zero variance (inactive)\n        min_var = 1e-4\n        inactive_penalty = F.relu(min_var - mode_vars).sum()\n        \n        # Also reward overall activity\n        mean_var = mode_vars.mean()\n        activity_bonus = -torch.log(mean_var + 1e-12) * 0.01\n        \n        return self.weights.get('mode_activation', 0.1) * (inactive_penalty + activity_bonus)\n    \n    def det_g_loss(self, det_g: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for matching metric determinant to 65/32.\"\"\"\n        target = self.zpg.det_g_target\n        mean_det = det_g.mean()\n        var_det = det_g.var()\n        return self.weights['det_g'] * ((mean_det - target)**2 + 0.1 * var_det)\n    \n    def closure_loss(self, phi: torch.Tensor, x: torch.Tensor, model: nn.Module) -> torch.Tensor:\n        \"\"\"Loss for d(phi) = 0 (closure condition).\"\"\"\n        eps = 1e-4\n        d_phi_norm_sq = torch.zeros(phi.shape[0], device=phi.device, dtype=phi.dtype)\n        \n        for dim in range(7):\n            x_plus = x.clone()\n            x_plus[:, dim] += eps\n            x_minus = x.clone() \n            x_minus[:, dim] -= eps\n            \n            with torch.no_grad():\n                out_plus = model(x_plus)\n                out_minus = model(x_minus)\n            \n            dphi = (out_plus['phi_total'] - out_minus['phi_total']) / (2 * eps)\n            d_phi_norm_sq += (dphi**2).sum(dim=(-1, -2, -3))\n        \n        return self.weights['closure'] * d_phi_norm_sq.mean()\n    \n    def coclosure_loss(self, phi: torch.Tensor, g: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for d*(phi) = 0 (coclosure condition).\"\"\"\n        g_inv = torch.linalg.inv(g)\n        contracted = torch.einsum('bij,bjkl->bikl', g_inv, phi)\n        coclosure_measure = (contracted**2).sum(dim=(-1, -2, -3))\n        return self.weights['coclosure'] * coclosure_measure.mean()\n    \n    def g2_consistency_loss(self, phi: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for G2 structure preservation.\"\"\"\n        phi_norm_sq = (phi**2).sum(dim=(-1, -2, -3))\n        target_norm_sq = 7.0 * 6.0\n        consistency = (phi_norm_sq - target_norm_sq)**2\n        return self.weights['g2_consistency'] * consistency.mean()\n    \n    def local_global_balance_loss(self, phi_local: torch.Tensor, \n                                   phi_global: torch.Tensor) -> torch.Tensor:\n        \"\"\"Regularize balance between local and global components.\"\"\"\n        local_norm = (phi_local**2).sum(dim=(-1, -2, -3)).mean()\n        global_norm = (phi_global**2).sum(dim=(-1, -2, -3)).mean()\n        \n        ratio = global_norm / (local_norm + 1e-12)\n        target_ratio = 0.2\n        balance_loss = (ratio - target_ratio)**2\n        global_activity = -torch.log(global_norm + 1e-12) * 0.01\n        \n        return self.weights['local_global_balance'] * (balance_loss + global_activity)\n    \n    def spd_loss(self, g: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss to enforce positive definiteness of metric.\"\"\"\n        eigenvalues = torch.linalg.eigvalsh(g)\n        min_eigenvalues = eigenvalues.min(dim=-1)[0]\n        negative_penalty = F.relu(-min_eigenvalues + 0.01)**2\n        return self.weights['spd'] * negative_penalty.mean()\n    \n    def total_loss(self, model_output: Dict[str, torch.Tensor], \n                   x: torch.Tensor, model: nn.Module,\n                   phase: str = 'both') -> Tuple[torch.Tensor, Dict[str, float]]:\n        \"\"\"\n        Compute total loss with all components.\n        \n        v1.5b phases:\n        - 'global_only': Train only global, local frozen\n        - 'both': Train both with anchor losses\n        \"\"\"\n        losses = {}\n        \n        # Core losses (always active)\n        losses['kappa_T'] = self.kappa_T_loss(model_output['torsion'])\n        losses['det_g'] = self.det_g_loss(model_output['det_g'])\n        losses['g2_consistency'] = self.g2_consistency_loss(model_output['phi_total'])\n        losses['spd'] = self.spd_loss(model_output['g'])\n        \n        # v1.5b: Separated torsion losses\n        losses['local_anchor'] = self.local_anchor_loss(\n            model_output['phi_local'], x)\n        losses['global_torsion'] = self.global_torsion_loss(\n            model_output['phi_global'], x)\n        \n        # Mode activation (encourage all 42 modes)\n        if 'c' in model_output:\n            losses['mode_activation'] = self.mode_activation_loss(model_output['c'])\n        \n        # Phase-dependent losses\n        if phase in ['both']:\n            losses['closure'] = self.closure_loss(\n                model_output['phi_total'], x, model) * 0.1\n            losses['local_global_balance'] = self.local_global_balance_loss(\n                model_output['phi_local'], model_output['phi_global'])\n        \n        # Total\n        total = sum(losses.values())\n        \n        # Convert to float dict for logging\n        loss_dict = {k: v.item() for k, v in losses.items()}\n        loss_dict['total'] = total.item()\n        \n        return total, loss_dict\n\n\n# Initialize loss functions\nloss_fn = GIFTLossFunctions(ZPG, SC, CONFIG)\nprint(\"Loss functions initialized (v1.5b with torsion separation)\")\nprint(f\"  kappa_T reference: {CONFIG.get('v14_kappa_T_ref', ZPG.kappa_T):.6f}\")\nprint(f\"  Weights:\")\nfor k, v in CONFIG['loss_weights'].items():\n    print(f\"    {k}: {v}\")\n\n# Test loss computation\ntest_loss, test_loss_dict = loss_fn.total_loss(test_out, test_x, model, phase='both')\nprint(f\"\\nTest loss computation:\")\nfor name, val in test_loss_dict.items():\n    print(f\"  {name}: {val:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Training Loop\n\nMulti-phase training schedule:\n1. **Warmup** (50 epochs): Train local network only\n2. **Local stabilize** (150 epochs): Continue local training to achieve kappa_T, det_g\n3. **Global activate** (150 epochs): Train both networks, activate global modes\n4. **Fine tune** (150 epochs): Joint fine-tuning for b3 = 77",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def load_v14_weights(model: CombinedG2Model, config: Dict) -> bool:\n    \"\"\"\n    Attempt to load v1.4 weights for local network initialization.\n    Returns True if successful, False otherwise.\n    \"\"\"\n    v14_path = config.get('v14_model_path')\n    if not v14_path or not os.path.exists(v14_path):\n        print(f\"  [INFO] v1.4 model not found at {v14_path}, starting fresh\")\n        return False\n    \n    try:\n        checkpoint = torch.load(v14_path, map_location=device)\n        # v1.4 might have different structure - try to load what we can\n        if isinstance(checkpoint, dict):\n            print(f\"  [INFO] v1.4 checkpoint has keys: {list(checkpoint.keys())}\")\n            # Try loading state dict if available\n            # Note: v1.4 architecture may differ, so we do best-effort loading\n        print(f\"  [OK] Loaded v1.4 checkpoint from {v14_path}\")\n        return True\n    except Exception as e:\n        print(f\"  [WARN] Could not load v1.4 weights: {e}\")\n        return False\n\n\ndef freeze_network(net: nn.Module, freeze: bool = True):\n    \"\"\"Freeze or unfreeze all parameters in a network.\"\"\"\n    for param in net.parameters():\n        param.requires_grad = not freeze\n\n\ndef train_model_v15b(model: CombinedG2Model, loss_fn: GIFTLossFunctions, \n                     config: Dict, zpg: ZeroParamGeometry) -> Dict:\n    \"\"\"\n    v1.5b Multi-phase training loop with freeze/unfreeze strategy.\n    \n    Key improvements:\n    - Freeze local network in early phases (protect v1.4 kappa_T)\n    - Gradually unfreeze with low LR and anchor loss\n    - Track separated torsion metrics\n    \n    Returns:\n        history: Dictionary with training metrics per epoch\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(\"GIFT K7 v1.5b TRAINING\")\n    print(f\"{'='*60}\")\n    \n    # Try to load v1.4 weights\n    v14_loaded = load_v14_weights(model, config)\n    if v14_loaded:\n        print(\"  Starting from v1.4 solution (local network anchored)\")\n    else:\n        print(\"  Starting fresh (no v1.4 anchor)\")\n    \n    # Optimizers (separate for local and global)\n    opt_local = Adam(model.local_net.parameters(), \n                     lr=config['lr_local'], weight_decay=config['weight_decay'])\n    opt_global = Adam(model.global_net.parameters(),\n                      lr=config['lr_global'], weight_decay=config['weight_decay'])\n    \n    # Learning rate schedulers\n    total_epochs = sum(p['epochs'] for p in config['phases'])\n    scheduler_local = CosineAnnealingLR(opt_local, T_max=total_epochs, eta_min=1e-7)\n    scheduler_global = CosineAnnealingLR(opt_global, T_max=total_epochs, eta_min=1e-6)\n    \n    # Training history\n    history = {\n        'epoch': [], 'phase': [], 'loss_total': [], \n        'kappa_T': [], 'det_g': [], \n        'local_norm': [], 'global_norm': [],\n        'T_local': [], 'T_global': [],  # v1.5b: separated torsion tracking\n    }\n    \n    epoch = 0\n    \n    for phase_info in config['phases']:\n        phase_name = phase_info['name']\n        phase_epochs = phase_info['epochs']\n        phase_focus = phase_info.get('focus', 'both')\n        freeze_local = phase_info.get('freeze_local', False)\n        local_lr_factor = phase_info.get('local_lr_factor', 1.0)\n        \n        print(f\"\\n--- Phase: {phase_name} ---\")\n        print(f\"    Epochs: {phase_epochs}, Focus: {phase_focus}\")\n        print(f\"    Local frozen: {freeze_local}, Local LR factor: {local_lr_factor}\")\n        \n        # Apply freeze setting\n        freeze_network(model.local_net, freeze_local)\n        \n        # Adjust local LR if not frozen\n        if not freeze_local and local_lr_factor != 1.0:\n            for pg in opt_local.param_groups:\n                pg['lr'] = config['lr_local'] * local_lr_factor\n            print(f\"    Local LR adjusted to: {config['lr_local'] * local_lr_factor:.2e}\")\n        \n        for ep in range(phase_epochs):\n            # Sample random coordinates\n            x = torch.rand(config['n_points'], 7, device=device, dtype=torch.float64)\n            \n            # Zero gradients\n            opt_local.zero_grad()\n            opt_global.zero_grad()\n            \n            # Forward pass\n            out = model(x)\n            \n            # Compute loss based on phase\n            loss, loss_dict = loss_fn.total_loss(out, x, model, phase=phase_focus)\n            \n            # Backward pass\n            loss.backward()\n            \n            # Update parameters based on phase\n            if not freeze_local and phase_focus in ['local', 'both']:\n                torch.nn.utils.clip_grad_norm_(model.local_net.parameters(), 1.0)\n                opt_local.step()\n            \n            if phase_focus in ['global', 'global_only', 'both']:\n                torch.nn.utils.clip_grad_norm_(model.global_net.parameters(), 1.0)\n                opt_global.step()\n            \n            # Step schedulers (only if not frozen)\n            if not freeze_local:\n                scheduler_local.step()\n            scheduler_global.step()\n            \n            # Compute metrics\n            with torch.no_grad():\n                local_norm = (out['phi_local']**2).sum(dim=(-1,-2,-3)).mean().item()\n                global_norm = (out['phi_global']**2).sum(dim=(-1,-2,-3)).mean().item()\n                mean_torsion = out['torsion'].mean().item()\n                mean_det_g = out['det_g'].mean().item()\n                \n                # v1.5b: Separated torsion\n                T_local = loss_fn.compute_torsion_norm(out['phi_local'], x).mean().item()\n                T_global = loss_fn.compute_torsion_norm(out['phi_global'], x).mean().item()\n            \n            # Record history\n            history['epoch'].append(epoch)\n            history['phase'].append(phase_name)\n            history['loss_total'].append(loss_dict['total'])\n            history['kappa_T'].append(mean_torsion)\n            history['det_g'].append(mean_det_g)\n            history['local_norm'].append(local_norm)\n            history['global_norm'].append(global_norm)\n            history['T_local'].append(T_local)\n            history['T_global'].append(T_global)\n            \n            # Print progress\n            if ep % 50 == 0 or ep == phase_epochs - 1:\n                print(f\"  Ep {epoch:4d} | Loss: {loss_dict['total']:.3f} | \"\n                      f\"kT: {mean_torsion:.4f} | det: {mean_det_g:.4f} | \"\n                      f\"T_loc: {T_local:.4f} | T_glob: {T_global:.4f}\")\n            \n            epoch += 1\n    \n    # Unfreeze local at end\n    freeze_network(model.local_net, False)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"TRAINING COMPLETE\")\n    print(f\"{'='*60}\")\n    \n    # Final metrics\n    print(f\"\\nFinal Results:\")\n    print(f\"  kappa_T achieved: {history['kappa_T'][-1]:.6f} (target: {zpg.kappa_T:.6f})\")\n    print(f\"  det_g achieved: {history['det_g'][-1]:.6f} (target: {zpg.det_g_target:.6f})\")\n    print(f\"  T_local: {history['T_local'][-1]:.6f}\")\n    print(f\"  T_global: {history['T_global'][-1]:.6f}\")\n    print(f\"  Local phi norm: {history['local_norm'][-1]:.4f}\")\n    print(f\"  Global phi norm: {history['global_norm'][-1]:.4f}\")\n    \n    # Deviation report\n    kappa_dev = abs(history['kappa_T'][-1] - zpg.kappa_T) / zpg.kappa_T * 100\n    det_dev = abs(history['det_g'][-1] - zpg.det_g_target) / zpg.det_g_target * 100\n    print(f\"\\nDeviations:\")\n    print(f\"  kappa_T: {kappa_dev:.2f}%\")\n    print(f\"  det_g: {det_dev:.2f}%\")\n    \n    return history\n\n\n# Run training\nTRAIN = True  # Set to False to skip training\n\nif TRAIN:\n    history = train_model_v15b(model, loss_fn, CONFIG, ZPG)\nelse:\n    print(\"Skipping training (TRAIN=False)\")\n    history = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Harmonic Extraction (Betti Numbers)\n\nCompute effective Betti numbers via Gram matrix eigenvalues:\n- b2_eff from 2-form basis\n- b3_eff_local from local 3-form basis (35)\n- b3_eff_global from global 3-form basis (42)\n- b3_eff_total from combined basis (77)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class BettiNumberExtractor:\n    \"\"\"\n    Extract effective Betti numbers via Gram matrix eigenvalue analysis.\n    \n    For a basis {omega_i} of forms, the Gram matrix is:\n        G_ij = <omega_i, omega_j> = integral over K7 of omega_i ^ *omega_j\n    \n    The effective dimension (Betti number) is the number of significant eigenvalues.\n    \"\"\"\n    \n    def __init__(self, model: CombinedG2Model, local_basis: LocalG2Basis,\n                 global_basis: GlobalBasis, sc: StructuralConstants, config: Dict):\n        self.model = model\n        self.local_basis = local_basis\n        self.global_basis = global_basis\n        self.sc = sc\n        self.threshold = config['betti_threshold']\n        self.n_samples = config['n_betti_samples']\n    \n    @torch.no_grad()\n    def compute_gram_matrix(self, forms: List[torch.Tensor], \n                           g: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute Gram matrix for a set of forms using the metric g.\n        \n        Args:\n            forms: List of tensors, each shape (7, 7, 7) for 3-forms\n            g: Metric tensor (batch, 7, 7) or (7, 7)\n            \n        Returns:\n            Gram matrix of shape (n_forms, n_forms)\n        \"\"\"\n        n_forms = len(forms)\n        gram = torch.zeros(n_forms, n_forms, device=device, dtype=torch.float64)\n        \n        # Use average metric if batched\n        if g.dim() == 3:\n            g_avg = g.mean(dim=0)\n        else:\n            g_avg = g\n        \n        # Compute det(g) for volume element\n        det_g = torch.linalg.det(g_avg)\n        vol_factor = torch.sqrt(det_g.abs() + 1e-12)\n        \n        # Inverse metric for raising indices\n        g_inv = torch.linalg.inv(g_avg)\n        \n        for i in range(n_forms):\n            for j in range(i, n_forms):\n                # Inner product: <omega_i, omega_j> = g^{abc} g^{def} omega_i_{acd} omega_j_{bef}\n                # Simplified: flat metric inner product scaled by vol_factor\n                inner = (forms[i] * forms[j]).sum() * vol_factor\n                gram[i, j] = inner\n                gram[j, i] = inner\n        \n        return gram\n    \n    @torch.no_grad()\n    def extract_b2(self) -> Dict[str, float]:\n        \"\"\"\n        Extract b2_eff from 2-form basis.\n        \n        For b2, we use the harmonic 2-forms derived from the K7 metric.\n        In this simplified model, we use the Jacobi matrix of phi.\n        \"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n        g = out['g']\n        \n        # Build 2-form basis from metric derivatives\n        # The 2-forms are approximately the curvature 2-forms\n        # For simplicity, use C(7,2) = 21 coordinate 2-forms\n        forms_2 = []\n        for i in range(7):\n            for j in range(i+1, 7):\n                omega_2 = torch.zeros(7, 7, device=device, dtype=torch.float64)\n                omega_2[i, j] = 1.0\n                omega_2[j, i] = -1.0\n                forms_2.append(omega_2)\n        \n        # Compute Gram matrix for 2-forms\n        n_2forms = len(forms_2)\n        gram_2 = torch.zeros(n_2forms, n_2forms, device=device, dtype=torch.float64)\n        \n        g_avg = g.mean(dim=0)\n        g_inv = torch.linalg.inv(g_avg)\n        det_g = torch.linalg.det(g_avg)\n        vol_factor = torch.sqrt(det_g.abs() + 1e-12)\n        \n        for i in range(n_2forms):\n            for j in range(i, n_2forms):\n                # <omega_i, omega_j> = g^{ac} g^{bd} omega_i_{ab} omega_j_{cd} * sqrt(det g)\n                inner = torch.einsum('ac,bd,ab,cd->', g_inv, g_inv, \n                                    forms_2[i], forms_2[j]) * vol_factor\n                gram_2[i, j] = inner\n                gram_2[j, i] = inner\n        \n        # Eigenvalue analysis\n        eigenvalues = torch.linalg.eigvalsh(gram_2)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n        \n        # Count significant eigenvalues\n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b2_eff = (eigenvalues.abs() > threshold).sum().item()\n        \n        return {\n            'b2_eff': b2_eff,\n            'b2_target': self.sc.b2_K7,\n            'b2_match': abs(b2_eff - self.sc.b2_K7) <= 1,\n            'eigenvalues_2form': eigenvalues.cpu().numpy()[:5],  # Top 5\n        }\n    \n    @torch.no_grad()\n    def extract_b3_local(self) -> Dict[str, float]:\n        \"\"\"Extract b3_eff from local 35-dimensional basis.\"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n        g = out['g']\n        \n        # Compute Gram matrix for local basis (35 forms)\n        gram_local = self.compute_gram_matrix(self.local_basis.local_basis, g)\n        \n        # Eigenvalue analysis\n        eigenvalues = torch.linalg.eigvalsh(gram_local)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n        \n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b3_local_eff = (eigenvalues.abs() > threshold).sum().item()\n        \n        return {\n            'b3_local_eff': b3_local_eff,\n            'b3_local_target': self.sc.local_dim,  # 35\n            'b3_local_match': abs(b3_local_eff - self.sc.local_dim) <= 2,\n            'eigenvalues_local': eigenvalues.cpu().numpy()[:5],\n        }\n    \n    @torch.no_grad()\n    def extract_b3_global(self) -> Dict[str, float]:\n        \"\"\"Extract b3_eff from global 42-dimensional basis.\"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n        g = out['g']\n        \n        # Compute Gram matrix for global basis (42 forms)\n        gram_global = self.compute_gram_matrix(self.global_basis.global_basis, g)\n        \n        # Eigenvalue analysis\n        eigenvalues = torch.linalg.eigvalsh(gram_global)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n        \n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b3_global_eff = (eigenvalues.abs() > threshold).sum().item()\n        \n        return {\n            'b3_global_eff': b3_global_eff,\n            'b3_global_target': self.sc.global_dim,  # 42\n            'b3_global_match': abs(b3_global_eff - self.sc.global_dim) <= 2,\n            'eigenvalues_global': eigenvalues.cpu().numpy()[:5],\n        }\n    \n    @torch.no_grad()\n    def extract_b3_total(self) -> Dict[str, float]:\n        \"\"\"Extract b3_eff from combined 77-dimensional basis.\"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n        g = out['g']\n        \n        # Combined basis (local + global = 35 + 42 = 77)\n        combined_basis = self.local_basis.local_basis + self.global_basis.global_basis\n        \n        # Compute Gram matrix for combined basis\n        gram_combined = self.compute_gram_matrix(combined_basis, g)\n        \n        # Eigenvalue analysis\n        eigenvalues = torch.linalg.eigvalsh(gram_combined)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n        \n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b3_total_eff = (eigenvalues.abs() > threshold).sum().item()\n        \n        return {\n            'b3_total_eff': b3_total_eff,\n            'b3_total_target': self.sc.b3_K7,  # 77\n            'b3_total_match': abs(b3_total_eff - self.sc.b3_K7) <= 3,\n            'eigenvalues_combined': eigenvalues.cpu().numpy()[:10],\n        }\n    \n    def full_extraction(self) -> Dict[str, any]:\n        \"\"\"Run full Betti number extraction.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"BETTI NUMBER EXTRACTION\")\n        print(\"=\"*60)\n        \n        results = {}\n        \n        # Extract b2\n        print(\"\\nExtracting b2 from 2-form basis...\")\n        b2_results = self.extract_b2()\n        results.update(b2_results)\n        status_b2 = \"OK\" if b2_results['b2_match'] else \"MISMATCH\"\n        print(f\"  b2_eff = {b2_results['b2_eff']} (target: {b2_results['b2_target']}) [{status_b2}]\")\n        \n        # Extract b3 local\n        print(\"\\nExtracting b3_local from 35-dim local basis...\")\n        b3_local_results = self.extract_b3_local()\n        results.update(b3_local_results)\n        status_local = \"OK\" if b3_local_results['b3_local_match'] else \"MISMATCH\"\n        print(f\"  b3_local_eff = {b3_local_results['b3_local_eff']} (target: {b3_local_results['b3_local_target']}) [{status_local}]\")\n        \n        # Extract b3 global\n        print(\"\\nExtracting b3_global from 42-dim global basis...\")\n        b3_global_results = self.extract_b3_global()\n        results.update(b3_global_results)\n        status_global = \"OK\" if b3_global_results['b3_global_match'] else \"MISMATCH\"\n        print(f\"  b3_global_eff = {b3_global_results['b3_global_eff']} (target: {b3_global_results['b3_global_target']}) [{status_global}]\")\n        \n        # Extract b3 total\n        print(\"\\nExtracting b3_total from 77-dim combined basis...\")\n        b3_total_results = self.extract_b3_total()\n        results.update(b3_total_results)\n        status_total = \"OK\" if b3_total_results['b3_total_match'] else \"MISMATCH\"\n        print(f\"  b3_total_eff = {b3_total_results['b3_total_eff']} (target: {b3_total_results['b3_total_target']}) [{status_total}]\")\n        \n        # Summary\n        print(\"\\n\" + \"-\"*60)\n        print(\"SUMMARY:\")\n        print(f\"  b2:       {results['b2_eff']:3d} / {self.sc.b2_K7} (2-forms)\")\n        print(f\"  b3_local: {results['b3_local_eff']:3d} / {self.sc.local_dim} (local 3-forms)\")\n        print(f\"  b3_global:{results['b3_global_eff']:3d} / {self.sc.global_dim} (global 3-forms)\")\n        print(f\"  b3_total: {results['b3_total_eff']:3d} / {self.sc.b3_K7} (combined)\")\n        print(\"=\"*60)\n        \n        return results\n\n\n# Run Betti number extraction\nprint(\"Initializing Betti number extractor...\")\nbetti_extractor = BettiNumberExtractor(model, LOCAL_BASIS, GLOBAL_BASIS, SC, CONFIG)\n\nif TRAIN or True:  # Always run extraction\n    betti_results = betti_extractor.full_extraction()\nelse:\n    betti_results = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Representation Diagnostics ((2, 21, 54) Decomposition)\n\nVerify that H3(K7) = 77 decomposes as:\n- n1 = 2 singlets (1 local + 1 global)\n- n7 = 3 copies of 7-rep (1 local + 2 global) = 21 dimensions\n- n27 = 2 copies of 27-rep (1 local + 1 global) = 54 dimensions\n\nTotal: 2 + 21 + 54 = 77",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class RepresentationDiagnostics:\n    \"\"\"\n    Diagnostics for the (2, 21, 54) G2 representation decomposition of H3(K7).\n    \n    The 77-dimensional H3(K7) decomposes under G2 as:\n    - 2 copies of the singlet (1-dim)\n    - 3 copies of the fundamental (7-dim)\n    - 2 copies of the symmetric traceless (27-dim)\n    \n    Local contribution: (1, 7, 27) from Lambda^3 decomposition\n    Global contribution: (1, 14, 27) from TCS topology\n    \"\"\"\n    \n    def __init__(self, model: CombinedG2Model, local_basis: LocalG2Basis,\n                 global_basis: GlobalBasis, sc: StructuralConstants):\n        self.model = model\n        self.local_basis = local_basis\n        self.global_basis = global_basis\n        self.sc = sc\n    \n    def compute_casimir(self, forms: List[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Compute quadratic Casimir eigenvalues to identify G2 representations.\n        \n        Casimir eigenvalues:\n        - Singlet (1): C2 = 0\n        - Fundamental (7): C2 = 4\n        - Adjoint (14): C2 = 4\n        - 27-rep: C2 = 6\n        \"\"\"\n        n_forms = len(forms)\n        if n_forms == 0:\n            return torch.tensor([])\n        \n        # Build representation matrix via G2 structure constants\n        # Simplified: use phi contraction as a proxy\n        phi = self.local_basis.phi_canonical\n        \n        casimir_values = []\n        for form in forms:\n            # Contract with phi to get Casimir-like measure\n            # For singlet (proportional to phi), this should be maximal\n            contracted = torch.einsum('ijk,ijk->', form, phi)\n            form_norm = torch.sqrt((form**2).sum() + 1e-12)\n            phi_norm = torch.sqrt((phi**2).sum())\n            \n            # Normalized overlap with phi (1 for singlet, <1 for others)\n            overlap = contracted.abs() / (form_norm * phi_norm + 1e-12)\n            casimir_values.append(overlap.item())\n        \n        return torch.tensor(casimir_values)\n    \n    @torch.no_grad()\n    def analyze_local(self) -> Dict[str, any]:\n        \"\"\"Analyze local representation content (should be 1+7+27=35).\"\"\"\n        results = {\n            'n_singlet_local': len(self.local_basis.basis_1),\n            'n_7rep_local': len(self.local_basis.basis_7),\n            'n_27rep_local': len(self.local_basis.basis_27),\n            'total_local': self.local_basis.get_local_dim(),\n        }\n        \n        # Verify dimensions match Lambda^3 decomposition\n        results['singlet_dim_ok'] = results['n_singlet_local'] == self.sc.dim_Lambda3_1\n        results['7rep_dim_ok'] = results['n_7rep_local'] == self.sc.dim_Lambda3_7\n        results['27rep_dim_ok'] = results['n_27rep_local'] == self.sc.dim_Lambda3_27\n        \n        # Casimir analysis\n        casimir_1 = self.compute_casimir(self.local_basis.basis_1)\n        casimir_7 = self.compute_casimir(self.local_basis.basis_7)\n        casimir_27 = self.compute_casimir(self.local_basis.basis_27)\n        \n        results['casimir_singlet_mean'] = casimir_1.mean().item() if len(casimir_1) > 0 else 0\n        results['casimir_7rep_mean'] = casimir_7.mean().item() if len(casimir_7) > 0 else 0\n        results['casimir_27rep_mean'] = casimir_27.mean().item() if len(casimir_27) > 0 else 0\n        \n        return results\n    \n    @torch.no_grad()\n    def analyze_global(self) -> Dict[str, any]:\n        \"\"\"Analyze global representation content (should be 1+14+27=42).\"\"\"\n        results = {\n            'n_singlet_global': self.global_basis.n_extra_singlet,  # 1\n            'n_7rep_global': self.global_basis.n_extra_7rep,        # 14 (2 copies of 7)\n            'n_27rep_global': self.global_basis.n_extra_27rep,      # 27\n            'total_global': self.global_basis.get_global_dim(),\n        }\n        \n        # Verify dimensions\n        results['singlet_ok'] = results['n_singlet_global'] == 1\n        results['7rep_ok'] = results['n_7rep_global'] == 14\n        results['27rep_ok'] = results['n_27rep_global'] == 27\n        results['total_ok'] = results['total_global'] == self.sc.global_dim\n        \n        return results\n    \n    @torch.no_grad()\n    def analyze_combined(self) -> Dict[str, any]:\n        \"\"\"Analyze total (2, 21, 54) decomposition.\"\"\"\n        local_res = self.analyze_local()\n        global_res = self.analyze_global()\n        \n        # Total counts\n        n1_total = local_res['n_singlet_local'] + global_res['n_singlet_global']\n        n7_total = local_res['n_7rep_local'] + global_res['n_7rep_global']\n        n27_total = local_res['n_27rep_local'] + global_res['n_27rep_global']\n        \n        results = {\n            'n1_total': n1_total,  # Should be 2\n            'n7_total_dims': n7_total,  # Should be 21 (7 + 14)\n            'n27_total_dims': n27_total,  # Should be 54 (27 + 27)\n            'grand_total': n1_total + n7_total + n27_total,  # Should be 77\n            \n            # Multiplicity counts (n_rep, not dims)\n            'n1_multiplicity': n1_total,  # 2 singlets\n            'n7_multiplicity': n7_total // 7 if n7_total >= 7 else 0,  # 3 copies of 7\n            'n27_multiplicity': n27_total // 27 if n27_total >= 27 else 0,  # 2 copies of 27\n        }\n        \n        # Verify (2, 21, 54) pattern\n        results['matches_2_21_54'] = (\n            results['n1_total'] == self.sc.n_singlets_global and\n            results['n7_total_dims'] == self.sc.n_7rep_global * self.sc.dim_Lambda3_7 and\n            results['n27_total_dims'] == self.sc.n_27rep_global * self.sc.dim_Lambda3_27\n        )\n        \n        return results, local_res, global_res\n    \n    def full_diagnostics(self) -> Dict[str, any]:\n        \"\"\"Run full representation diagnostics.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"REPRESENTATION DIAGNOSTICS: (2, 21, 54)\")\n        print(\"=\"*60)\n        \n        combined, local_res, global_res = self.analyze_combined()\n        \n        # Local analysis\n        print(\"\\nLOCAL DECOMPOSITION (Lambda^3 = 1 + 7 + 27 = 35):\")\n        print(f\"  Lambda^3_1 (singlet):   {local_res['n_singlet_local']} forms\")\n        print(f\"  Lambda^3_7 (fund):      {local_res['n_7rep_local']} forms\")\n        print(f\"  Lambda^3_27 (sym-tr):   {local_res['n_27rep_local']} forms\")\n        print(f\"  Total local:            {local_res['total_local']} forms\")\n        \n        # Global analysis\n        print(\"\\nGLOBAL DECOMPOSITION (TCS = 1 + 14 + 27 = 42):\")\n        print(f\"  Extra singlet:          {global_res['n_singlet_global']} forms\")\n        print(f\"  Extra 7-rep (2x7):      {global_res['n_7rep_global']} forms\")\n        print(f\"  Extra 27-rep:           {global_res['n_27rep_global']} forms\")\n        print(f\"  Total global:           {global_res['total_global']} forms\")\n        \n        # Combined (2, 21, 54) verification\n        print(\"\\nCOMBINED (2, 21, 54) DECOMPOSITION:\")\n        print(f\"  Singlets (n1=2):        {combined['n1_total']} dims  [target: 2]\")\n        print(f\"  7-reps (n7=3 -> 21):    {combined['n7_total_dims']} dims  [target: 21]\")\n        print(f\"  27-reps (n27=2 -> 54):  {combined['n27_total_dims']} dims  [target: 54]\")\n        print(f\"  Grand total:            {combined['grand_total']} dims  [target: 77]\")\n        \n        status = \"OK\" if combined['matches_2_21_54'] else \"MISMATCH\"\n        print(f\"\\n  (2, 21, 54) pattern: [{status}]\")\n        \n        # Multiplicity counts\n        print(\"\\nMULTIPLICITY SUMMARY:\")\n        print(f\"  n1 (singlet count):     {combined['n1_multiplicity']} copies\")\n        print(f\"  n7 (7-rep count):       {combined['n7_multiplicity']} copies\")\n        print(f\"  n27 (27-rep count):     {combined['n27_multiplicity']} copies\")\n        \n        print(\"=\"*60)\n        \n        return {\n            'combined': combined,\n            'local': local_res,\n            'global': global_res,\n        }\n\n\n# Run representation diagnostics\nrep_diag = RepresentationDiagnostics(model, LOCAL_BASIS, GLOBAL_BASIS, SC)\nrep_results = rep_diag.full_diagnostics()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 14. Output and Save\n\nSave trained models, results, and metadata:\n- `models_v1_5.pt`: Neural network weights\n- `results_v1_5.json`: All numerical results\n- `results_v1_5.tex`: LaTeX table for publication\n- Training history and configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def save_results(model: CombinedG2Model, history: Dict, betti_results: Dict,\n                 rep_results: Dict, zpg: ZeroParamGeometry, sc: StructuralConstants,\n                 config: Dict, output_dir: str = '.'):\n    \"\"\"Save all results to files.\"\"\"\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"SAVING RESULTS\")\n    print(\"=\"*60)\n    \n    # 1. Save model weights\n    model_path = os.path.join(output_dir, 'models_v1_5.pt')\n    torch.save({\n        'local_net_state_dict': model.local_net.state_dict(),\n        'global_net_state_dict': model.global_net.state_dict(),\n        'config': config,\n        'timestamp': timestamp,\n    }, model_path)\n    print(f\"  Model weights: {model_path}\")\n    \n    # 2. Sample coordinates and compute final metrics\n    x_sample = torch.rand(1024, 7, device=device, dtype=torch.float64)\n    with torch.no_grad():\n        out = model(x_sample)\n        final_kappa_T = out['torsion'].mean().item()\n        final_det_g = out['det_g'].mean().item()\n        local_norm = (out['phi_local']**2).sum(dim=(-1,-2,-3)).mean().item()\n        global_norm = (out['phi_global']**2).sum(dim=(-1,-2,-3)).mean().item()\n    \n    # 3. Compile results dictionary\n    results = {\n        'version': '1.5',\n        'timestamp': timestamp,\n        'targets': {\n            'kappa_T': str(zpg.kappa_T_fraction),\n            'det_g': str(zpg.det_g_fraction),\n            'b2': sc.b2_K7,\n            'b3': sc.b3_K7,\n            'b3_local': sc.local_dim,\n            'b3_global': sc.global_dim,\n        },\n        'achieved': {\n            'kappa_T': final_kappa_T,\n            'det_g': final_det_g,\n            'local_phi_norm': local_norm,\n            'global_phi_norm': global_norm,\n        },\n        'betti_numbers': {\n            'b2_eff': betti_results['b2_eff'] if betti_results else None,\n            'b3_local_eff': betti_results['b3_local_eff'] if betti_results else None,\n            'b3_global_eff': betti_results['b3_global_eff'] if betti_results else None,\n            'b3_total_eff': betti_results['b3_total_eff'] if betti_results else None,\n        },\n        'representation': {\n            'n1': rep_results['combined']['n1_total'],\n            'n7_dims': rep_results['combined']['n7_total_dims'],\n            'n27_dims': rep_results['combined']['n27_total_dims'],\n            'matches_2_21_54': rep_results['combined']['matches_2_21_54'],\n        },\n        'deviations': {\n            'kappa_T_rel': abs(final_kappa_T - zpg.kappa_T) / zpg.kappa_T * 100,\n            'det_g_rel': abs(final_det_g - zpg.det_g_target) / zpg.det_g_target * 100,\n        },\n        'training': {\n            'n_epochs': len(history['epoch']) if history else 0,\n            'final_loss': history['loss_total'][-1] if history else None,\n        },\n        'config': config,\n    }\n    \n    # Save JSON\n    json_path = os.path.join(output_dir, 'results_v1_5.json')\n    \n    # Convert non-serializable items\n    def make_serializable(obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, (np.float32, np.float64)):\n            return float(obj)\n        if isinstance(obj, (np.int32, np.int64)):\n            return int(obj)\n        if isinstance(obj, dict):\n            return {k: make_serializable(v) for k, v in obj.items()}\n        if isinstance(obj, list):\n            return [make_serializable(i) for i in obj]\n        return obj\n    \n    with open(json_path, 'w') as f:\n        json.dump(make_serializable(results), f, indent=2)\n    print(f\"  Results JSON: {json_path}\")\n    \n    # 4. Generate LaTeX table\n    tex_content = r\"\"\"\\begin{table}[h]\n\\centering\n\\caption{GIFT K7 v1.5 Results: Local/Global G2 Decomposition}\n\\label{tab:gift_v1_5}\n\\begin{tabular}{lrrr}\n\\toprule\n\\textbf{Observable} & \\textbf{Target} & \\textbf{Achieved} & \\textbf{Deviation} \\\\\n\\midrule\n$\\kappa_T$ (torsion) & $1/61$ & \"\"\" + f\"{final_kappa_T:.6f}\" + r\"\"\" & \"\"\" + f\"{results['deviations']['kappa_T_rel']:.2f}\" + r\"\"\"\\% \\\\\n$\\det(g)$ (metric det) & $65/32$ & \"\"\" + f\"{final_det_g:.6f}\" + r\"\"\" & \"\"\" + f\"{results['deviations']['det_g_rel']:.2f}\" + r\"\"\"\\% \\\\\n\\midrule\n$b_2$ (2-forms) & 21 & \"\"\" + f\"{betti_results['b2_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n$b_3^{\\text{local}}$ & 35 & \"\"\" + f\"{betti_results['b3_local_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n$b_3^{\\text{global}}$ & 42 & \"\"\" + f\"{betti_results['b3_global_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n$b_3^{\\text{total}}$ & 77 & \"\"\" + f\"{betti_results['b3_total_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n\\midrule\n$(n_1, n_7 \\cdot 7, n_{27} \\cdot 27)$ & (2, 21, 54) & \"\"\" + f\"({rep_results['combined']['n1_total']}, {rep_results['combined']['n7_total_dims']}, {rep_results['combined']['n27_total_dims']})\" + r\"\"\" & -- \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\"\"\"\n    \n    tex_path = os.path.join(output_dir, 'results_v1_5.tex')\n    with open(tex_path, 'w') as f:\n        f.write(tex_content)\n    print(f\"  LaTeX table: {tex_path}\")\n    \n    # 5. Save sample coordinates and outputs\n    coords_path = os.path.join(output_dir, 'sample_coords_v1_5.pt')\n    torch.save({\n        'x': x_sample.cpu(),\n        'phi_local': out['phi_local'].cpu(),\n        'phi_global': out['phi_global'].cpu(),\n        'phi_total': out['phi_total'].cpu(),\n        'g': out['g'].cpu(),\n        'det_g': out['det_g'].cpu(),\n        'torsion': out['torsion'].cpu(),\n    }, coords_path)\n    print(f\"  Sample coordinates: {coords_path}\")\n    \n    # 6. Save training history\n    if history:\n        hist_path = os.path.join(output_dir, 'history_v1_5.json')\n        with open(hist_path, 'w') as f:\n            json.dump(make_serializable(history), f, indent=2)\n        print(f\"  Training history: {hist_path}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL SUMMARY\")\n    print(\"=\"*60)\n    print(f\"\\n  kappa_T: {final_kappa_T:.6f} (target: {zpg.kappa_T:.6f}, dev: {results['deviations']['kappa_T_rel']:.2f}%)\")\n    print(f\"  det(g):  {final_det_g:.6f} (target: {zpg.det_g_target:.6f}, dev: {results['deviations']['det_g_rel']:.2f}%)\")\n    print(f\"\\n  Betti numbers: b2={betti_results['b2_eff'] if betti_results else 'N/A'}, \" +\n          f\"b3_local={betti_results['b3_local_eff'] if betti_results else 'N/A'}, \" +\n          f\"b3_global={betti_results['b3_global_eff'] if betti_results else 'N/A'}, \" +\n          f\"b3_total={betti_results['b3_total_eff'] if betti_results else 'N/A'}\")\n    print(f\"\\n  Representation: (2, 21, 54) = ({rep_results['combined']['n1_total']}, \" +\n          f\"{rep_results['combined']['n7_total_dims']}, {rep_results['combined']['n27_total_dims']})\")\n    \n    match_status = \"MATCH\" if rep_results['combined']['matches_2_21_54'] else \"MISMATCH\"\n    print(f\"  Status: {match_status}\")\n    print(\"=\"*60)\n    \n    return results\n\n\n# Save results\nif history is not None or True:\n    final_results = save_results(model, history, betti_results, rep_results, \n                                  ZPG, SC, CONFIG, output_dir='.')\nelse:\n    print(\"Skipping save (no training history)\")\n    final_results = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}