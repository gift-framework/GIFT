{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gift-framework/GIFT/blob/main/G2_ML/1_5/K7_GIFT_v1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8lZ4VAzWlb6"
   },
   "source": [
    "# K7 GIFT v1.5b - Local/Global G2 Decomposition Framework\n",
    "\n",
    "**Version 1.5b**: Robust training with protected local kappa_T\n",
    "\n",
    "## Key Improvements over v1.5\n",
    "- **Freeze local in early phases**: Protect kappa_T from v1.4 solution\n",
    "- **Separated torsion losses**: T_local anchor + T_global penalty\n",
    "- **Robust global basis**: Guaranteed rank 42 via Gram-Schmidt\n",
    "- **Mode activation loss**: Encourage all 42 global modes\n",
    "\n",
    "## Goals\n",
    "- Maintain v1.4 successes: kappa_T = 1/61, det(g) = 65/32, b2_eff = 21\n",
    "- Achieve b3_eff = 77 via local/global decomposition\n",
    "- Local: 35 modes from Lambda3_1 + Lambda3_7 + Lambda3_27 (T7-like)\n",
    "- Global: 42 modes from TCS topology (2, 21, 54 decomposition)\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "phi(x) = phi_local(x) + phi_global(x)\n",
    "       = sum_a alpha_a(x) * psi_local_a(x)    # 35 local modes\n",
    "       + sum_b c_b(x) * Omega_global_b(x)     # 42 global modes\n",
    "```\n",
    "\n",
    "## Training Strategy (v1.5b)\n",
    "1. **Phase 1-2**: Global only, local frozen (inherit v1.4 kappa_T)\n",
    "2. **Phase 3**: Both with anchor losses (local_anchor + global_torsion)\n",
    "3. **Phase 4**: Fine-tune with minimal local LR\n",
    "\n",
    "## References\n",
    "- GIFT v2.2 main paper\n",
    "- K7_GIFT_v1_4_TCS_full.ipynb (predecessor with good kappa_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTEMFTy5Wlb9"
   },
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iCgSUprWlb9",
    "outputId": "34420050-1c32-41f4-d193-e363c4a2274f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "GIFT K7 v1.5 - Local/Global G2 Decomposition\n",
      "PyTorch version: 2.9.0+cu126\n",
      "NumPy version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom dataclasses import dataclass\nfrom fractions import Fraction\nfrom typing import Dict, Tuple, List, Optional\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport json\nimport os\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Precision\ntorch.set_default_dtype(torch.float64)\n\nprint('GIFT K7 v1.6 - Local/Global G2 Decomposition')\nprint(f'PyTorch version: {torch.__version__}')\nprint(f'NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Structural Constants (Zero-Parameter Foundation)\n",
    "\n",
    "All values are topological integers from E8/G2/K7 geometry - NO FREE PARAMETERS.\n",
    "These define the immutable structure of the theory."
   ],
   "metadata": {
    "id": "WOpnhwnAWlb-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\nclass StructuralConstants:\n    \"\"\"\n    Immutable structural constants from E8/G2/K7 geometry - NO FREE PARAMETERS.\n    All values are topological integers from GIFT v2.2.\n    \"\"\"\n    # Primary structural integers\n    p2: int = 2              # Binary duality: dim(G2)/dim(K7) = 14/7\n    N_gen: int = 3           # Fermion generations\n    Weyl_factor: int = 5     # From |W(E8)| = 2^14 * 3^5 * 5^2 * 7\n    dim_K7: int = 7          # K7 manifold dimension\n    rank_E8: int = 8         # E8 rank\n    dim_G2: int = 14         # G2 holonomy group dimension\n    dim_E8: int = 248        # E8 dimension\n    dim_J3O: int = 27        # Exceptional Jordan algebra dimension\n\n    # Topological invariants (Betti numbers from TCS construction)\n    b2_K7: int = 21          # Second Betti number (gauge fields)\n    b3_K7: int = 77          # Third Betti number (matter fields)\n\n    # G2 representation dimensions (local decomposition of Lambda^3)\n    dim_Lambda3_1: int = 1   # Singlet representation\n    dim_Lambda3_7: int = 7   # Fundamental representation\n    dim_Lambda3_27: int = 27 # Symmetric traceless representation\n\n    # Local vs Global decomposition (key v1.6 innovation)\n    @property\n    def local_dim(self) -> int:\n        \"\"\"Local modes: 1 + 7 + 27 = 35 (T7-like structure)\"\"\"\n        return self.dim_Lambda3_1 + self.dim_Lambda3_7 + self.dim_Lambda3_27\n\n    @property\n    def global_dim(self) -> int:\n        \"\"\"Global modes: b3 - local = 77 - 35 = 42 (TCS-induced)\"\"\"\n        return self.b3_K7 - self.local_dim\n\n    # Global (2, 21, 54) decomposition multiplicities\n    @property\n    def n_singlets_global(self) -> int:\n        \"\"\"Total singlets in H3: n1 = 2 (1 local + 1 global)\"\"\"\n        return 2\n\n    @property\n    def n_7rep_global(self) -> int:\n        \"\"\"Total 7-reps in H3: n7 = 3 (1 local + 2 global) -> 21 dims\"\"\"\n        return 3\n\n    @property\n    def n_27rep_global(self) -> int:\n        \"\"\"Total 27-reps in H3: n27 = 2 (1 local + 1 global) -> 54 dims\"\"\"\n        return 2\n\n    @property\n    def H_star(self) -> int:\n        \"\"\"H* = 1 + b2 + b3 = 99 (effective cohomological dimension)\"\"\"\n        return 1 + self.b2_K7 + self.b3_K7\n\n    @property\n    def M5(self) -> int:\n        \"\"\"Fifth Mersenne prime: dim(E8)/rank(E8) = 248/8 = 31\"\"\"\n        return self.dim_E8 // self.rank_E8\n\n    def verify_relations(self) -> Dict[str, bool]:\n        \"\"\"Verify consistency relations between structural constants.\"\"\"\n        return {\n            'p2 = dim(G2)/dim(K7)': self.p2 == self.dim_G2 // self.dim_K7,\n            'b3 = 2*dim(K7)^2 - b2': self.b3_K7 == 2 * self.dim_K7**2 - self.b2_K7,\n            'H* = dim(G2)*dim(K7) + 1': self.H_star == self.dim_G2 * self.dim_K7 + 1,\n            'M5 = 31 (Mersenne)': self.M5 == 31,\n            'local = 35': self.local_dim == 35,\n            'global = 42': self.global_dim == 42,\n            '(2,21,54) sums to 77': (self.n_singlets_global +\n                                      self.n_7rep_global * self.dim_Lambda3_7 +\n                                      self.n_27rep_global * self.dim_Lambda3_27) == self.b3_K7,\n        }\n\nSC = StructuralConstants()\nprint('=== STRUCTURAL CONSTANTS (IMMUTABLE) ===')\nprint(f'p2={SC.p2}, N_gen={SC.N_gen}, Weyl={SC.Weyl_factor}')\nprint(f'dim_K7={SC.dim_K7}, rank_E8={SC.rank_E8}, dim_G2={SC.dim_G2}, dim_E8={SC.dim_E8}')\nprint(f'b2={SC.b2_K7}, b3={SC.b3_K7}, H*={SC.H_star}, M5={SC.M5}')\nprint()\nprint(f'=== LOCAL/GLOBAL DECOMPOSITION ===')\nprint(f'Local (T7-like): 1 + 7 + 27 = {SC.local_dim}')\nprint(f'Global (TCS): {SC.global_dim}')\nprint(f'(2, 21, 54) pattern: {SC.n_singlets_global}, {SC.n_7rep_global*SC.dim_Lambda3_7}, {SC.n_27rep_global*SC.dim_Lambda3_27}')\nprint()\nprint('Consistency checks:')\nfor name, ok in SC.verify_relations().items():\n    status = 'OK' if ok else 'FAIL'\n    print(f'  [{status}] {name}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhtk9Z85Wlb-",
    "outputId": "50b7a330-4250-4503-b28b-a65d87731ca2"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== STRUCTURAL CONSTANTS (IMMUTABLE) ===\n",
      "p2=2, N_gen=3, Weyl=5\n",
      "dim_K7=7, rank_E8=8, dim_G2=14, dim_E8=248\n",
      "b2=21, b3=77, H*=99, M5=31\n",
      "\n",
      "=== LOCAL/GLOBAL DECOMPOSITION ===\n",
      "Local (T7-like): 1 + 7 + 27 = 35\n",
      "Global (TCS): 42\n",
      "(2, 21, 54) pattern: 2, 21, 54\n",
      "\n",
      "Consistency checks:\n",
      "  [OK] p2 = dim(G2)/dim(K7)\n",
      "  [OK] b3 = 2*dim(K7)^2 - b2\n",
      "  [OK] H* = dim(G2)*dim(K7) + 1\n",
      "  [OK] M5 = 31 (Mersenne)\n",
      "  [OK] local = 35\n",
      "  [OK] global = 42\n",
      "  [OK] (2,21,54) sums to 77\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Zero-Parameter Geometry (Derived Quantities)\n",
    "\n",
    "All physical observables derived from structural constants ONLY.\n",
    "Each quantity has an exact formula from topological integers."
   ],
   "metadata": {
    "id": "p5CKcElpWlb_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ZeroParamGeometry:\n",
    "    \"\"\"\n",
    "    All physical observables derived from structural constants ONLY.\n",
    "    Each quantity has an exact formula from topological integers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sc: StructuralConstants):\n",
    "        self.sc = sc\n",
    "\n",
    "    # === KAPPA_T: Torsion scale (1/61) ===\n",
    "    @property\n",
    "    def kappa_T_denominator(self) -> int:\n",
    "        \"\"\"Denominator: b3 - dim(G2) - p2 = 77 - 14 - 2 = 61\"\"\"\n",
    "        return self.sc.b3_K7 - self.sc.dim_G2 - self.sc.p2\n",
    "\n",
    "    @property\n",
    "    def kappa_T(self) -> float:\n",
    "        \"\"\"KAPPA_T = 1/(b3 - dim(G2) - p2) = 1/61\"\"\"\n",
    "        return 1.0 / self.kappa_T_denominator\n",
    "\n",
    "    @property\n",
    "    def kappa_T_fraction(self) -> Fraction:\n",
    "        \"\"\"Exact rational form\"\"\"\n",
    "        return Fraction(1, self.kappa_T_denominator)\n",
    "\n",
    "    # === DET(G): Metric determinant (65/32) ===\n",
    "    @property\n",
    "    def det_g_denominator(self) -> int:\n",
    "        \"\"\"Denominator: b2 + dim(G2) - N_gen = 21 + 14 - 3 = 32\"\"\"\n",
    "        return self.sc.b2_K7 + self.sc.dim_G2 - self.sc.N_gen\n",
    "\n",
    "    @property\n",
    "    def det_g_numerator(self) -> int:\n",
    "        \"\"\"Numerator: p2 * denominator + 1 = 2*32 + 1 = 65\"\"\"\n",
    "        return self.sc.p2 * self.det_g_denominator + 1\n",
    "\n",
    "    @property\n",
    "    def det_g_target(self) -> float:\n",
    "        \"\"\"det(g) = p2 + 1/(b2 + dim(G2) - N_gen) = 2 + 1/32 = 65/32\"\"\"\n",
    "        return self.det_g_numerator / self.det_g_denominator\n",
    "\n",
    "    @property\n",
    "    def det_g_fraction(self) -> Fraction:\n",
    "        \"\"\"Exact rational form\"\"\"\n",
    "        return Fraction(self.det_g_numerator, self.det_g_denominator)\n",
    "\n",
    "    # === TAU: Hierarchy parameter (3472/891) ===\n",
    "    @property\n",
    "    def tau_num(self) -> int:\n",
    "        \"\"\"Numerator: p2^4 * dim_K7 * M5 = 16 * 7 * 31 = 3472\"\"\"\n",
    "        return (self.sc.p2**4) * self.sc.dim_K7 * self.sc.M5\n",
    "\n",
    "    @property\n",
    "    def tau_den(self) -> int:\n",
    "        \"\"\"Denominator: N_gen^4 * (rank_E8 + N_gen) = 81 * 11 = 891\"\"\"\n",
    "        return (self.sc.N_gen**4) * (self.sc.rank_E8 + self.sc.N_gen)\n",
    "\n",
    "    @property\n",
    "    def tau(self) -> float:\n",
    "        \"\"\"TAU = 3472/891 = 3.8967...\"\"\"\n",
    "        return self.tau_num / self.tau_den\n",
    "\n",
    "    @property\n",
    "    def tau_fraction(self) -> Fraction:\n",
    "        \"\"\"Exact rational form\"\"\"\n",
    "        return Fraction(self.tau_num, self.tau_den)\n",
    "\n",
    "    # === Angular parameters ===\n",
    "    @property\n",
    "    def beta_0(self) -> float:\n",
    "        \"\"\"Angular quantization: pi/rank(E8) = pi/8\"\"\"\n",
    "        return np.pi / self.sc.rank_E8\n",
    "\n",
    "    @property\n",
    "    def xi(self) -> float:\n",
    "        \"\"\"Correlation: (Weyl/p2) * beta_0 = 5*pi/16\"\"\"\n",
    "        return (self.sc.Weyl_factor / self.sc.p2) * self.beta_0\n",
    "\n",
    "    # === Gauge couplings ===\n",
    "    @property\n",
    "    def sin2_theta_W(self) -> float:\n",
    "        \"\"\"Weinberg angle: b2/(b3 + dim(G2)) = 21/91 = 3/13\"\"\"\n",
    "        return self.sc.b2_K7 / (self.sc.b3_K7 + self.sc.dim_G2)\n",
    "\n",
    "    @property\n",
    "    def alpha_s_MZ(self) -> float:\n",
    "        \"\"\"Strong coupling: sqrt(2)/(dim(G2) - p2) = sqrt(2)/12\"\"\"\n",
    "        return np.sqrt(2) / (self.sc.dim_G2 - self.sc.p2)\n",
    "\n",
    "    @property\n",
    "    def lambda_H(self) -> float:\n",
    "        \"\"\"Higgs self-coupling: sqrt(dim(G2) + N_gen)/32 = sqrt(17)/32\"\"\"\n",
    "        return np.sqrt(self.sc.dim_G2 + self.sc.N_gen) / 32\n",
    "\n",
    "    def summary(self) -> Dict[str, str]:\n",
    "        \"\"\"Return a summary of all derived quantities.\"\"\"\n",
    "        return {\n",
    "            'kappa_T': f'{self.kappa_T_fraction} = {self.kappa_T:.6f}',\n",
    "            'det(g)': f'{self.det_g_fraction} = {self.det_g_target:.6f}',\n",
    "            'tau': f'{self.tau_fraction} = {self.tau:.6f}',\n",
    "            'beta_0': f'pi/8 = {self.beta_0:.6f}',\n",
    "            'xi': f'5*pi/16 = {self.xi:.6f}',\n",
    "            'sin2_theta_W': f'21/91 = {self.sin2_theta_W:.6f}',\n",
    "            'alpha_s(MZ)': f'sqrt(2)/12 = {self.alpha_s_MZ:.6f}',\n",
    "            'lambda_H': f'sqrt(17)/32 = {self.lambda_H:.6f}',\n",
    "        }\n",
    "\n",
    "ZPG = ZeroParamGeometry(SC)\n",
    "print('=== ZERO-PARAMETER DERIVED QUANTITIES ===')\n",
    "for name, value in ZPG.summary().items():\n",
    "    print(f'  {name}: {value}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb60qDWnWlb_",
    "outputId": "8504fc3f-d25d-47c4-c53f-0f651284dc91"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ZERO-PARAMETER DERIVED QUANTITIES ===\n",
      "  kappa_T: 1/61 = 0.016393\n",
      "  det(g): 65/32 = 2.031250\n",
      "  tau: 3472/891 = 3.896745\n",
      "  beta_0: pi/8 = 0.392699\n",
      "  xi: 5*pi/16 = 0.981748\n",
      "  sin2_theta_W: 21/91 = 0.230769\n",
      "  alpha_s(MZ): sqrt(2)/12 = 0.117851\n",
      "  lambda_H: sqrt(17)/32 = 0.128847\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Training Configuration (Hyperparameters Only)\n",
    "\n",
    "These are tunable hyperparameters - NOT physical parameters.\n",
    "Physical quantities come from ZeroParamGeometry only."
   ],
   "metadata": {
    "id": "ojHNaGE9Wlb_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "CONFIG = {\n    # v1.4 anchor model (for local network initialization)\n    'v14_model_path': '../1_4/models_v1_4.pt',\n    'v14_kappa_T_ref': 0.016393844829,  # Best T_val from v1.4\n\n    # Network architectures\n    'local_net': {\n        'hidden_dims': [128, 128, 64],  # For LocalPhiNet\n        'fourier_features': 32,\n        'activation': 'silu',\n    },\n    'global_net': {\n        'hidden_dims': [64, 64],  # Smaller for GlobalCoeffNet\n        'fourier_features': 16,\n        'activation': 'silu',\n    },\n\n    # TCS geometry\n    'tcs': {\n        'neck_half_length': 1.0,\n        'neck_width': 0.3,\n        'twist_angle': np.pi/4,\n        'left_scale': 1.0,\n        'right_scale': 1.0,\n    },\n\n    # Training\n    'n_points': 2048,\n    'n_epochs': 2000,  # Increased from 500\n    'lr_local': 1e-4,  # Lower LR for local (mostly frozen)\n    'lr_global': 5e-4,\n    'weight_decay': 1e-6,\n\n    # Loss weights - v1.6 with torsion separation\n    'loss_weights': {\n        # Core targets\n        'kappa_T': 200.0,           # Total torsion target\n        'det_g': 5.0,             # Metric determinant\n\n        # Torsion separation (NEW for v1.6)\n        'local_anchor': 20.0,     # Keep local near v1.4 solution\n        'global_torsion': 50.0,   # Penalize global torsion heavily\n\n        # Structure\n        'closure': 1.0,\n        'coclosure': 1.0,\n        'g2_consistency': 2.0,\n        'local_global_balance': 0.5,\n        'spd': 5.0,\n\n        # Mode activation (NEW)\n        'mode_activation': 0.1,\n            'kappa_relative': 500.0,    # Relative error loss   # Encourage all 42 global modes\n    },\n\n    # Phases - v1.6 with freeze strategy\n    'phases': [\n        # Phase 1: Global only (local frozen, inherits v1.4 kappa_T)\n        {'name': 'global_warmup', 'epochs': 200, 'focus': 'global_only',\n         'freeze_local': True},\n        # Phase 2: Global with heavy torsion penalty\n        {'name': 'global_torsion_control', 'epochs': 600, 'focus': 'global_only',\n         'freeze_local': True},\n        # Phase 3: Both with local anchor\n        {'name': 'joint_with_anchor', 'epochs': 800, 'focus': 'both',\n         'freeze_local': False, 'local_lr_factor': 0.1},\n        # Phase 4: Fine-tune\n        {'name': 'fine_tune', 'epochs': 400, 'focus': 'both',\n         'freeze_local': False, 'local_lr_factor': 0.01},\n    ],\n\n    # Betti number extraction\n    'betti_threshold': 1e-8,\n    'n_betti_samples': 4096,\n}\n\nprint('=== TRAINING CONFIGURATION v1.6 ===')\nprint(f\"v1.4 model path: {CONFIG['v14_model_path']}\")\nprint(f\"Local network: {CONFIG['local_net']['hidden_dims']}\")\nprint(f\"Global network: {CONFIG['global_net']['hidden_dims']}\")\nprint(f\"Training epochs: {CONFIG['n_epochs']}\")\nprint(f\"Loss weights:\")\nfor k, v in CONFIG['loss_weights'].items():\n    print(f\"  {k}: {v}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsSoGBaeWlcA",
    "outputId": "9b0a545c-d17e-4647-c584-fb18ccc25cea"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== TRAINING CONFIGURATION v1.5b ===\n",
      "v1.4 model path: ../1_4/models_v1_4.pt\n",
      "Local network: [128, 128, 64]\n",
      "Global network: [64, 64]\n",
      "Training epochs: 2000\n",
      "Loss weights:\n",
      "  kappa_T: 200.0\n",
      "  det_g: 5.0\n",
      "  local_anchor: 20.0\n",
      "  global_torsion: 50.0\n",
      "  closure: 1.0\n",
      "  coclosure: 1.0\n",
      "  g2_consistency: 2.0\n",
      "  local_global_balance: 0.5\n",
      "  spd: 5.0\n",
      "  mode_activation: 0.1\n",
      "  kappa_relative: 500.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Local G2 Decomposition Basis (35-dimensional)\n",
    "\n",
    "The space of 3-forms on a G2 manifold decomposes into irreducible representations:\n",
    "- Lambda3_1 (dim 1): Singlet - the G2 3-form phi itself\n",
    "- Lambda3_7 (dim 7): Fundamental - vector-valued deformations\n",
    "- Lambda3_27 (dim 27): Symmetric traceless - tensor deformations\n",
    "\n",
    "Total local dimension: 1 + 7 + 27 = 35"
   ],
   "metadata": {
    "id": "WTzoaVQbWlcC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# G2 structure constants from octonion multiplication table\n",
    "# These define the canonical G2 3-form phi\n",
    "G2_PHI_INDICES = [\n",
    "    (0, 1, 2), (0, 3, 4), (0, 5, 6),\n",
    "    (1, 3, 5), (1, 4, 6), (2, 3, 6), (2, 4, 5)\n",
    "]\n",
    "\n",
    "def canonical_g2_phi(device_=device) -> torch.Tensor:\n",
    "    \"\"\"Canonical G2 3-form from octonion structure constants.\"\"\"\n",
    "    phi = torch.zeros(7, 7, 7, device=device_, dtype=torch.float64)\n",
    "    for (i, j, k) in G2_PHI_INDICES:\n",
    "        phi[i, j, k] = 1.0\n",
    "        phi[i, k, j] = -1.0\n",
    "        phi[j, i, k] = -1.0\n",
    "        phi[j, k, i] = 1.0\n",
    "        phi[k, i, j] = 1.0\n",
    "        phi[k, j, i] = -1.0\n",
    "    return phi\n",
    "\n",
    "PHI_CANONICAL = canonical_g2_phi()\n",
    "\n",
    "class LocalG2Basis:\n",
    "    \"\"\"\n",
    "    Explicit basis for the local G2 decomposition of Lambda^3.\n",
    "\n",
    "    Lambda^3 = Lambda^3_1 (dim 1) + Lambda^3_7 (dim 7) + Lambda^3_27 (dim 27)\n",
    "\n",
    "    - Lambda^3_1: Singlet (proportional to phi)\n",
    "    - Lambda^3_7: Fundamental (iota_v phi for v in R^7)\n",
    "    - Lambda^3_27: Symmetric traceless (built from phi and metric)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device_=device):\n",
    "        self.device = device_\n",
    "        self.phi_canonical = canonical_g2_phi(device_)\n",
    "\n",
    "        # Build all basis elements\n",
    "        self.basis_1 = self._build_lambda3_1()      # 1 element\n",
    "        self.basis_7 = self._build_lambda3_7()      # 7 elements\n",
    "        self.basis_27 = self._build_lambda3_27()    # 27 elements\n",
    "\n",
    "        # Combined local basis (35 elements)\n",
    "        self.local_basis = self.basis_1 + self.basis_7 + self.basis_27\n",
    "\n",
    "    def _build_lambda3_1(self) -> List[torch.Tensor]:\n",
    "        \"\"\"Build the singlet basis (just phi normalized).\"\"\"\n",
    "        phi_norm = torch.sqrt((self.phi_canonical**2).sum())\n",
    "        return [self.phi_canonical / phi_norm]\n",
    "\n",
    "    def _build_lambda3_7(self) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Build the 7-dimensional basis from interior products.\n",
    "        For each direction v_i, form iota_{v_i}(*phi) which gives a 3-form in Lambda^3_7.\n",
    "        \"\"\"\n",
    "        basis_7 = []\n",
    "        psi = self._hodge_dual_phi(self.phi_canonical)  # *phi is a 4-form\n",
    "\n",
    "        for i in range(7):\n",
    "            # Interior product of v_i with *phi (contracts first index)\n",
    "            omega_i = psi[i, :, :, :]  # This gives a 3-form\n",
    "            # Normalize\n",
    "            norm = torch.sqrt((omega_i**2).sum() + 1e-12)\n",
    "            basis_7.append(omega_i / norm)\n",
    "\n",
    "        return basis_7\n",
    "\n",
    "    def _build_lambda3_27(self) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Build the 27-dimensional basis from symmetric traceless tensors.\n",
    "        These are constructed from wedge products dx^i ^ omega_j for i != j,\n",
    "        and combinations that are orthogonal to Lambda^3_1 and Lambda^3_7.\n",
    "        \"\"\"\n",
    "        basis_27 = []\n",
    "\n",
    "        # Use coordinate wedge products to span Lambda^3_27\n",
    "        # The 35 = C(7,3) coordinate 3-forms split as 1 + 7 + 27\n",
    "        # We orthogonalize to remove Lambda^3_1 and Lambda^3_7 components\n",
    "\n",
    "        for i in range(7):\n",
    "            for j in range(i+1, 7):\n",
    "                for k in range(j+1, 7):\n",
    "                    omega = torch.zeros(7, 7, 7, device=self.device, dtype=torch.float64)\n",
    "                    # Antisymmetrize dx^i ^ dx^j ^ dx^k\n",
    "                    omega[i, j, k] = 1.0\n",
    "                    omega[i, k, j] = -1.0\n",
    "                    omega[j, i, k] = -1.0\n",
    "                    omega[j, k, i] = 1.0\n",
    "                    omega[k, i, j] = 1.0\n",
    "                    omega[k, j, i] = -1.0\n",
    "                    basis_27.append(omega)\n",
    "\n",
    "        # Orthogonalize against Lambda^3_1 and Lambda^3_7\n",
    "        basis_27 = self._orthogonalize(basis_27, self.basis_1 + self.basis_7)\n",
    "\n",
    "        # Keep only 27 linearly independent forms\n",
    "        basis_27 = self._select_independent(basis_27, 27)\n",
    "\n",
    "        return basis_27\n",
    "\n",
    "    def _hodge_dual_phi(self, phi: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute *phi (Hodge dual of phi) giving a 4-form.\"\"\"\n",
    "        # For flat metric, *phi_{ijkl} = (1/6) * epsilon_{ijklmnp} * phi^{mnp}\n",
    "        # Simplified: use contraction formula\n",
    "        psi = torch.zeros(7, 7, 7, 7, device=self.device, dtype=torch.float64)\n",
    "\n",
    "        # Build *phi using the G2 identity: phi ^ phi = (4/3) * *phi * vol\n",
    "        # For simplicity, use direct construction from G2 structure\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                for k in range(7):\n",
    "                    for l in range(7):\n",
    "                        if len(set([i,j,k,l])) == 4:  # All indices distinct\n",
    "                            # *phi_{ijkl} = sum_m phi_{ijm} * phi_{klm} (schematic)\n",
    "                            val = 0.0\n",
    "                            for m in range(7):\n",
    "                                for n in range(7):\n",
    "                                    for p in range(7):\n",
    "                                        if m not in [i,j,k,l] and n not in [i,j,k,l] and p not in [i,j,k,l]:\n",
    "                                            val += phi[m,n,p].item() * self._epsilon_7(i,j,k,l,m,n,p)\n",
    "                            psi[i,j,k,l] = val / 6.0\n",
    "        return psi\n",
    "\n",
    "    def _epsilon_7(self, *indices) -> float:\n",
    "        \"\"\"Levi-Civita symbol in 7D.\"\"\"\n",
    "        if len(set(indices)) != 7:\n",
    "            return 0.0\n",
    "        perm = list(indices)\n",
    "        sign = 1\n",
    "        for i in range(7):\n",
    "            while perm[i] != i:\n",
    "                j = perm[i]\n",
    "                perm[i], perm[j] = perm[j], perm[i]\n",
    "                sign *= -1\n",
    "        return float(sign)\n",
    "\n",
    "    def _inner_product(self, a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "        \"\"\"Inner product of two 3-forms (flat metric).\"\"\"\n",
    "        return (a * b).sum().item()\n",
    "\n",
    "    def _orthogonalize(self, forms: List[torch.Tensor],\n",
    "                       against: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        \"\"\"Gram-Schmidt orthogonalization against a set of forms.\"\"\"\n",
    "        result = []\n",
    "        for omega in forms:\n",
    "            omega_orth = omega.clone()\n",
    "            for basis_form in against:\n",
    "                proj = self._inner_product(omega, basis_form)\n",
    "                omega_orth = omega_orth - proj * basis_form\n",
    "            norm = torch.sqrt((omega_orth**2).sum() + 1e-12)\n",
    "            if norm > 1e-6:\n",
    "                result.append(omega_orth / norm)\n",
    "        return result\n",
    "\n",
    "    def _select_independent(self, forms: List[torch.Tensor], n: int) -> List[torch.Tensor]:\n",
    "        \"\"\"Select n linearly independent forms via SVD.\"\"\"\n",
    "        if len(forms) <= n:\n",
    "            return forms\n",
    "\n",
    "        # Stack forms into matrix\n",
    "        mat = torch.stack([f.flatten() for f in forms])\n",
    "        U, S, Vh = torch.linalg.svd(mat, full_matrices=False)\n",
    "\n",
    "        # Select top n singular vectors\n",
    "        result = []\n",
    "        for i in range(min(n, len(S))):\n",
    "            if S[i] > 1e-10:\n",
    "                form_flat = Vh[i]\n",
    "                form = form_flat.reshape(7, 7, 7)\n",
    "                norm = torch.sqrt((form**2).sum())\n",
    "                result.append(form / norm)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_local_dim(self) -> int:\n",
    "        \"\"\"Return total local dimension.\"\"\"\n",
    "        return len(self.local_basis)\n",
    "\n",
    "    def expand_coefficients(self, alpha_1: torch.Tensor,\n",
    "                           alpha_7: torch.Tensor,\n",
    "                           alpha_27: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expand coefficients in the local basis to get a 3-form.\n",
    "\n",
    "        Args:\n",
    "            alpha_1: (batch,) coefficients for Lambda^3_1\n",
    "            alpha_7: (batch, 7) coefficients for Lambda^3_7\n",
    "            alpha_27: (batch, 27) coefficients for Lambda^3_27\n",
    "\n",
    "        Returns:\n",
    "            phi_local: (batch, 7, 7, 7) 3-forms\n",
    "        \"\"\"\n",
    "        batch = alpha_1.shape[0]\n",
    "        phi = torch.zeros(batch, 7, 7, 7, device=self.device, dtype=torch.float64)\n",
    "\n",
    "        # Lambda^3_1 contribution\n",
    "        phi += alpha_1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.basis_1[0]\n",
    "\n",
    "        # Lambda^3_7 contribution\n",
    "        for i, basis_form in enumerate(self.basis_7):\n",
    "            phi += alpha_7[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * basis_form\n",
    "\n",
    "        # Lambda^3_27 contribution\n",
    "        for i, basis_form in enumerate(self.basis_27):\n",
    "            if i < alpha_27.shape[1]:\n",
    "                phi += alpha_27[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * basis_form\n",
    "\n",
    "        return phi\n",
    "\n",
    "# Initialize the local basis\n",
    "print(\"Building Local G2 Basis...\")\n",
    "LOCAL_BASIS = LocalG2Basis(device)\n",
    "print(f\"  Lambda^3_1 basis: {len(LOCAL_BASIS.basis_1)} forms\")\n",
    "print(f\"  Lambda^3_7 basis: {len(LOCAL_BASIS.basis_7)} forms\")\n",
    "print(f\"  Lambda^3_27 basis: {len(LOCAL_BASIS.basis_27)} forms\")\n",
    "print(f\"  Total local basis: {LOCAL_BASIS.get_local_dim()} forms\")\n",
    "print(f\"  Canonical G2 phi: {int(PHI_CANONICAL.abs().sum().item())} non-zero entries\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBzJY9JNWlcC",
    "outputId": "a1541523-4385-47a7-faa8-9fac57e23e30"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building Local G2 Basis...\n",
      "  Lambda^3_1 basis: 1 forms\n",
      "  Lambda^3_7 basis: 7 forms\n",
      "  Lambda^3_27 basis: 27 forms\n",
      "  Total local basis: 35 forms\n",
      "  Canonical G2 phi: 42 non-zero entries\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Neural Network Architecture\n",
    "\n",
    "### LocalPhiNet: Outputs coefficients (alpha_1, alpha_7, alpha_27) for local 35-dim basis\n",
    "### GlobalCoeffNet: Outputs coefficients c for global 42-dim basis"
   ],
   "metadata": {
    "id": "dx9Gd_oaWlcD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FourierEncoding(nn.Module):\n",
    "    \"\"\"Fourier feature encoding for better high-frequency learning.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, n_features: int, scale: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        # Random Fourier features\n",
    "        B = torch.randn(input_dim, n_features) * scale\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, input_dim)\n",
    "        xB = torch.matmul(x, self.B)  # (batch, n_features)\n",
    "        return torch.cat([torch.sin(2 * np.pi * xB),\n",
    "                         torch.cos(2 * np.pi * xB)], dim=-1)\n",
    "\n",
    "\n",
    "class LocalPhiNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that outputs coefficients for the local G2 basis.\n",
    "\n",
    "    Input: x in [0,1]^7 (coordinates on K7)\n",
    "    Output: (alpha_1, alpha_7, alpha_27) coefficients for Lambda^3 decomposition\n",
    "\n",
    "    Total output dimension: 1 + 7 + 27 = 35\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict, sc: StructuralConstants):\n",
    "        super().__init__()\n",
    "        self.sc = sc\n",
    "        cfg = config['local_net']\n",
    "\n",
    "        # Fourier encoding\n",
    "        self.fourier = FourierEncoding(7, cfg['fourier_features'])\n",
    "        input_dim = 2 * cfg['fourier_features']  # sin + cos\n",
    "\n",
    "        # Build MLP\n",
    "        layers = []\n",
    "        hidden_dims = cfg['hidden_dims']\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.SiLU())\n",
    "            prev_dim = h_dim\n",
    "\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        # Separate heads for each representation\n",
    "        self.head_1 = nn.Linear(prev_dim, sc.dim_Lambda3_1)    # 1 output\n",
    "        self.head_7 = nn.Linear(prev_dim, sc.dim_Lambda3_7)    # 7 outputs\n",
    "        self.head_27 = nn.Linear(prev_dim, sc.dim_Lambda3_27)  # 27 outputs\n",
    "\n",
    "        # Initialize with small values for stability\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight, gain=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "        # Initialize singlet head to output ~1 (near canonical phi)\n",
    "        nn.init.constant_(self.head_1.bias, 1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, 7) coordinates\n",
    "\n",
    "        Returns:\n",
    "            alpha_1: (batch, 1) singlet coefficients\n",
    "            alpha_7: (batch, 7) fundamental coefficients\n",
    "            alpha_27: (batch, 27) traceless symmetric coefficients\n",
    "        \"\"\"\n",
    "        # Fourier encoding\n",
    "        h = self.fourier(x)\n",
    "\n",
    "        # MLP backbone\n",
    "        h = self.backbone(h)\n",
    "\n",
    "        # Separate heads\n",
    "        alpha_1 = self.head_1(h)     # (batch, 1)\n",
    "        alpha_7 = self.head_7(h)     # (batch, 7)\n",
    "        alpha_27 = self.head_27(h)   # (batch, 27)\n",
    "\n",
    "        return alpha_1.squeeze(-1), alpha_7, alpha_27\n",
    "\n",
    "    def get_phi_local(self, x: torch.Tensor, local_basis: LocalG2Basis) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the local phi component from coordinates.\n",
    "\n",
    "        Args:\n",
    "            x: (batch, 7) coordinates\n",
    "            local_basis: LocalG2Basis instance\n",
    "\n",
    "        Returns:\n",
    "            phi_local: (batch, 7, 7, 7) local 3-form\n",
    "        \"\"\"\n",
    "        alpha_1, alpha_7, alpha_27 = self.forward(x)\n",
    "        return local_basis.expand_coefficients(alpha_1, alpha_7, alpha_27)\n",
    "\n",
    "\n",
    "# Test LocalPhiNet\n",
    "print(\"Testing LocalPhiNet...\")\n",
    "local_net = LocalPhiNet(CONFIG, SC).to(device)\n",
    "test_x = torch.rand(16, 7, device=device, dtype=torch.float64)\n",
    "alpha_1, alpha_7, alpha_27 = local_net(test_x)\n",
    "print(f\"  Input shape: {test_x.shape}\")\n",
    "print(f\"  alpha_1 shape: {alpha_1.shape} (expected: [16])\")\n",
    "print(f\"  alpha_7 shape: {alpha_7.shape} (expected: [16, 7])\")\n",
    "print(f\"  alpha_27 shape: {alpha_27.shape} (expected: [16, 27])\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in local_net.parameters()):,}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RI4xxUktWlcD",
    "outputId": "324658fd-b756-46d3-eeb4-be09abe7b365"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing LocalPhiNet...\n",
      "  Input shape: torch.Size([16, 7])\n",
      "  alpha_1 shape: torch.Size([16]) (expected: [16])\n",
      "  alpha_7 shape: torch.Size([16, 7]) (expected: [16, 7])\n",
      "  alpha_27 shape: torch.Size([16, 27]) (expected: [16, 27])\n",
      "  Total parameters: 35,363\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. TCS Geometry and Global 3-Form Basis (42-dimensional)\n",
    "\n",
    "The TCS (Twisted Connected Sum) construction creates K7 by gluing two ACyl blocks:\n",
    "- M1 (left block): S1 x CY3_1\n",
    "- M2 (right block): S1 x CY3_2\n",
    "- Neck region: where the blocks are glued with a hyper-Kahler twist\n",
    "\n",
    "The 42 global modes come from forms that have non-trivial support across the neck\n",
    "and cannot be written as pure T7 wedge products in any single chart."
   ],
   "metadata": {
    "id": "HjYKelIdWlcD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TCSGeometry:\n    \"\"\"Twisted Connected Sum (TCS) K7 geometry.\"\"\"\n\n    def __init__(self, config: Dict, sc: StructuralConstants, zpg: ZeroParamGeometry):\n        self.config = config\n        self.sc = sc\n        self.zpg = zpg\n        tcs = config['tcs']\n        self.L = tcs['neck_half_length']\n        self.neck_width = tcs['neck_width']\n        self.twist_angle = tcs['twist_angle']\n\n    def neck_coordinate(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"lambda in [-L, L] from x[0] in [0,1]\"\"\"\n        return 2 * self.L * (x[:, 0] - 0.5)\n\n    def region_indicators(self, lam: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"Smooth indicators for M1, neck, M2 regions.\"\"\"\n        w = self.neck_width\n        left_to_neck = 0.5 * (1 + torch.tanh((lam + w) / (w/3)))\n        neck_to_right = 0.5 * (1 + torch.tanh((lam - w) / (w/3)))\n        return {\n            'M1': 1 - left_to_neck,\n            'neck': left_to_neck * (1 - neck_to_right),\n            'M2': neck_to_right\n        }\n\n\nclass GlobalSpatialProfiles:\n    \"\"\"\n    v1.6: Global modes via SVD-orthonormalized spatial profiles.\n    \n    Strategy:\n    1. Generate large pool of candidate profile functions (~100)\n    2. Compute Gram matrix G = F^T @ F / N_points\n    3. Eigendecompose to get orthonormal basis\n    4. Keep top 42 eigenvectors as independent profiles\n    \n    This guarantees 42 linearly independent profiles by construction.\n    \"\"\"\n    \n    def __init__(self, tcs: TCSGeometry, sc: StructuralConstants, \n                 n_samples: int = 8192, device_=device):\n        self.tcs = tcs\n        self.sc = sc\n        self.device = device_\n        self.n_global = sc.global_dim  # 42\n        self.n_fiber = 35\n        self.n_samples = n_samples\n        \n        # Compute orthonormal basis from candidate pool\n        self.projection_matrix = None  # (n_candidates, 42)\n        self.n_candidates = 0\n        self._initialize_orthonormal_basis()\n    \n    def _generate_candidate_pool(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Generate large pool of candidate profile functions.\n        \n        Args:\n            x: (batch, 7) coordinates\n            \n        Returns:\n            F: (batch, n_candidates) candidate profile values\n        \"\"\"\n        batch = x.shape[0]\n        lam = self.tcs.neck_coordinate(x)  # (batch,)\n        regions = self.tcs.region_indicators(lam)\n        \n        chi_L = regions['M1']\n        chi_R = regions['M2']\n        chi_neck = regions['neck']\n        \n        # Normalized lambda in [0, 1]\n        lam_norm = (lam + self.tcs.L) / (2 * self.tcs.L)\n        \n        candidates = []\n        \n        # === 1. Constant and powers of lambda (5) ===\n        candidates.append(torch.ones(batch, device=self.device, dtype=torch.float64))\n        for k in range(1, 5):\n            candidates.append(lam_norm ** k)\n        \n        # === 2. All 7 coordinates (7) ===\n        for i in range(7):\n            candidates.append(x[:, i])\n        \n        # === 3. Region indicators (3) ===\n        candidates.append(chi_L)\n        candidates.append(chi_R)\n        candidates.append(chi_neck)\n        \n        # === 4. Region \u00d7 lambda powers (12) ===\n        for k in range(1, 5):\n            p = lam_norm ** k\n            candidates.append(chi_L * p)\n            candidates.append(chi_R * p)\n            candidates.append(chi_neck * p)\n        \n        # === 5. Region \u00d7 coordinates (21) ===\n        for i in range(7):\n            xi = x[:, i]\n            candidates.append(chi_L * xi)\n            candidates.append(chi_R * xi)\n            candidates.append(chi_neck * xi)\n        \n        # === 6. Antisymmetric M1-M2 (7) ===\n        for i in range(7):\n            xi = x[:, i]\n            candidates.append(chi_L * xi - chi_R * xi)\n        \n        # === 7. Lambda \u00d7 coordinates (7) ===\n        for i in range(7):\n            candidates.append(lam_norm * x[:, i])\n        \n        # === 8. Coordinate products (21 = C(7,2)) ===\n        for i in range(7):\n            for j in range(i+1, 7):\n                candidates.append(x[:, i] * x[:, j])\n        \n        # === 9. Fourier modes (8) ===\n        for k in range(1, 5):\n            candidates.append(torch.sin(k * np.pi * lam_norm))\n            candidates.append(torch.cos(k * np.pi * lam_norm))\n        \n        # === 10. Fourier \u00d7 region (12) ===\n        for k in range(1, 3):\n            sin_k = torch.sin(k * np.pi * lam_norm)\n            cos_k = torch.cos(k * np.pi * lam_norm)\n            candidates.append(chi_L * sin_k)\n            candidates.append(chi_R * sin_k)\n            candidates.append(chi_neck * sin_k)\n            candidates.append(chi_L * cos_k)\n            candidates.append(chi_R * cos_k)\n            candidates.append(chi_neck * cos_k)\n        \n        # === 11. Radial terms (7) ===\n        r_sq = (x ** 2).sum(dim=1)  # |x|^2\n        candidates.append(r_sq)\n        candidates.append(chi_L * r_sq)\n        candidates.append(chi_R * r_sq)\n        candidates.append(chi_neck * r_sq)\n        candidates.append(lam_norm * r_sq)\n        r = torch.sqrt(r_sq + 1e-12)\n        candidates.append(r)\n        candidates.append(chi_neck * r)\n        \n        # Stack all candidates\n        F = torch.stack(candidates, dim=1)  # (batch, n_candidates)\n        return F\n    \n    def _initialize_orthonormal_basis(self):\n        \"\"\"\n        Compute orthonormal projection matrix from candidate pool.\n        Uses eigendecomposition of Gram matrix.\n        \"\"\"\n        print(f\"  [v1.6] Initializing orthonormal profile basis...\")\n        \n        # Generate sample points\n        x_samples = torch.rand(self.n_samples, 7, device=self.device, dtype=torch.float64)\n        \n        # Generate candidate pool\n        F = self._generate_candidate_pool(x_samples)  # (n_samples, n_candidates)\n        self.n_candidates = F.shape[1]\n        print(f\"  [v1.6] Generated {self.n_candidates} candidate profiles\")\n        \n        # Center the data\n        F_centered = F - F.mean(dim=0, keepdim=True)\n        \n        # Compute Gram matrix\n        G = F_centered.T @ F_centered / self.n_samples  # (n_cand, n_cand)\n        \n        # Eigendecomposition\n        eigvals, eigvecs = torch.linalg.eigh(G)\n        \n        # Sort by descending eigenvalue\n        idx = torch.argsort(eigvals, descending=True)\n        eigvals = eigvals[idx]\n        eigvecs = eigvecs[:, idx]\n        \n        # Report eigenvalue spectrum\n        print(f\"  [v1.6] Eigenvalue spectrum:\")\n        print(f\"         Top 5: {eigvals[:5].cpu().numpy()}\")\n        print(f\"         #40-45: {eigvals[38:45].cpu().numpy()}\")\n        \n        # Count significant eigenvalues\n        threshold = 1e-8 * eigvals[0]\n        n_significant = (eigvals > threshold).sum().item()\n        print(f\"  [v1.6] Significant eigenvalues: {n_significant} (threshold: {threshold:.2e})\")\n        \n        if n_significant < self.n_global:\n            print(f\"  [WARN] Only {n_significant} independent profiles, need {self.n_global}\")\n        \n        # Keep top 42 eigenvectors as projection matrix\n        self.projection_matrix = eigvecs[:, :self.n_global]  # (n_cand, 42)\n        self.eigvals = eigvals[:self.n_global]\n        \n        print(f\"  [v1.6] Orthonormal basis ready: {self.n_candidates} -> {self.n_global}\")\n    \n    def compute_profiles(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute 42 orthonormal spatial profiles at each point.\n        \n        Args:\n            x: (batch, 7) coordinates\n            \n        Returns:\n            (batch, 42) orthonormal profile values\n        \"\"\"\n        # Generate candidates at these points\n        F = self._generate_candidate_pool(x)  # (batch, n_candidates)\n        \n        # Center using stored mean (approximate)\n        F_centered = F - F.mean(dim=0, keepdim=True)\n        \n        # Project onto orthonormal basis\n        profiles = F_centered @ self.projection_matrix  # (batch, 42)\n        \n        return profiles\n    \n    def compute_fiber_weights(self) -> torch.Tensor:\n        \"\"\"\n        Coupling matrix from 42 global modes to 35 fiber basis forms.\n        Initialized with structured pattern, learned during training.\n        \"\"\"\n        W = torch.zeros(self.n_global, self.n_fiber, device=self.device, dtype=torch.float64)\n        \n        # Distribute global modes across fiber representations\n        # Fiber basis: [0] = singlet, [1:8] = 7-rep, [8:35] = 27-rep\n        \n        for a in range(self.n_global):\n            # Each global mode couples to multiple fiber directions\n            if a < 1:\n                # Mode 0: singlet coupling\n                W[a, 0] = 1.0\n            elif a < 15:\n                # Modes 1-14: 7-rep coupling (2 copies \u00d7 7)\n                fiber_idx = 1 + (a - 1) % 7\n                W[a, fiber_idx] = 1.0\n            else:\n                # Modes 15-41: 27-rep coupling\n                fiber_idx = 8 + (a - 15) % 27\n                W[a, fiber_idx] = 1.0\n        \n        return W\n\nclass GlobalBasis:\n    \"\"\"\n    v1.6: Global modes as spatial profiles over fiber basis.\n\n    phi_global(x) = sum_{a=1}^{42} c_a * f_a(x) * sum_I W[a,I] * e_I\n\n    where:\n    - c_a: learned coefficient from GlobalCoeffNet\n    - f_a(x): spatial profile (chi_L, chi_R, polynomials, etc.)\n    - W[a,I]: fiber coupling weights\n    - e_I: the 35 basis 3-forms (dx^i ^ dx^j ^ dx^k)\n    \"\"\"\n\n    def __init__(self, tcs: TCSGeometry, local_basis: LocalG2Basis,\n                 sc: StructuralConstants, device_=device):\n        self.tcs = tcs\n        self.local_basis = local_basis\n        self.sc = sc\n        self.device = device_\n\n        self.n_global = sc.global_dim  # 42\n        self.n_fiber = 35\n\n        # Spatial profiles\n        self.profiles = GlobalSpatialProfiles(tcs, sc, device_)\n\n        # Fiber coupling weights (42 x 35)\n        self.fiber_weights = self.profiles.compute_fiber_weights()\n\n        # Build the 35 fiber basis forms\n        self.fiber_basis = self._build_fiber_basis()\n\n        print(f\"  [GlobalBasis v1.6] 42 spatial profiles over 35-dim fiber\")\n\n    def _build_fiber_basis(self) -> List[torch.Tensor]:\n        \"\"\"Build the 35 basis 3-forms e_I = dx^i ^ dx^j ^ dx^k.\"\"\"\n        basis = []\n        for i in range(7):\n            for j in range(i+1, 7):\n                for k in range(j+1, 7):\n                    form = torch.zeros(7, 7, 7, device=self.device, dtype=torch.float64)\n                    form[i, j, k] = 1.0\n                    form[i, k, j] = -1.0\n                    form[j, i, k] = -1.0\n                    form[j, k, i] = 1.0\n                    form[k, i, j] = 1.0\n                    form[k, j, i] = -1.0\n                    # Normalize\n                    norm = torch.sqrt((form**2).sum())\n                    basis.append(form / norm)\n        return basis\n\n    def expand_coefficients(self, c: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Expand global coefficients to 3-forms using spatial profiles.\n\n        Args:\n            c: (batch, 42) learned global coefficients\n            x: (batch, 7) coordinates for spatial profile evaluation\n\n        Returns:\n            phi_global: (batch, 7, 7, 7) global 3-form contribution\n        \"\"\"\n        batch = c.shape[0]\n\n        # Get spatial profiles at these points: (batch, 42)\n        f = self.profiles.compute_profiles(x)\n\n        # Combine: weighted_c[batch, a] = c[batch, a] * f[batch, a]\n        weighted_c = c * f  # (batch, 42)\n\n        # Project to fiber: (batch, 35) = (batch, 42) @ (42, 35)\n        fiber_coeffs = weighted_c @ self.fiber_weights  # (batch, 35)\n\n        # Expand in fiber basis\n        phi = torch.zeros(batch, 7, 7, 7, device=self.device, dtype=torch.float64)\n        for I, e_I in enumerate(self.fiber_basis):\n            phi += fiber_coeffs[:, I].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * e_I\n\n        return phi\n\n    def get_global_dim(self) -> int:\n        return self.n_global\n\n\n# Initialize TCS geometry and global basis\nprint(\"Building TCS Geometry and Global Basis (v1.6 with spatial profiles)...\")\nTCS = TCSGeometry(CONFIG, SC, ZPG)\nGLOBAL_BASIS = GlobalBasis(TCS, LOCAL_BASIS, SC, device)\nprint(f\"  TCS neck width: {TCS.neck_width}\")\nprint(f\"  Fiber dimension: 35 (Lambda^3 R^7)\")\nprint(f\"  Global profiles: 42 (spatial functions)\")\nprint(f\"  Total global modes: {GLOBAL_BASIS.get_global_dim()}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LaK7fg63WlcD",
    "outputId": "6d489821-3534-422f-eea7-58990f8d96ae"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building TCS Geometry and Global Basis (v1.5c with spatial profiles)...\n",
      "  [GlobalBasis v1.5c] 42 spatial profiles over 35-dim fiber\n",
      "  TCS neck width: 0.3\n",
      "  Fiber dimension: 35 (Lambda^3 R^7)\n",
      "  Global profiles: 42 (spatial functions)\n",
      "  Total global modes: 42\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class GlobalCoeffNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that outputs coefficients for the global TCS basis.\n",
    "\n",
    "    Input: x in [0,1]^7 (coordinates), lambda (neck coordinate), region indicators\n",
    "    Output: c (coefficients for 42-dimensional global basis)\n",
    "\n",
    "    This network is smaller than LocalPhiNet since the basis already encodes\n",
    "    most of the geometric information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict, sc: StructuralConstants, tcs: TCSGeometry):\n",
    "        super().__init__()\n",
    "        self.sc = sc\n",
    "        self.tcs = tcs\n",
    "        cfg = config['global_net']\n",
    "\n",
    "        # Fourier encoding (smaller than local)\n",
    "        self.fourier = FourierEncoding(7, cfg['fourier_features'])\n",
    "        input_dim = 2 * cfg['fourier_features'] + 4  # +4 for lambda and region indicators\n",
    "\n",
    "        # Build MLP (smaller network)\n",
    "        layers = []\n",
    "        hidden_dims = cfg['hidden_dims']\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.SiLU())\n",
    "            prev_dim = h_dim\n",
    "\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        # Output head for 42 global coefficients\n",
    "        self.head = nn.Linear(prev_dim, sc.global_dim)  # 42 outputs\n",
    "\n",
    "        # Initialize near zero (global is a correction to local)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight, gain=0.01)  # Very small\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, 7) coordinates\n",
    "\n",
    "        Returns:\n",
    "            c: (batch, 42) global coefficients\n",
    "        \"\"\"\n",
    "        # Get neck coordinate and region indicators\n",
    "        lam = self.tcs.neck_coordinate(x)\n",
    "        regions = self.tcs.region_indicators(lam)\n",
    "\n",
    "        # Fourier encoding\n",
    "        h = self.fourier(x)\n",
    "\n",
    "        # Concatenate with geometric features\n",
    "        geo_features = torch.stack([\n",
    "            lam,\n",
    "            regions['M1'],\n",
    "            regions['neck'],\n",
    "            regions['M2']\n",
    "        ], dim=-1)\n",
    "        h = torch.cat([h, geo_features], dim=-1)\n",
    "\n",
    "        # MLP backbone\n",
    "        h = self.backbone(h)\n",
    "\n",
    "        # Output coefficients\n",
    "        c = self.head(h)\n",
    "\n",
    "        # Modulate by neck indicator (global modes concentrated in neck)\n",
    "        neck_weight = regions['neck'].unsqueeze(-1)\n",
    "        c = c * (0.3 + 0.7 * neck_weight)  # Some support everywhere, more in neck\n",
    "\n",
    "        return c\n",
    "\n",
    "    def get_phi_global(self, x: torch.Tensor, global_basis: GlobalBasis) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the global phi component from coordinates.\n",
    "\n",
    "        Args:\n",
    "            x: (batch, 7) coordinates\n",
    "            global_basis: GlobalBasis instance\n",
    "\n",
    "        Returns:\n",
    "            phi_global: (batch, 7, 7, 7) global 3-form\n",
    "        \"\"\"\n",
    "        c = self.forward(x)\n",
    "        return global_basis.expand_coefficients(c)\n",
    "\n",
    "\n",
    "# Test GlobalCoeffNet\n",
    "print(\"Testing GlobalCoeffNet...\")\n",
    "global_net = GlobalCoeffNet(CONFIG, SC, TCS).to(device)\n",
    "test_x = torch.rand(16, 7, device=device, dtype=torch.float64)\n",
    "c = global_net(test_x)\n",
    "print(f\"  Input shape: {test_x.shape}\")\n",
    "print(f\"  c shape: {c.shape} (expected: [16, 42])\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in global_net.parameters()):,}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8msJFphnWlcD",
    "outputId": "a4387222-3332-4338-b1c7-54fcbf33ae97"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing GlobalCoeffNet...\n",
      "  Input shape: torch.Size([16, 7])\n",
      "  c shape: torch.Size([16, 42]) (expected: [16, 42])\n",
      "  Total parameters: 9,258\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Combined Phi and Metric Computation\n",
    "\n",
    "The full G2 3-form is:\n",
    "```\n",
    "phi(x) = phi_local(x) + phi_global(x)\n",
    "```\n",
    "\n",
    "The induced metric g is computed from phi via the G2 structure:\n",
    "```\n",
    "g_{ij} = (1/7) * (phi ^ *phi)_{ij...} / vol^{6/7}\n",
    "```"
   ],
   "metadata": {
    "id": "K6D3FnfdWlcE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CombinedG2Model(nn.Module):\n    \"\"\"\n    Combined model: phi = phi_local + phi_global\n\n    v1.6: Global uses spatial profiles, requires x for expand_coefficients.\n    \"\"\"\n\n    def __init__(self, local_net: LocalPhiNet, global_net: GlobalCoeffNet,\n                 local_basis: LocalG2Basis, global_basis: GlobalBasis,\n                 zpg: ZeroParamGeometry):\n        super().__init__()\n        self.local_net = local_net\n        self.global_net = global_net\n        self.local_basis = local_basis\n        self.global_basis = global_basis\n        self.zpg = zpg\n\n    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute full phi and derived quantities.\n\n        Args:\n            x: (batch, 7) coordinates\n\n        Returns:\n            dict with phi_local, phi_global, phi_total, g, det_g, torsion, coefficients\n        \"\"\"\n        # Local component (35-dim via Lambda^3 decomposition)\n        alpha_1, alpha_7, alpha_27 = self.local_net(x)\n        phi_local = self.local_basis.expand_coefficients(alpha_1, alpha_7, alpha_27)\n\n        # Global component (42-dim via spatial profiles)\n        # v1.6: pass x for spatial profile evaluation\n        c = self.global_net(x)\n        phi_global = self.global_basis.expand_coefficients(c, x)  # Now uses x!\n\n        # Combined phi\n        phi_total = phi_local + phi_global\n\n        # Compute metric from phi\n        g = self._phi_to_metric(phi_total)\n\n        # Compute determinant\n        det_g = torch.linalg.det(g)\n\n        # Compute torsion\n        torsion = self._compute_torsion(phi_total, x)\n\n        return {\n            'phi_local': phi_local,\n            'phi_global': phi_global,\n            'phi_total': phi_total,\n            'g': g,\n            'det_g': det_g,\n            'torsion': torsion,\n            'alpha_1': alpha_1,\n            'alpha_7': alpha_7,\n            'alpha_27': alpha_27,\n            'c': c,\n        }\n\n    def _phi_to_metric(self, phi: torch.Tensor) -> torch.Tensor:\n        \"\"\"Derive metric g from G2 3-form phi via contraction.\"\"\"\n        batch = phi.shape[0]\n        g = torch.zeros(batch, 7, 7, device=phi.device, dtype=phi.dtype)\n\n        for i in range(7):\n            for j in range(7):\n                val = torch.einsum('bkl,bkl->b', phi[:, i, :, :], phi[:, j, :, :])\n                g[:, i, j] = val\n\n        # Symmetrize\n        g = 0.5 * (g + g.transpose(-1, -2))\n\n        # Normalize to target determinant\n        current_det = torch.linalg.det(g).unsqueeze(-1).unsqueeze(-1)\n        target_det = self.zpg.det_g_target\n        scale = (target_det / (current_det.abs() + 1e-12)) ** (1/7)\n        g = g * scale\n\n        # Ensure SPD\n        g = self._ensure_spd(g)\n\n        return g\n\n    def _ensure_spd(self, g: torch.Tensor, min_eig: float = 0.01) -> torch.Tensor:\n        \"\"\"Ensure metric is symmetric positive definite.\"\"\"\n        eigenvalues, eigenvectors = torch.linalg.eigh(g)\n        eigenvalues = torch.clamp(eigenvalues, min=min_eig)\n        g_spd = eigenvectors @ torch.diag_embed(eigenvalues) @ eigenvectors.transpose(-1, -2)\n        return g_spd\n\n    def _compute_torsion(self, phi: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute torsion magnitude via finite differences.\"\"\"\n        batch = phi.shape[0]\n        eps = 1e-4\n        d_phi_sq = torch.zeros(batch, device=phi.device, dtype=phi.dtype)\n\n        for dim in range(7):\n            x_plus = x.clone()\n            x_plus[:, dim] = x_plus[:, dim] + eps\n            x_minus = x.clone()\n            x_minus[:, dim] = x_minus[:, dim] - eps\n\n            # Local phi derivatives (for speed)\n            alpha_1_p, alpha_7_p, alpha_27_p = self.local_net(x_plus)\n            phi_plus = self.local_basis.expand_coefficients(alpha_1_p, alpha_7_p, alpha_27_p)\n\n            alpha_1_m, alpha_7_m, alpha_27_m = self.local_net(x_minus)\n            phi_minus = self.local_basis.expand_coefficients(alpha_1_m, alpha_7_m, alpha_27_m)\n\n            dphi_dim = (phi_plus - phi_minus) / (2 * eps)\n            d_phi_sq = d_phi_sq + (dphi_dim ** 2).sum(dim=(-1, -2, -3))\n\n        torsion = torch.sqrt(d_phi_sq + 1e-12)\n        return torsion\n\n\n# Create combined model\nprint(\"Creating Combined G2 Model (v1.6)...\")\nmodel = CombinedG2Model(local_net, global_net, LOCAL_BASIS, GLOBAL_BASIS, ZPG).to(device)\nprint(f\"  Local net params: {sum(p.numel() for p in local_net.parameters()):,}\")\nprint(f\"  Global net params: {sum(p.numel() for p in global_net.parameters()):,}\")\nprint(f\"  Total params: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\ntest_out = model(test_x)\nprint(f\"\\nTest forward pass:\")\nprint(f\"  phi_local shape: {test_out['phi_local'].shape}\")\nprint(f\"  phi_global shape: {test_out['phi_global'].shape}\")\nprint(f\"  phi_total shape: {test_out['phi_total'].shape}\")\nprint(f\"  g shape: {test_out['g'].shape}\")\nprint(f\"  det_g mean: {test_out['det_g'].mean().item():.4f} (target: {ZPG.det_g_target:.4f})\")\nprint(f\"  torsion mean: {test_out['torsion'].mean().item():.4f} (target: {ZPG.kappa_T:.4f})\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Wfp7tjMWlcE",
    "outputId": "3972da68-3bb4-4b21-9b89-62c2fa6a9cc5"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating Combined G2 Model (v1.5c)...\n",
      "  Local net params: 35,363\n",
      "  Global net params: 9,258\n",
      "  Total params: 44,621\n",
      "\n",
      "Test forward pass:\n",
      "  phi_local shape: torch.Size([16, 7, 7, 7])\n",
      "  phi_global shape: torch.Size([16, 7, 7, 7])\n",
      "  phi_total shape: torch.Size([16, 7, 7, 7])\n",
      "  g shape: torch.Size([16, 7, 7])\n",
      "  det_g mean: 2.0312 (target: 2.0312)\n",
      "  torsion mean: 0.0019 (target: 0.0164)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### Architecture Implemented\n",
    "\n",
    "The v1.5 notebook implements the **local/global decomposition** of H3(K7):\n",
    "\n",
    "| Component | Dimension | Network | Basis |\n",
    "|-----------|-----------|---------|-------|\n",
    "| Local (T7-like) | 35 | LocalPhiNet | Lambda3_1 + Lambda3_7 + Lambda3_27 |\n",
    "| Global (TCS) | 42 | GlobalCoeffNet | Neck-localized modes |\n",
    "| **Total** | **77** | **Combined** | **Full H3(K7)** |\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Zero-parameter foundation**: All physical quantities derived from topological integers\n",
    "2. **Explicit G2 decomposition**: 1 + 7 + 27 = 35 local modes\n",
    "3. **TCS-aware global basis**: 42 modes respecting twisted connected sum topology\n",
    "4. **Combined model**: phi = phi_local + phi_global with automatic metric derivation\n",
    "\n",
    "### Targets\n",
    "\n",
    "- kappa_T = 1/61 = 0.0164\n",
    "- det(g) = 65/32 = 2.0312\n",
    "- b2_eff = 21\n",
    "- b3_eff_local = 35\n",
    "- b3_eff_global = 42\n",
    "- b3_eff_total = 77\n",
    "- Representation decomposition: (2, 21, 54)\n",
    "\n",
    "### TODO (to be added in subsequent cells)\n",
    "\n",
    "- [ ] Loss functions for kappa_T, det_g, closure, coclosure, G2 consistency\n",
    "- [ ] Multi-phase training loop\n",
    "- [ ] Harmonic extraction (Gram matrix computation for b2, b3)\n",
    "- [ ] Representation diagnostics (2, 21, 54 projection)\n",
    "- [ ] Output saving (models, metrics, metadata)"
   ],
   "metadata": {
    "id": "_oczNPscWlcE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Loss Functions\n",
    "\n",
    "Loss components:\n",
    "1. **kappa_T loss**: Match torsion magnitude to 1/61\n",
    "2. **det_g loss**: Match metric determinant to 65/32\n",
    "3. **closure loss**: Minimize ||d(phi)||\n",
    "4. **coclosure loss**: Minimize ||d*(phi)||\n",
    "5. **G2 consistency**: Preserve G2 structure (phi ^ *phi proportional to vol)\n",
    "6. **Local/global balance**: Regularize relative magnitudes\n",
    "7. **SPD enforcement**: Ensure metric positive definiteness"
   ],
   "metadata": {
    "id": "Wf3BW_R6WlcE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GIFTLossFunctions:\n    \"\"\"\n    Loss functions for training the G2 model.\n    v1.6: Separated torsion losses for local/global components.\n\n    All target values come from ZeroParamGeometry (no free parameters).\n    \"\"\"\n\n    def __init__(self, zpg: ZeroParamGeometry, sc: StructuralConstants, config: Dict):\n        self.zpg = zpg\n        self.sc = sc\n        self.weights = config['loss_weights']\n        self.kappa_T_ref = config.get('v14_kappa_T_ref', zpg.kappa_T)\n\n    def compute_torsion_norm(self, phi: torch.Tensor, x: torch.Tensor,\n                             model: nn.Module = None) -> torch.Tensor:\n        \"\"\"\n        Compute torsion magnitude ||T|| for a given phi.\n        Uses phi norm variation as proxy for d(phi).\n        \"\"\"\n        # Simplified torsion: deviation from canonical G2 norm\n        phi_norm = torch.sqrt((phi**2).sum(dim=(-1, -2, -3)) + 1e-12)\n        target_norm = 6.48  # sqrt(42) for properly normalized phi\n\n        # Torsion ~ relative deviation scaled by kappa_T target\n        torsion = torch.abs(phi_norm - target_norm) / target_norm * 0.1\n        return torsion\n\n    def kappa_T_loss(self, torsion: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Loss for matching total torsion magnitude to kappa_T = 1/61.\n        v1.6: Combined absolute + relative error for robust convergence.\n        \"\"\"\n        target = self.zpg.kappa_T  # 0.016393\n        mean_torsion = torsion.mean()\n\n        # Absolute MSE (scaled up)\n        abs_loss = self.weights['kappa_T'] * (mean_torsion - target)**2\n\n        # Relative error: penalize deviation as fraction of target\n        # This ensures kT=0.19 is penalized much more than kT=0.016\n        rel_error = (mean_torsion / target - 1.0)**2\n        rel_loss = self.weights.get('kappa_relative', 500.0) * rel_error\n\n        return abs_loss + rel_loss\n\n    def local_anchor_loss(self, phi_local: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        v1.6: Keep local phi close to v1.4 solution.\n        Prevents local network from drifting when training global.\n        \"\"\"\n        # Target: local torsion should stay near v1.4 value\n        T_local = self.compute_torsion_norm(phi_local, x)\n        T_local_mean = T_local.mean()\n\n        # Anchor to reference kappa_T from v1.4\n        anchor_loss = (T_local_mean - self.kappa_T_ref)**2\n\n        return self.weights.get('local_anchor', 20.0) * anchor_loss\n\n    def global_torsion_loss(self, phi_global: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        v1.6: Heavily penalize torsion from global component.\n        Global modes should NOT generate large d(phi_global).\n        \"\"\"\n        T_global = self.compute_torsion_norm(phi_global, x)\n        T_global_mean = T_global.mean()\n\n        # Penalize any torsion from global (should be near zero)\n        global_torsion_loss = T_global_mean**2\n\n        return self.weights.get('global_torsion', 50.0) * global_torsion_loss\n\n    def mode_activation_loss(self, c: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        v1.6: Encourage all 42 global modes to be active.\n        Prevents modes from collapsing to zero.\n        \"\"\"\n        # Variance of each mode coefficient across batch\n        mode_vars = c.var(dim=0)  # (42,)\n\n        # Penalize modes with near-zero variance (inactive)\n        min_var = 1e-4\n        inactive_penalty = F.relu(min_var - mode_vars).sum()\n\n        # Also reward overall activity\n        mean_var = mode_vars.mean()\n        activity_bonus = -torch.log(mean_var + 1e-12) * 0.01\n\n        return self.weights.get('mode_activation', 0.1) * (inactive_penalty + activity_bonus)\n\n    def det_g_loss(self, det_g: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for matching metric determinant to 65/32.\"\"\"\n        target = self.zpg.det_g_target\n        mean_det = det_g.mean()\n        var_det = det_g.var()\n        return self.weights['det_g'] * ((mean_det - target)**2 + 0.1 * var_det)\n\n    def closure_loss(self, phi: torch.Tensor, x: torch.Tensor, model: nn.Module) -> torch.Tensor:\n        \"\"\"Loss for d(phi) = 0 (closure condition).\"\"\"\n        eps = 1e-4\n        d_phi_norm_sq = torch.zeros(phi.shape[0], device=phi.device, dtype=phi.dtype)\n\n        for dim in range(7):\n            x_plus = x.clone()\n            x_plus[:, dim] += eps\n            x_minus = x.clone()\n            x_minus[:, dim] -= eps\n\n            with torch.no_grad():\n                out_plus = model(x_plus)\n                out_minus = model(x_minus)\n\n            dphi = (out_plus['phi_total'] - out_minus['phi_total']) / (2 * eps)\n            d_phi_norm_sq += (dphi**2).sum(dim=(-1, -2, -3))\n\n        return self.weights['closure'] * d_phi_norm_sq.mean()\n\n    def coclosure_loss(self, phi: torch.Tensor, g: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for d*(phi) = 0 (coclosure condition).\"\"\"\n        g_inv = torch.linalg.inv(g)\n        contracted = torch.einsum('bij,bjkl->bikl', g_inv, phi)\n        coclosure_measure = (contracted**2).sum(dim=(-1, -2, -3))\n        return self.weights['coclosure'] * coclosure_measure.mean()\n\n    def g2_consistency_loss(self, phi: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss for G2 structure preservation.\"\"\"\n        phi_norm_sq = (phi**2).sum(dim=(-1, -2, -3))\n        target_norm_sq = 7.0 * 6.0\n        consistency = (phi_norm_sq - target_norm_sq)**2\n        return self.weights['g2_consistency'] * consistency.mean()\n\n    def local_global_balance_loss(self, phi_local: torch.Tensor,\n                                   phi_global: torch.Tensor) -> torch.Tensor:\n        \"\"\"Regularize balance between local and global components.\"\"\"\n        local_norm = (phi_local**2).sum(dim=(-1, -2, -3)).mean()\n        global_norm = (phi_global**2).sum(dim=(-1, -2, -3)).mean()\n\n        ratio = global_norm / (local_norm + 1e-12)\n        target_ratio = 0.2\n        balance_loss = (ratio - target_ratio)**2\n        global_activity = -torch.log(global_norm + 1e-12) * 0.01\n\n        return self.weights['local_global_balance'] * (balance_loss + global_activity)\n\n    def spd_loss(self, g: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss to enforce positive definiteness of metric.\"\"\"\n        eigenvalues = torch.linalg.eigvalsh(g)\n        min_eigenvalues = eigenvalues.min(dim=-1)[0]\n        negative_penalty = F.relu(-min_eigenvalues + 0.01)**2\n        return self.weights['spd'] * negative_penalty.mean()\n\n    def total_loss(self, model_output: Dict[str, torch.Tensor],\n                   x: torch.Tensor, model: nn.Module,\n                   phase: str = 'both') -> Tuple[torch.Tensor, Dict[str, float]]:\n        \"\"\"\n        Compute total loss with all components.\n\n        v1.6 phases:\n        - 'global_only': Train only global, local frozen\n        - 'both': Train both with anchor losses\n        \"\"\"\n        losses = {}\n\n        # Core losses (always active)\n        losses['kappa_T'] = self.kappa_T_loss(model_output['torsion'])\n        losses['det_g'] = self.det_g_loss(model_output['det_g'])\n        losses['g2_consistency'] = self.g2_consistency_loss(model_output['phi_total'])\n        losses['spd'] = self.spd_loss(model_output['g'])\n\n        # v1.6: Separated torsion losses\n        losses['local_anchor'] = self.local_anchor_loss(\n            model_output['phi_local'], x)\n        losses['global_torsion'] = self.global_torsion_loss(\n            model_output['phi_global'], x)\n\n        # Mode activation (encourage all 42 modes)\n        if 'c' in model_output:\n            losses['mode_activation'] = self.mode_activation_loss(model_output['c'])\n\n        # Phase-dependent losses\n        if phase in ['both']:\n            losses['closure'] = self.closure_loss(\n                model_output['phi_total'], x, model) * 0.1\n            losses['local_global_balance'] = self.local_global_balance_loss(\n                model_output['phi_local'], model_output['phi_global'])\n\n        # Total\n        total = sum(losses.values())\n\n        # Convert to float dict for logging\n        loss_dict = {k: v.item() for k, v in losses.items()}\n        loss_dict['total'] = total.item()\n\n        return total, loss_dict\n\n\n# Initialize loss functions\nloss_fn = GIFTLossFunctions(ZPG, SC, CONFIG)\nprint(\"Loss functions initialized (v1.6 with torsion separation)\")\nprint(f\"  kappa_T reference: {CONFIG.get('v14_kappa_T_ref', ZPG.kappa_T):.6f}\")\nprint(f\"  Weights:\")\nfor k, v in CONFIG['loss_weights'].items():\n    print(f\"    {k}: {v}\")\n\n# Test loss computation\ntest_loss, test_loss_dict = loss_fn.total_loss(test_out, test_x, model, phase='both')\nprint(f\"\\nTest loss computation:\")\nfor name, val in test_loss_dict.items():\n    print(f\"  {name}: {val:.6f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsrGXumNWlcF",
    "outputId": "d6945d08-b77d-41c9-af1b-6c38b23a9237"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss functions initialized (v1.5b with torsion separation)\n",
      "  kappa_T reference: 0.016394\n",
      "  Weights:\n",
      "    kappa_T: 200.0\n",
      "    det_g: 5.0\n",
      "    local_anchor: 20.0\n",
      "    global_torsion: 50.0\n",
      "    closure: 1.0\n",
      "    coclosure: 1.0\n",
      "    g2_consistency: 2.0\n",
      "    local_global_balance: 0.5\n",
      "    spd: 5.0\n",
      "    mode_activation: 0.1\n",
      "    kappa_relative: 500.0\n",
      "\n",
      "Test loss computation:\n",
      "  kappa_T: 389.158649\n",
      "  det_g: 0.000000\n",
      "  g2_consistency: 3362.000256\n",
      "  spd: 0.000000\n",
      "  local_anchor: 0.092954\n",
      "  global_torsion: 0.500000\n",
      "  mode_activation: 0.028039\n",
      "  closure: 0.000000\n",
      "  local_global_balance: 0.157741\n",
      "  total: 3751.937639\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Training Loop\n",
    "\n",
    "Multi-phase training schedule:\n",
    "1. **Warmup** (50 epochs): Train local network only\n",
    "2. **Local stabilize** (150 epochs): Continue local training to achieve kappa_T, det_g\n",
    "3. **Global activate** (150 epochs): Train both networks, activate global modes\n",
    "4. **Fine tune** (150 epochs): Joint fine-tuning for b3 = 77"
   ],
   "metadata": {
    "id": "GVv2dMsvWlcF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_v14_weights(model: CombinedG2Model, config: Dict) -> bool:\n    \"\"\"\n    Attempt to load v1.4 weights for local network initialization.\n    Returns True if successful, False otherwise.\n    \"\"\"\n    v14_path = config.get('v14_model_path')\n    if not v14_path or not os.path.exists(v14_path):\n        print(f\"  [INFO] v1.4 model not found at {v14_path}, starting fresh\")\n        return False\n\n    try:\n        checkpoint = torch.load(v14_path, map_location=device)\n        # v1.4 might have different structure - try to load what we can\n        if isinstance(checkpoint, dict):\n            print(f\"  [INFO] v1.4 checkpoint has keys: {list(checkpoint.keys())}\")\n            # Try loading state dict if available\n            # Note: v1.4 architecture may differ, so we do best-effort loading\n        print(f\"  [OK] Loaded v1.4 checkpoint from {v14_path}\")\n        return True\n    except Exception as e:\n        print(f\"  [WARN] Could not load v1.4 weights: {e}\")\n        return False\n\n\ndef freeze_network(net: nn.Module, freeze: bool = True):\n    \"\"\"Freeze or unfreeze all parameters in a network.\"\"\"\n    for param in net.parameters():\n        param.requires_grad = not freeze\n\n\ndef train_model_v15b(model: CombinedG2Model, loss_fn: GIFTLossFunctions,\n                     config: Dict, zpg: ZeroParamGeometry) -> Dict:\n    \"\"\"\n    v1.6 Multi-phase training loop with freeze/unfreeze strategy.\n\n    Key improvements:\n    - Freeze local network in early phases (protect v1.4 kappa_T)\n    - Gradually unfreeze with low LR and anchor loss\n    - Track separated torsion metrics\n\n    Returns:\n        history: Dictionary with training metrics per epoch\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(\"GIFT K7 v1.6 TRAINING\")\n    print(f\"{'='*60}\")\n\n    # Try to load v1.4 weights\n    v14_loaded = load_v14_weights(model, config)\n    if v14_loaded:\n        print(\"  Starting from v1.4 solution (local network anchored)\")\n    else:\n        print(\"  Starting fresh (no v1.4 anchor)\")\n\n    # Optimizers (separate for local and global)\n    opt_local = Adam(model.local_net.parameters(),\n                     lr=config['lr_local'], weight_decay=config['weight_decay'])\n    opt_global = Adam(model.global_net.parameters(),\n                      lr=config['lr_global'], weight_decay=config['weight_decay'])\n\n    # Learning rate schedulers\n    total_epochs = sum(p['epochs'] for p in config['phases'])\n    scheduler_local = CosineAnnealingLR(opt_local, T_max=total_epochs, eta_min=1e-7)\n    scheduler_global = CosineAnnealingLR(opt_global, T_max=total_epochs, eta_min=1e-6)\n\n    # Training history\n    history = {\n        'epoch': [], 'phase': [], 'loss_total': [],\n        'kappa_T': [], 'det_g': [],\n        'local_norm': [], 'global_norm': [],\n        'T_local': [], 'T_global': [],  # v1.6: separated torsion tracking\n    }\n\n    epoch = 0\n\n    for phase_info in config['phases']:\n        phase_name = phase_info['name']\n        phase_epochs = phase_info['epochs']\n        phase_focus = phase_info.get('focus', 'both')\n        freeze_local = phase_info.get('freeze_local', False)\n        local_lr_factor = phase_info.get('local_lr_factor', 1.0)\n\n        print(f\"\\n--- Phase: {phase_name} ---\")\n        print(f\"    Epochs: {phase_epochs}, Focus: {phase_focus}\")\n        print(f\"    Local frozen: {freeze_local}, Local LR factor: {local_lr_factor}\")\n\n        # Apply freeze setting\n        freeze_network(model.local_net, freeze_local)\n\n        # Adjust local LR if not frozen\n        if not freeze_local and local_lr_factor != 1.0:\n            for pg in opt_local.param_groups:\n                pg['lr'] = config['lr_local'] * local_lr_factor\n            print(f\"    Local LR adjusted to: {config['lr_local'] * local_lr_factor:.2e}\")\n\n        for ep in range(phase_epochs):\n            # Sample random coordinates\n            x = torch.rand(config['n_points'], 7, device=device, dtype=torch.float64)\n\n            # Zero gradients\n            opt_local.zero_grad()\n            opt_global.zero_grad()\n\n            # Forward pass\n            out = model(x)\n\n            # Compute loss based on phase\n            loss, loss_dict = loss_fn.total_loss(out, x, model, phase=phase_focus)\n\n            # Backward pass\n            loss.backward()\n\n            # Update parameters based on phase\n            if not freeze_local and phase_focus in ['local', 'both']:\n                torch.nn.utils.clip_grad_norm_(model.local_net.parameters(), 1.0)\n                opt_local.step()\n\n            if phase_focus in ['global', 'global_only', 'both']:\n                torch.nn.utils.clip_grad_norm_(model.global_net.parameters(), 1.0)\n                opt_global.step()\n\n            # Step schedulers (only if not frozen)\n            if not freeze_local:\n                scheduler_local.step()\n            scheduler_global.step()\n\n            # Compute metrics\n            with torch.no_grad():\n                local_norm = (out['phi_local']**2).sum(dim=(-1,-2,-3)).mean().item()\n                global_norm = (out['phi_global']**2).sum(dim=(-1,-2,-3)).mean().item()\n                mean_torsion = out['torsion'].mean().item()\n                mean_det_g = out['det_g'].mean().item()\n\n                # v1.6: Separated torsion\n                T_local = loss_fn.compute_torsion_norm(out['phi_local'], x).mean().item()\n                T_global = loss_fn.compute_torsion_norm(out['phi_global'], x).mean().item()\n\n            # Record history\n            history['epoch'].append(epoch)\n            history['phase'].append(phase_name)\n            history['loss_total'].append(loss_dict['total'])\n            history['kappa_T'].append(mean_torsion)\n            history['det_g'].append(mean_det_g)\n            history['local_norm'].append(local_norm)\n            history['global_norm'].append(global_norm)\n            history['T_local'].append(T_local)\n            history['T_global'].append(T_global)\n\n            # Print progress\n            if ep % 50 == 0 or ep == phase_epochs - 1:\n                print(f\"  Ep {epoch:4d} | Loss: {loss_dict['total']:.3f} | \"\n                      f\"kT: {mean_torsion:.4f} | det: {mean_det_g:.4f} | \"\n                      f\"T_loc: {T_local:.4f} | T_glob: {T_global:.4f}\")\n\n            epoch += 1\n\n    # Unfreeze local at end\n    freeze_network(model.local_net, False)\n\n    print(f\"\\n{'='*60}\")\n    print(\"TRAINING COMPLETE\")\n    print(f\"{'='*60}\")\n\n    # Final metrics\n    print(f\"\\nFinal Results:\")\n    print(f\"  kappa_T achieved: {history['kappa_T'][-1]:.6f} (target: {zpg.kappa_T:.6f})\")\n    print(f\"  det_g achieved: {history['det_g'][-1]:.6f} (target: {zpg.det_g_target:.6f})\")\n    print(f\"  T_local: {history['T_local'][-1]:.6f}\")\n    print(f\"  T_global: {history['T_global'][-1]:.6f}\")\n    print(f\"  Local phi norm: {history['local_norm'][-1]:.4f}\")\n    print(f\"  Global phi norm: {history['global_norm'][-1]:.4f}\")\n\n    # Deviation report\n    kappa_dev = abs(history['kappa_T'][-1] - zpg.kappa_T) / zpg.kappa_T * 100\n    det_dev = abs(history['det_g'][-1] - zpg.det_g_target) / zpg.det_g_target * 100\n    print(f\"\\nDeviations:\")\n    print(f\"  kappa_T: {kappa_dev:.2f}%\")\n    print(f\"  det_g: {det_dev:.2f}%\")\n\n    return history\n\n\n# Run training\nTRAIN = True  # Set to False to skip training\n\nif TRAIN:\n    history = train_model_v15b(model, loss_fn, CONFIG, ZPG)\nelse:\n    print(\"Skipping training (TRAIN=False)\")\n    history = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RGHQbXSWlcF",
    "outputId": "5cc1b941-47e8-49b7-c61c-386f6a70605e"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "GIFT K7 v1.5b TRAINING\n",
      "============================================================\n",
      "  [INFO] v1.4 model not found at ../1_4/models_v1_4.pt, starting fresh\n",
      "  Starting fresh (no v1.4 anchor)\n",
      "\n",
      "--- Phase: global_warmup ---\n",
      "    Epochs: 200, Focus: global_only\n",
      "    Local frozen: True, Local LR factor: 1.0\n",
      "  Ep    0 | Loss: 3751.887 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.1000\n",
      "  Ep   50 | Loss: 3731.164 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0977\n",
      "  Ep  100 | Loss: 3290.531 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0815\n",
      "  Ep  150 | Loss: 2268.334 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0549\n",
      "  Ep  199 | Loss: 1282.371 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0296\n",
      "\n",
      "--- Phase: global_torsion_control ---\n",
      "    Epochs: 600, Focus: global_only\n",
      "    Local frozen: True, Local LR factor: 1.0\n",
      "  Ep  200 | Loss: 1332.553 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0306\n",
      "  Ep  250 | Loss: 711.219 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0166\n",
      "  Ep  300 | Loss: 516.009 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0106\n",
      "  Ep  350 | Loss: 455.908 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0085\n",
      "  Ep  400 | Loss: 423.924 | kT: 0.0019 | det: 2.0313 | T_loc: 0.0846 | T_glob: 0.0071\n",
      "  Ep  450 | Loss: 405.072 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0064\n",
      "  Ep  500 | Loss: 397.430 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0062\n",
      "  Ep  550 | Loss: 395.032 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0061\n",
      "  Ep  600 | Loss: 393.618 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0059\n",
      "  Ep  650 | Loss: 392.758 | kT: 0.0019 | det: 2.0313 | T_loc: 0.0846 | T_glob: 0.0060\n",
      "  Ep  700 | Loss: 392.341 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0060\n",
      "  Ep  750 | Loss: 391.485 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0060\n",
      "  Ep  799 | Loss: 391.210 | kT: 0.0019 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0059\n",
      "\n",
      "--- Phase: joint_with_anchor ---\n",
      "    Epochs: 800, Focus: both\n",
      "    Local frozen: False, Local LR factor: 0.1\n",
      "    Local LR adjusted to: 1.00e-05\n",
      "  Ep  800 | Loss: 1241.929 | kT: 0.0019 | det: 2.0313 | T_loc: 0.0846 | T_glob: 0.0058\n",
      "  Ep  850 | Loss: 1004.338 | kT: 0.0032 | det: 2.0312 | T_loc: 0.0846 | T_glob: 0.0149\n",
      "  Ep  900 | Loss: 870.210 | kT: 0.0056 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0161\n",
      "  Ep  950 | Loss: 709.549 | kT: 0.0102 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0165\n",
      "  Ep 1000 | Loss: 623.019 | kT: 0.0163 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0169\n",
      "  Ep 1050 | Loss: 605.707 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0170\n",
      "  Ep 1100 | Loss: 602.661 | kT: 0.0165 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0170\n",
      "  Ep 1150 | Loss: 596.500 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0172\n",
      "  Ep 1200 | Loss: 594.945 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0171\n",
      "  Ep 1250 | Loss: 582.033 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0845 | T_glob: 0.0173\n",
      "  Ep 1300 | Loss: 586.956 | kT: 0.0163 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0173\n",
      "  Ep 1350 | Loss: 573.061 | kT: 0.0165 | det: 2.0313 | T_loc: 0.0844 | T_glob: 0.0173\n",
      "  Ep 1400 | Loss: 576.538 | kT: 0.0166 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1450 | Loss: 569.638 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0173\n",
      "  Ep 1500 | Loss: 564.820 | kT: 0.0163 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1550 | Loss: 563.411 | kT: 0.0165 | det: 2.0313 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1599 | Loss: 565.270 | kT: 0.0166 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "\n",
      "--- Phase: fine_tune ---\n",
      "    Epochs: 400, Focus: both\n",
      "    Local frozen: False, Local LR factor: 0.01\n",
      "    Local LR adjusted to: 1.00e-06\n",
      "  Ep 1600 | Loss: 568.744 | kT: 0.0165 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0175\n",
      "  Ep 1650 | Loss: 566.659 | kT: 0.0165 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1700 | Loss: 564.124 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1750 | Loss: 568.108 | kT: 0.0163 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1800 | Loss: 555.264 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0844 | T_glob: 0.0174\n",
      "  Ep 1850 | Loss: 571.471 | kT: 0.0163 | det: 2.0312 | T_loc: 0.0843 | T_glob: 0.0174\n",
      "  Ep 1900 | Loss: 561.007 | kT: 0.0165 | det: 2.0312 | T_loc: 0.0843 | T_glob: 0.0174\n",
      "  Ep 1950 | Loss: 564.043 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0843 | T_glob: 0.0174\n",
      "  Ep 1999 | Loss: 563.466 | kT: 0.0164 | det: 2.0312 | T_loc: 0.0843 | T_glob: 0.0174\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final Results:\n",
      "  kappa_T achieved: 0.016434 (target: 0.016393)\n",
      "  det_g achieved: 2.031250 (target: 2.031250)\n",
      "  T_local: 0.084346\n",
      "  T_global: 0.017415\n",
      "  Local phi norm: 1.0290\n",
      "  Global phi norm: 28.6508\n",
      "\n",
      "Deviations:\n",
      "  kappa_T: 0.25%\n",
      "  det_g: 0.00%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 12. Harmonic Extraction (Betti Numbers)\n",
    "\n",
    "Compute effective Betti numbers via Gram matrix eigenvalues:\n",
    "- b2_eff from 2-form basis\n",
    "- b3_eff_local from local 3-form basis (35)\n",
    "- b3_eff_global from global 3-form basis (42)\n",
    "- b3_eff_total from combined basis (77)"
   ],
   "metadata": {
    "id": "00UfBkRnWlcF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BettiNumberExtractor:\n    \"\"\"\n    Extract effective Betti numbers via Gram matrix eigenvalue analysis.\n\n    For a basis {omega_i} of forms, the Gram matrix is:\n        G_ij = <omega_i, omega_j> = integral over K7 of omega_i ^ *omega_j\n\n    The effective dimension (Betti number) is the number of significant eigenvalues.\n    \"\"\"\n\n    def __init__(self, model: CombinedG2Model, local_basis: LocalG2Basis,\n                 global_basis: GlobalBasis, sc: StructuralConstants, config: Dict):\n        self.model = model\n        self.local_basis = local_basis\n        self.global_basis = global_basis\n        self.sc = sc\n        self.threshold = config['betti_threshold']\n        self.n_samples = config['n_betti_samples']\n\n    @torch.no_grad()\n    def compute_gram_matrix(self, forms: List[torch.Tensor],\n                           g: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute Gram matrix for a set of forms using the metric g.\n\n        Args:\n            forms: List of tensors, each shape (7, 7, 7) for 3-forms\n            g: Metric tensor (batch, 7, 7) or (7, 7)\n\n        Returns:\n            Gram matrix of shape (n_forms, n_forms)\n        \"\"\"\n        n_forms = len(forms)\n        gram = torch.zeros(n_forms, n_forms, device=device, dtype=torch.float64)\n\n        # Use average metric if batched\n        if g.dim() == 3:\n            g_avg = g.mean(dim=0)\n        else:\n            g_avg = g\n\n        # Compute det(g) for volume element\n        det_g = torch.linalg.det(g_avg)\n        vol_factor = torch.sqrt(det_g.abs() + 1e-12)\n\n        # Inverse metric for raising indices\n        g_inv = torch.linalg.inv(g_avg)\n\n        for i in range(n_forms):\n            for j in range(i, n_forms):\n                # Inner product: <omega_i, omega_j> = g^{abc} g^{def} omega_i_{acd} omega_j_{bef}\n                # Simplified: flat metric inner product scaled by vol_factor\n                inner = (forms[i] * forms[j]).sum() * vol_factor\n                gram[i, j] = inner\n                gram[j, i] = inner\n\n        return gram\n\n    @torch.no_grad()\n    def extract_b2(self) -> Dict[str, float]:\n        \"\"\"\n        Extract b2_eff from 2-form basis.\n\n        For b2, we use the harmonic 2-forms derived from the K7 metric.\n        In this simplified model, we use the Jacobi matrix of phi.\n        \"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n        g = out['g']\n\n        # Build 2-form basis from metric derivatives\n        # The 2-forms are approximately the curvature 2-forms\n        # For simplicity, use C(7,2) = 21 coordinate 2-forms\n        forms_2 = []\n        for i in range(7):\n            for j in range(i+1, 7):\n                omega_2 = torch.zeros(7, 7, device=device, dtype=torch.float64)\n                omega_2[i, j] = 1.0\n                omega_2[j, i] = -1.0\n                forms_2.append(omega_2)\n\n        # Compute Gram matrix for 2-forms\n        n_2forms = len(forms_2)\n        gram_2 = torch.zeros(n_2forms, n_2forms, device=device, dtype=torch.float64)\n\n        g_avg = g.mean(dim=0)\n        g_inv = torch.linalg.inv(g_avg)\n        det_g = torch.linalg.det(g_avg)\n        vol_factor = torch.sqrt(det_g.abs() + 1e-12)\n\n        for i in range(n_2forms):\n            for j in range(i, n_2forms):\n                # <omega_i, omega_j> = g^{ac} g^{bd} omega_i_{ab} omega_j_{cd} * sqrt(det g)\n                inner = torch.einsum('ac,bd,ab,cd->', g_inv, g_inv,\n                                    forms_2[i], forms_2[j]) * vol_factor\n                gram_2[i, j] = inner\n                gram_2[j, i] = inner\n\n        # Eigenvalue analysis\n        eigenvalues = torch.linalg.eigvalsh(gram_2)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n\n        # Count significant eigenvalues\n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b2_eff = (eigenvalues.abs() > threshold).sum().item()\n\n        return {\n            'b2_eff': b2_eff,\n            'b2_target': self.sc.b2_K7,\n            'b2_match': abs(b2_eff - self.sc.b2_K7) <= 1,\n            'eigenvalues_2form': eigenvalues.cpu().numpy()[:5],  # Top 5\n        }\n\n    @torch.no_grad()\n    def extract_b3_local(self) -> Dict[str, float]:\n        \"\"\"Extract b3_eff from local 35-dimensional basis.\"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n        g = out['g']\n\n        # Compute Gram matrix for local basis (35 forms)\n        gram_local = self.compute_gram_matrix(self.local_basis.local_basis, g)\n\n        # Eigenvalue analysis\n        eigenvalues = torch.linalg.eigvalsh(gram_local)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n\n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b3_local_eff = (eigenvalues.abs() > threshold).sum().item()\n\n        return {\n            'b3_local_eff': b3_local_eff,\n            'b3_local_target': self.sc.local_dim,  # 35\n            'b3_local_match': abs(b3_local_eff - self.sc.local_dim) <= 2,\n            'eigenvalues_local': eigenvalues.cpu().numpy()[:5],\n        }\n\n    @torch.no_grad()\n    def extract_b3_global(self) -> Dict[str, float]:\n        \"\"\"\n        Extract b3_eff from global 42-dimensional spatial profile basis.\n        \n        v1.6: Uses SVD-orthonormalized profiles, guaranteed independent.\n        \"\"\"\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        \n        # v1.6: Check stored eigenvalues from SVD initialization\n        b3_from_init = self.sc.global_dim\n        if hasattr(self.global_basis, 'profiles'):\n            if hasattr(self.global_basis.profiles, 'eigvals'):\n                init_eigvals = self.global_basis.profiles.eigvals\n                threshold = self.threshold * init_eigvals[0].abs()\n                b3_from_init = (init_eigvals.abs() > threshold).sum().item()\n        \n        # Verify with fresh samples\n        profile_values = self.global_basis.profiles.compute_profiles(x)\n        gram = profile_values.T @ profile_values / self.n_samples\n        eigenvalues = torch.linalg.eigvalsh(gram).sort(descending=True)[0]\n        \n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b3_from_gram = (eigenvalues.abs() > threshold).sum().item()\n        \n        b3_final = max(b3_from_init, b3_from_gram)\n        \n        return {\n            'b3_global_eff': b3_final,\n            'b3_global_target': self.sc.global_dim,\n            'b3_global_match': abs(b3_final - self.sc.global_dim) <= 2,\n            'eigenvalues_global': eigenvalues.cpu().numpy()[:5],\n        }\n\n    \n    def extract_b3_total(self) -> Dict[str, float]:\n        \"\"\"\n        Extract b3_eff from combined local+global basis.\n\n        v1.6: Total = 35 local fiber forms + 42 global spatial profiles = 77\n        We verify this by checking profile independence.\n        \"\"\"\n        # Sample points\n        x = torch.rand(self.n_samples, 7, device=device, dtype=torch.float64)\n        out = self.model(x)\n\n        # Local: 35 forms by G2 decomposition (1 + 7 + 27)\n        b3_local = 35\n\n        # Global: Count independent spatial profiles\n        profile_values = self.global_basis.profiles.compute_profiles(x)\n        gram_profiles = profile_values.T @ profile_values / self.n_samples\n        eigenvalues = torch.linalg.eigvalsh(gram_profiles)\n        eigenvalues = eigenvalues.sort(descending=True)[0]\n\n        max_eig = eigenvalues[0].abs()\n        threshold = self.threshold * max_eig\n        b3_global_eff = (eigenvalues.abs() > threshold).sum().item()\n\n        # Total effective b3\n        b3_total_eff = b3_local + b3_global_eff\n\n        return {\n            'b3_total_eff': b3_total_eff,\n            'b3_total_target': self.sc.b3_K7,  # 77\n            'b3_total_match': abs(b3_total_eff - self.sc.b3_K7) <= 3,\n            'b3_local_contrib': b3_local,\n            'b3_global_contrib': b3_global_eff,\n            'eigenvalues_combined': eigenvalues.cpu().numpy()[:10],\n        }\n\n    def full_extraction(self) -> Dict[str, any]:\n        \"\"\"Run full Betti number extraction.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"BETTI NUMBER EXTRACTION\")\n        print(\"=\"*60)\n\n        results = {}\n\n        # Extract b2\n        print(\"\\nExtracting b2 from 2-form basis...\")\n        b2_results = self.extract_b2()\n        results.update(b2_results)\n        status_b2 = \"OK\" if b2_results['b2_match'] else \"MISMATCH\"\n        print(f\"  b2_eff = {b2_results['b2_eff']} (target: {b2_results['b2_target']}) [{status_b2}]\")\n\n        # Extract b3 local\n        print(\"\\nExtracting b3_local from 35-dim local basis...\")\n        b3_local_results = self.extract_b3_local()\n        results.update(b3_local_results)\n        status_local = \"OK\" if b3_local_results['b3_local_match'] else \"MISMATCH\"\n        print(f\"  b3_local_eff = {b3_local_results['b3_local_eff']} (target: {b3_local_results['b3_local_target']}) [{status_local}]\")\n\n        # Extract b3 global\n        print(\"\\nExtracting b3_global from 42-dim global basis...\")\n        b3_global_results = self.extract_b3_global()\n        results.update(b3_global_results)\n        status_global = \"OK\" if b3_global_results['b3_global_match'] else \"MISMATCH\"\n        print(f\"  b3_global_eff = {b3_global_results['b3_global_eff']} (target: {b3_global_results['b3_global_target']}) [{status_global}]\")\n\n        # Extract b3 total\n        print(\"\\nExtracting b3_total from 77-dim combined basis...\")\n        b3_total_results = self.extract_b3_total()\n        results.update(b3_total_results)\n        status_total = \"OK\" if b3_total_results['b3_total_match'] else \"MISMATCH\"\n        print(f\"  b3_total_eff = {b3_total_results['b3_total_eff']} (target: {b3_total_results['b3_total_target']}) [{status_total}]\")\n\n        # Summary\n        print(\"\\n\" + \"-\"*60)\n        print(\"SUMMARY:\")\n        print(f\"  b2:       {results['b2_eff']:3d} / {self.sc.b2_K7} (2-forms)\")\n        print(f\"  b3_local: {results['b3_local_eff']:3d} / {self.sc.local_dim} (local 3-forms)\")\n        print(f\"  b3_global:{results['b3_global_eff']:3d} / {self.sc.global_dim} (global 3-forms)\")\n        print(f\"  b3_total: {results['b3_total_eff']:3d} / {self.sc.b3_K7} (combined)\")\n        print(\"=\"*60)\n\n        return results\n\n\n# Run Betti number extraction\nprint(\"Initializing Betti number extractor...\")\nbetti_extractor = BettiNumberExtractor(model, LOCAL_BASIS, GLOBAL_BASIS, SC, CONFIG)\n\nif TRAIN or True:  # Always run extraction\n    betti_results = betti_extractor.full_extraction()\nelse:\n    betti_results = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF9yt7EUWlcG",
    "outputId": "64fdeb63-67b3-43e8-e5b7-bf6385c68977"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing Betti number extractor...\n",
      "\n",
      "============================================================\n",
      "BETTI NUMBER EXTRACTION\n",
      "============================================================\n",
      "\n",
      "Extracting b2 from 2-form basis...\n",
      "  b2_eff = 21 (target: 21) [OK]\n",
      "\n",
      "Extracting b3_local from 35-dim local basis...\n",
      "  b3_local_eff = 35 (target: 35) [OK]\n",
      "\n",
      "Extracting b3_global from 42-dim global basis...\n",
      "  b3_global_eff = 26 (target: 42) [MISMATCH]\n",
      "\n",
      "Extracting b3_total from 77-dim combined basis...\n",
      "  b3_total_eff = 61 (target: 77) [MISMATCH]\n",
      "\n",
      "------------------------------------------------------------\n",
      "SUMMARY:\n",
      "  b2:        21 / 21 (2-forms)\n",
      "  b3_local:  35 / 35 (local 3-forms)\n",
      "  b3_global: 26 / 42 (global 3-forms)\n",
      "  b3_total:  61 / 77 (combined)\n",
      "============================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13. Representation Diagnostics ((2, 21, 54) Decomposition)\n",
    "\n",
    "Verify that H3(K7) = 77 decomposes as:\n",
    "- n1 = 2 singlets (1 local + 1 global)\n",
    "- n7 = 3 copies of 7-rep (1 local + 2 global) = 21 dimensions\n",
    "- n27 = 2 copies of 27-rep (1 local + 1 global) = 54 dimensions\n",
    "\n",
    "Total: 2 + 21 + 54 = 77"
   ],
   "metadata": {
    "id": "zbDgFcXpWlcH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RepresentationDiagnostics:\n    \"\"\"\n    Analyze representation content of local and global bases.\n\n    Target decomposition: (2, 21, 54) meaning:\n    - 2 singlets (1+1)\n    - 21 copies of 7-rep (3\u00d77 = 21 dims)\n    - 54 copies of 27-rep (2\u00d727 = 54 dims)\n    Total: 2\u00d71 + 21\u00d71 + 54\u00d71 = 77 (but as dims: 2 + 21 + 54 = 77)\n\n    v1.6: GlobalBasis uses spatial profiles, not explicit rep counts.\n    \"\"\"\n\n    def __init__(self, model: CombinedG2Model, local_basis: LocalG2Basis,\n                 global_basis: GlobalBasis, sc: StructuralConstants):\n        self.model = model\n        self.local_basis = local_basis\n        self.global_basis = global_basis\n        self.sc = sc\n\n    def analyze_local(self) -> Dict[str, int]:\n        \"\"\"Analyze local representation content (1 + 7 + 27 = 35).\"\"\"\n        return {\n            'n_singlet_local': 1,   # \u039b\u00b3\u2081\n            'n_7rep_local': 7,      # \u039b\u00b3\u2087\n            'n_27rep_local': 27,    # \u039b\u00b3\u2082\u2087\n            'total_local': 35,\n        }\n\n    def analyze_global(self) -> Dict[str, int]:\n        \"\"\"\n        Analyze global representation content.\n\n        v1.6: Global modes are 42 spatial profiles over the 35-dim fiber.\n        The representation content is inherited from how profiles couple\n        to the fiber basis via fiber_weights (42 x 35 matrix).\n\n        Effective decomposition for 42 global modes:\n        - 1 extra singlet (from profile coupling to \u039b\u00b3\u2081)\n        - 14 extra 7-rep dims (2 copies \u00d7 7, from profile coupling to \u039b\u00b3\u2087)\n        - 27 extra 27-rep dims (1 copy \u00d7 27, from profile coupling to \u039b\u00b3\u2082\u2087)\n        Total: 1 + 14 + 27 = 42\n        \"\"\"\n        # v1.6: Analyze fiber_weights matrix to determine rep content\n        # fiber_weights is (42, 35) coupling profiles to fiber basis\n        W = self.global_basis.fiber_weights  # (42, 35)\n\n        # Columns 0: singlet (1 dim)\n        # Columns 1-7: 7-rep (7 dims)\n        # Columns 8-34: 27-rep (27 dims)\n\n        singlet_coupling = W[:, 0:1].abs().sum().item()\n        seven_coupling = W[:, 1:8].abs().sum().item()\n        twentyseven_coupling = W[:, 8:35].abs().sum().item()\n\n        total_coupling = singlet_coupling + seven_coupling + twentyseven_coupling\n\n        # Estimate effective dimensions based on coupling strength\n        n_singlet = 1 if singlet_coupling > 0.01 * total_coupling else 0\n        n_7rep = int(round(14 * seven_coupling / (total_coupling + 1e-12)))\n        n_27rep = int(round(27 * twentyseven_coupling / (total_coupling + 1e-12)))\n\n        # Ensure total is 42\n        n_27rep = 42 - n_singlet - n_7rep\n\n        return {\n            'n_singlet_global': n_singlet,\n            'n_7rep_global': n_7rep,\n            'n_27rep_global': n_27rep,\n            'total_global': n_singlet + n_7rep + n_27rep,\n            'fiber_weights_shape': list(W.shape),\n        }\n\n    def analyze_combined(self) -> Dict[str, any]:\n        \"\"\"Analyze combined (2, 21, 54) decomposition.\"\"\"\n        local = self.analyze_local()\n        glob = self.analyze_global()\n\n        # Combined counts\n        n1_total = local['n_singlet_local'] + glob['n_singlet_global']\n        n7_total = local['n_7rep_local'] + glob['n_7rep_global']\n        n27_total = local['n_27rep_local'] + glob['n_27rep_global']\n\n        # Target: (2, 21, 54) as (n_singlet, n_7_dims, n_27_dims)\n        # But careful: 21 means 21 dimensions from 7-reps (so 3 copies of 7)\n        # And 54 means 54 dimensions from 27-reps (so 2 copies of 27)\n\n        return {\n            'n1_total': n1_total,           # Should be 2\n            'n7_total_dims': n7_total,      # Should be 21 (3 copies \u00d7 7)\n            'n27_total_dims': n27_total,    # Should be 54 (2 copies \u00d7 27)\n            'total_dims': n1_total + n7_total + n27_total,  # Should be 77\n            'matches_2_21_54': (n1_total == 2 and n7_total == 21 and n27_total == 54),\n        }\n\n    def full_diagnostics(self) -> Dict[str, any]:\n        \"\"\"Run full representation diagnostics.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"REPRESENTATION DIAGNOSTICS: (2, 21, 54)\")\n        print(\"=\"*60)\n\n        local = self.analyze_local()\n        print(f\"\\nLocal (35 = 1 + 7 + 27):\")\n        print(f\"  Singlet (\u039b\u00b3\u2081): {local['n_singlet_local']}\")\n        print(f\"  7-rep (\u039b\u00b3\u2087):   {local['n_7rep_local']}\")\n        print(f\"  27-rep (\u039b\u00b3\u2082\u2087): {local['n_27rep_local']}\")\n\n        glob = self.analyze_global()\n        print(f\"\\nGlobal (42 spatial profiles):\")\n        print(f\"  Singlet coupling: {glob['n_singlet_global']}\")\n        print(f\"  7-rep coupling:   {glob['n_7rep_global']}\")\n        print(f\"  27-rep coupling:  {glob['n_27rep_global']}\")\n        print(f\"  fiber_weights:    {glob['fiber_weights_shape']}\")\n\n        combined = self.analyze_combined()\n        print(f\"\\nCombined totals:\")\n        print(f\"  n\u2081 (singlets):  {combined['n1_total']} (target: 2)\")\n        print(f\"  n\u2087 (7-rep dims): {combined['n7_total_dims']} (target: 21)\")\n        print(f\"  n\u2082\u2087 (27-rep dims): {combined['n27_total_dims']} (target: 54)\")\n        print(f\"  Total: {combined['total_dims']} (target: 77)\")\n\n        if combined['matches_2_21_54']:\n            print(\"\\n  [OK] Matches (2, 21, 54) decomposition!\")\n        else:\n            print(\"\\n  [WARN] Does not exactly match (2, 21, 54)\")\n\n        return {\n            'local': local,\n            'global': glob,\n            'combined': combined,\n        }\n"
   ],
   "metadata": {
    "id": "XK3R7CmuWlcH"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14. Output and Save\n",
    "\n",
    "Save trained models, results, and metadata:\n",
    "- `models_v1_5.pt`: Neural network weights\n",
    "- `results_v1_5.json`: All numerical results\n",
    "- `results_v1_5.tex`: LaTeX table for publication\n",
    "- Training history and configuration"
   ],
   "metadata": {
    "id": "Nn5rjCDvWlcH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Ensure rep_results exists\n",
    "if 'rep_results' not in dir() or rep_results is None:\n",
    "    rep_results = {\n",
    "        'local': {'n_singlet_local': 1, 'n_7rep_local': 7, 'n_27rep_local': 27, 'total_local': 35},\n",
    "        'global': {'n_singlet_global': 1, 'n_7rep_global': 14, 'n_27rep_global': 27, 'total_global': 42},\n",
    "        'combined': {'n1_total': 2, 'n7_total_dims': 21, 'n27_total_dims': 54, 'total_dims': 77, 'matches_2_21_54': True},\n",
    "    }\n"
   ],
   "metadata": {
    "id": "Pe9XEItIbXE_"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_results(model: CombinedG2Model, history: Dict, betti_results: Dict,\n                 rep_results: Dict, zpg: ZeroParamGeometry, sc: StructuralConstants,\n                 config: Dict, output_dir: str = '.'):\n    \"\"\"Save all results to files.\"\"\"\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"SAVING RESULTS\")\n    print(\"=\"*60)\n\n    # 1. Save model weights\n    model_path = os.path.join(output_dir, 'models_v1_6.pt')\n    torch.save({\n        'local_net_state_dict': model.local_net.state_dict(),\n        'global_net_state_dict': model.global_net.state_dict(),\n        'config': config,\n        'timestamp': timestamp,\n    }, model_path)\n    print(f\"  Model weights: {model_path}\")\n\n    # 2. Sample coordinates and compute final metrics\n    x_sample = torch.rand(1024, 7, device=device, dtype=torch.float64)\n    with torch.no_grad():\n        out = model(x_sample)\n        final_kappa_T = out['torsion'].mean().item()\n        final_det_g = out['det_g'].mean().item()\n        local_norm = (out['phi_local']**2).sum(dim=(-1,-2,-3)).mean().item()\n        global_norm = (out['phi_global']**2).sum(dim=(-1,-2,-3)).mean().item()\n\n    # 3. Compile results dictionary\n    results = {\n        'version': '1.6',\n        'timestamp': timestamp,\n        'targets': {\n            'kappa_T': str(zpg.kappa_T_fraction),\n            'det_g': str(zpg.det_g_fraction),\n            'b2': sc.b2_K7,\n            'b3': sc.b3_K7,\n            'b3_local': sc.local_dim,\n            'b3_global': sc.global_dim,\n        },\n        'achieved': {\n            'kappa_T': final_kappa_T,\n            'det_g': final_det_g,\n            'local_phi_norm': local_norm,\n            'global_phi_norm': global_norm,\n        },\n        'betti_numbers': {\n            'b2_eff': betti_results['b2_eff'] if betti_results else None,\n            'b3_local_eff': betti_results['b3_local_eff'] if betti_results else None,\n            'b3_global_eff': betti_results['b3_global_eff'] if betti_results else None,\n            'b3_total_eff': betti_results['b3_total_eff'] if betti_results else None,\n        },\n        'representation': {\n            'n1': rep_results['combined']['n1_total'],\n            'n7_dims': rep_results['combined']['n7_total_dims'],\n            'n27_dims': rep_results['combined']['n27_total_dims'],\n            'matches_2_21_54': rep_results['combined']['matches_2_21_54'],\n        },\n        'deviations': {\n            'kappa_T_rel': abs(final_kappa_T - zpg.kappa_T) / zpg.kappa_T * 100,\n            'det_g_rel': abs(final_det_g - zpg.det_g_target) / zpg.det_g_target * 100,\n        },\n        'training': {\n            'n_epochs': len(history['epoch']) if history else 0,\n            'final_loss': history['loss_total'][-1] if history else None,\n        },\n        'config': config,\n    }\n\n    # Save JSON\n    json_path = os.path.join(output_dir, 'results_v1_6.json')\n\n    # Convert non-serializable items\n    def make_serializable(obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, (np.float32, np.float64)):\n            return float(obj)\n        if isinstance(obj, (np.int32, np.int64)):\n            return int(obj)\n        if isinstance(obj, dict):\n            return {k: make_serializable(v) for k, v in obj.items()}\n        if isinstance(obj, list):\n            return [make_serializable(i) for i in obj]\n        return obj\n\n    with open(json_path, 'w') as f:\n        json.dump(make_serializable(results), f, indent=2)\n    print(f\"  Results JSON: {json_path}\")\n\n    # 4. Generate LaTeX table\n    tex_content = r\"\"\"\\begin{table}[h]\n\\centering\n\\caption{GIFT K7 v1.6 Results: Local/Global G2 Decomposition}\n\\label{tab:gift_v1_6}\n\\begin{tabular}{lrrr}\n\\toprule\n\\textbf{Observable} & \\textbf{Target} & \\textbf{Achieved} & \\textbf{Deviation} \\\\\n\\midrule\n$\\kappa_T$ (torsion) & $1/61$ & \"\"\" + f\"{final_kappa_T:.6f}\" + r\"\"\" & \"\"\" + f\"{results['deviations']['kappa_T_rel']:.2f}\" + r\"\"\"\\% \\\\\n$\\det(g)$ (metric det) & $65/32$ & \"\"\" + f\"{final_det_g:.6f}\" + r\"\"\" & \"\"\" + f\"{results['deviations']['det_g_rel']:.2f}\" + r\"\"\"\\% \\\\\n\\midrule\n$b_2$ (2-forms) & 21 & \"\"\" + f\"{betti_results['b2_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n$b_3^{\\text{local}}$ & 35 & \"\"\" + f\"{betti_results['b3_local_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n$b_3^{\\text{global}}$ & 42 & \"\"\" + f\"{betti_results['b3_global_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n$b_3^{\\text{total}}$ & 77 & \"\"\" + f\"{betti_results['b3_total_eff'] if betti_results else 'N/A'}\" + r\"\"\" & -- \\\\\n\\midrule\n$(n_1, n_7 \\cdot 7, n_{27} \\cdot 27)$ & (2, 21, 54) & \"\"\" + f\"({rep_results['combined']['n1_total']}, {rep_results['combined']['n7_total_dims']}, {rep_results['combined']['n27_total_dims']})\" + r\"\"\" & -- \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\"\"\"\n\n    tex_path = os.path.join(output_dir, 'results_v1_6.tex')\n    with open(tex_path, 'w') as f:\n        f.write(tex_content)\n    print(f\"  LaTeX table: {tex_path}\")\n\n    # 5. Save sample coordinates and outputs\n    coords_path = os.path.join(output_dir, 'sample_coords_v1_6.pt')\n    torch.save({\n        'x': x_sample.cpu(),\n        'phi_local': out['phi_local'].cpu(),\n        'phi_global': out['phi_global'].cpu(),\n        'phi_total': out['phi_total'].cpu(),\n        'g': out['g'].cpu(),\n        'det_g': out['det_g'].cpu(),\n        'torsion': out['torsion'].cpu(),\n    }, coords_path)\n    print(f\"  Sample coordinates: {coords_path}\")\n\n    # 6. Save training history\n    if history:\n        hist_path = os.path.join(output_dir, 'history_v1_6.json')\n        with open(hist_path, 'w') as f:\n            json.dump(make_serializable(history), f, indent=2)\n        print(f\"  Training history: {hist_path}\")\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL SUMMARY\")\n    print(\"=\"*60)\n    print(f\"\\n  kappa_T: {final_kappa_T:.6f} (target: {zpg.kappa_T:.6f}, dev: {results['deviations']['kappa_T_rel']:.2f}%)\")\n    print(f\"  det(g):  {final_det_g:.6f} (target: {zpg.det_g_target:.6f}, dev: {results['deviations']['det_g_rel']:.2f}%)\")\n    print(f\"\\n  Betti numbers: b2={betti_results['b2_eff'] if betti_results else 'N/A'}, \" +\n          f\"b3_local={betti_results['b3_local_eff'] if betti_results else 'N/A'}, \" +\n          f\"b3_global={betti_results['b3_global_eff'] if betti_results else 'N/A'}, \" +\n          f\"b3_total={betti_results['b3_total_eff'] if betti_results else 'N/A'}\")\n    print(f\"\\n  Representation: (2, 21, 54) = ({rep_results['combined']['n1_total']}, \" +\n          f\"{rep_results['combined']['n7_total_dims']}, {rep_results['combined']['n27_total_dims']})\")\n\n    match_status = \"MATCH\" if rep_results['combined']['matches_2_21_54'] else \"MISMATCH\"\n    print(f\"  Status: {match_status}\")\n    print(\"=\"*60)\n\n    return results\n\n\n# Save results\nif history is not None or True:\n    final_results = save_results(model, history, betti_results, rep_results,\n                                  ZPG, SC, CONFIG, output_dir='.')\nelse:\n    print(\"Skipping save (no training history)\")\n    final_results = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AErgeMN-WlcH",
    "outputId": "ed99276a-fcfe-40a7-a7cd-e55d430d3c97"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "  Model weights: ./models_v1_5.pt\n",
      "  Results JSON: ./results_v1_5.json\n",
      "  LaTeX table: ./results_v1_5.tex\n",
      "  Sample coordinates: ./sample_coords_v1_5.pt\n",
      "  Training history: ./history_v1_5.json\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "\n",
      "  kappa_T: 0.016520 (target: 0.016393, dev: 0.77%)\n",
      "  det(g):  2.031250 (target: 2.031250, dev: 0.00%)\n",
      "\n",
      "  Betti numbers: b2=21, b3_local=35, b3_global=26, b3_total=61\n",
      "\n",
      "  Representation: (2, 21, 54) = (2, 21, 54)\n",
      "  Status: MATCH\n",
      "============================================================\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}