{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Topological Verification - b3 = 77\n",
    "\n",
    "**Objective**: Verify that our G2-structure lives on a manifold with b3 = 77.\n",
    "\n",
    "## What We Have (Phase 1)\n",
    "- G2-structure with ||T|| = 0.00140 < epsilon_0 = 0.0288 ✓\n",
    "- det(g) = 65/32 exact ✓\n",
    "- Positive definite metric ✓\n",
    "\n",
    "## What We Need (Phase 2)\n",
    "- **b3 = 77**: Third Betti number = dim(H^3) = 77 harmonic 3-forms\n",
    "- **Spectral gap**: Clear separation after eigenvalue #77\n",
    "- **3 generations**: Clustering structure for N_gen = 3\n",
    "\n",
    "## Methods\n",
    "1. **High-resolution Hodge Laplacian**: Discrete approximation on fine mesh\n",
    "2. **Persistent Homology**: Topological data analysis with Gudhi\n",
    "3. **Spectral clustering**: Verify 3-generation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q gudhi torch numpy scipy matplotlib scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import math\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    import gudhi as gd\n",
    "    GUDHI_AVAILABLE = True\n",
    "    print(\"Gudhi available for persistent homology\")\n",
    "except ImportError:\n",
    "    GUDHI_AVAILABLE = False\n",
    "    print(\"Gudhi not available - will use spectral methods only\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# GIFT targets\n",
    "TARGET_B3 = 77\n",
    "TARGET_B2 = 21\n",
    "TARGET_DET = 65.0 / 32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, input_dim=7, num_frequencies=64, scale=0.5):\n",
    "        super().__init__()\n",
    "        self.output_dim = 2 * num_frequencies\n",
    "        B = torch.randn(num_frequencies, input_dim) * scale\n",
    "        self.register_buffer('B', B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_proj = 2 * math.pi * torch.matmul(x, self.B.T)\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "def standard_g2_phi(device=None):\n",
    "    phi = torch.zeros(35, device=device, dtype=torch.float64)\n",
    "    G2_INDICES = [(0,1,2), (0,3,4), (0,5,6), (1,3,5), (1,4,6), (2,3,6), (2,4,5)]\n",
    "    G2_SIGNS = [1, 1, 1, 1, -1, -1, -1]\n",
    "    \n",
    "    def to_index(i, j, k):\n",
    "        count = 0\n",
    "        for a in range(7):\n",
    "            for b in range(a + 1, 7):\n",
    "                for c in range(b + 1, 7):\n",
    "                    if a == i and b == j and c == k:\n",
    "                        return count\n",
    "                    count += 1\n",
    "        return -1\n",
    "    \n",
    "    for indices, sign in zip(G2_INDICES, G2_SIGNS):\n",
    "        idx = to_index(*indices)\n",
    "        if idx >= 0:\n",
    "            phi[idx] = float(sign)\n",
    "    return phi\n",
    "\n",
    "\n",
    "class G2LowTorsionNet(nn.Module):\n",
    "    def __init__(self, hidden_dims=[256, 512, 512, 512, 256], num_frequencies=64, \n",
    "                 fourier_scale=0.5, perturbation_scale=0.05, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device or torch.device('cpu')\n",
    "        self.fourier = FourierFeatures(7, num_frequencies, fourier_scale)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = self.fourier.output_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_dim, 35)\n",
    "        self.bias = nn.Parameter(standard_g2_phi(self.device))\n",
    "        self.scale = nn.Parameter(torch.ones(35, device=self.device, dtype=torch.float64) * perturbation_scale)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_enc = self.fourier(x)\n",
    "        h = self.mlp(x_enc)\n",
    "        phi_raw = self.output_layer(h)\n",
    "        return phi_raw * self.scale + self.bias\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = G2LowTorsionNet(device=device).to(device).double()\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load('g2_low_torsion_model.pt', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model loaded!\")\n",
    "    print(f\"  det(g) = {checkpoint['final_det_g']:.6f}\")\n",
    "    print(f\"  ||T|| = {checkpoint['torsion_scaled']:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Upload g2_low_torsion_model.pt: {e}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build High-Resolution Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_manifold_points(model, n_points: int = 8192, method: str = 'uniform') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample points on the G2 manifold.\n",
    "    \n",
    "    For high-quality spectrum, we need well-distributed points.\n",
    "    \"\"\"\n",
    "    if method == 'uniform':\n",
    "        # Uniform in [-1, 1]^7\n",
    "        points = torch.rand(n_points, 7, device=device, dtype=torch.float64) * 2 - 1\n",
    "    \n",
    "    elif method == 'sobol':\n",
    "        # Sobol sequence for better space-filling\n",
    "        from torch.quasirandom import SobolEngine\n",
    "        sobol = SobolEngine(dimension=7, scramble=True)\n",
    "        points = sobol.draw(n_points).to(device).double() * 2 - 1\n",
    "    \n",
    "    elif method == 'latin_hypercube':\n",
    "        # Latin hypercube sampling\n",
    "        from scipy.stats import qmc\n",
    "        sampler = qmc.LatinHypercube(d=7)\n",
    "        sample = sampler.random(n=n_points)\n",
    "        points = torch.from_numpy(sample * 2 - 1).to(device).double()\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "# Sample points\n",
    "N_POINTS = 8192  # High resolution\n",
    "print(f\"Sampling {N_POINTS} points...\")\n",
    "\n",
    "try:\n",
    "    points = sample_manifold_points(model, N_POINTS, method='sobol')\n",
    "    print(\"Using Sobol sequence\")\n",
    "except:\n",
    "    points = sample_manifold_points(model, N_POINTS, method='uniform')\n",
    "    print(\"Using uniform sampling\")\n",
    "\n",
    "print(f\"Points shape: {points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_phi_to_tensor(phi_components):\n",
    "    \"\"\"Expand 35 components to full (N, 7, 7, 7) tensor.\"\"\"\n",
    "    N = phi_components.shape[0]\n",
    "    phi_full = torch.zeros(N, 7, 7, 7, device=phi_components.device, dtype=phi_components.dtype)\n",
    "    \n",
    "    idx = 0\n",
    "    for i in range(7):\n",
    "        for j in range(i + 1, 7):\n",
    "            for k in range(j + 1, 7):\n",
    "                val = phi_components[:, idx]\n",
    "                phi_full[:, i, j, k] = val\n",
    "                phi_full[:, i, k, j] = -val\n",
    "                phi_full[:, j, i, k] = -val\n",
    "                phi_full[:, j, k, i] = val\n",
    "                phi_full[:, k, i, j] = val\n",
    "                phi_full[:, k, j, i] = -val\n",
    "                idx += 1\n",
    "    return phi_full\n",
    "\n",
    "\n",
    "def compute_metric(phi_full):\n",
    "    \"\"\"Compute g_ij from phi.\"\"\"\n",
    "    return torch.einsum('...ikl,...jkl->...ij', phi_full, phi_full) / 6.0\n",
    "\n",
    "\n",
    "# Compute phi and metric at all points\n",
    "print(\"Computing G2 structure at all points...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    phi = model(points)  # (N, 35)\n",
    "    phi_full = expand_phi_to_tensor(phi)  # (N, 7, 7, 7)\n",
    "    metric = compute_metric(phi_full)  # (N, 7, 7)\n",
    "\n",
    "print(f\"phi shape: {phi.shape}\")\n",
    "print(f\"metric shape: {metric.shape}\")\n",
    "print(f\"det(g) mean: {torch.det(metric).mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Graph Laplacian for 3-Forms\n",
    "\n",
    "The Hodge Laplacian on 3-forms: $\\Delta_3 = d d^* + d^* d$\n",
    "\n",
    "We approximate this using a discrete graph Laplacian weighted by the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knn_graph(points: torch.Tensor, k: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build k-nearest neighbor graph.\n",
    "    \n",
    "    Returns:\n",
    "        indices: (N, k) neighbor indices\n",
    "        distances: (N, k) distances to neighbors\n",
    "    \"\"\"\n",
    "    points_np = points.cpu().numpy()\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(points_np)\n",
    "    distances, indices = nbrs.kneighbors(points_np)\n",
    "    \n",
    "    # Remove self-connections\n",
    "    return indices[:, 1:], distances[:, 1:]\n",
    "\n",
    "\n",
    "# Build k-NN graph\n",
    "K_NEIGHBORS = 30  # More neighbors for better approximation\n",
    "print(f\"Building {K_NEIGHBORS}-NN graph...\")\n",
    "\n",
    "neighbor_idx, neighbor_dist = build_knn_graph(points, k=K_NEIGHBORS)\n",
    "print(f\"Graph built: {neighbor_idx.shape}\")\n",
    "print(f\"Mean distance: {neighbor_dist.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hodge_laplacian_3forms(points: torch.Tensor, phi: torch.Tensor, \n",
    "                                  metric: torch.Tensor, neighbor_idx: np.ndarray,\n",
    "                                  neighbor_dist: np.ndarray) -> sparse.csr_matrix:\n",
    "    \"\"\"\n",
    "    Build discrete Hodge Laplacian for 3-forms.\n",
    "    \n",
    "    For 3-forms on a 7-manifold:\n",
    "    - dim(Lambda^3) = C(7,3) = 35\n",
    "    - Total DOFs = N_points * 35\n",
    "    \n",
    "    The Laplacian couples neighboring points through the metric.\n",
    "    \"\"\"\n",
    "    N = points.shape[0]\n",
    "    n_3forms = 35\n",
    "    total_dof = N * n_3forms\n",
    "    \n",
    "    print(f\"Building Hodge Laplacian: {total_dof} DOFs\")\n",
    "    \n",
    "    # Convert to numpy\n",
    "    phi_np = phi.cpu().numpy()  # (N, 35)\n",
    "    metric_np = metric.cpu().numpy()  # (N, 7, 7)\n",
    "    \n",
    "    # Build sparse Laplacian\n",
    "    # Using weighted graph Laplacian: L = D - W\n",
    "    # Weight w_ij = exp(-d_ij^2 / sigma^2) * metric_coupling\n",
    "    \n",
    "    sigma = neighbor_dist.mean() * 1.5\n",
    "    \n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    \n",
    "    print(\"Computing Laplacian entries...\")\n",
    "    for i in tqdm(range(N)):\n",
    "        # Diagonal block: sum of weights\n",
    "        diag_weight = 0.0\n",
    "        \n",
    "        for k_idx, j in enumerate(neighbor_idx[i]):\n",
    "            d_ij = neighbor_dist[i, k_idx]\n",
    "            w_ij = np.exp(-d_ij**2 / sigma**2)\n",
    "            \n",
    "            # Metric coupling between phi at i and j\n",
    "            # Use inner product of 3-forms: <phi_i, phi_j>_g\n",
    "            phi_i = phi_np[i]  # (35,)\n",
    "            phi_j = phi_np[j]  # (35,)\n",
    "            \n",
    "            # Simple coupling: correlation of phi values\n",
    "            coupling = np.abs(np.dot(phi_i, phi_j)) / (np.linalg.norm(phi_i) * np.linalg.norm(phi_j) + 1e-10)\n",
    "            \n",
    "            weight = w_ij * (0.5 + 0.5 * coupling)\n",
    "            diag_weight += weight\n",
    "            \n",
    "            # Off-diagonal: -weight for each 3-form component\n",
    "            for f in range(n_3forms):\n",
    "                row = i * n_3forms + f\n",
    "                col = j * n_3forms + f\n",
    "                rows.append(row)\n",
    "                cols.append(col)\n",
    "                data.append(-weight)\n",
    "        \n",
    "        # Diagonal entries\n",
    "        for f in range(n_3forms):\n",
    "            row = i * n_3forms + f\n",
    "            rows.append(row)\n",
    "            cols.append(row)\n",
    "            data.append(diag_weight)\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    laplacian = sparse.csr_matrix((data, (rows, cols)), shape=(total_dof, total_dof))\n",
    "    \n",
    "    # Symmetrize\n",
    "    laplacian = (laplacian + laplacian.T) / 2\n",
    "    \n",
    "    print(f\"Laplacian shape: {laplacian.shape}\")\n",
    "    print(f\"Non-zeros: {laplacian.nnz}\")\n",
    "    \n",
    "    return laplacian\n",
    "\n",
    "\n",
    "# Build Laplacian\n",
    "laplacian = build_hodge_laplacian_3forms(points, phi, metric, neighbor_idx, neighbor_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Spectrum and Find Gap at b3 = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrum(laplacian: sparse.csr_matrix, n_eigenvalues: int = 150) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute smallest eigenvalues of Laplacian.\n",
    "    \n",
    "    For b3 = 77, we expect 77 eigenvalues near 0 (harmonic forms).\n",
    "    \"\"\"\n",
    "    print(f\"Computing {n_eigenvalues} smallest eigenvalues...\")\n",
    "    \n",
    "    # Shift-invert mode for small eigenvalues\n",
    "    eigenvalues, eigenvectors = eigsh(\n",
    "        laplacian, \n",
    "        k=n_eigenvalues, \n",
    "        which='SM',  # Smallest magnitude\n",
    "        tol=1e-8,\n",
    "        maxiter=5000,\n",
    "    )\n",
    "    \n",
    "    # Sort by magnitude\n",
    "    idx = np.argsort(np.abs(eigenvalues))\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    print(f\"Computed {len(eigenvalues)} eigenvalues\")\n",
    "    print(f\"Range: [{eigenvalues.min():.6e}, {eigenvalues.max():.6e}]\")\n",
    "    \n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "\n",
    "# Compute spectrum\n",
    "N_EIGENVALUES = 150  # More than 77 to see the gap\n",
    "eigenvalues, eigenvectors = compute_spectrum(laplacian, n_eigenvalues=N_EIGENVALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_spectral_gap(eigenvalues: np.ndarray, target_b3: int = 77) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze spectrum to find gap at b3.\n",
    "    \n",
    "    For a manifold with b3 = 77:\n",
    "    - First 77 eigenvalues should be small (near-harmonic forms)\n",
    "    - Clear gap after eigenvalue #77\n",
    "    \"\"\"\n",
    "    # Take absolute values (eigenvalues should be non-negative)\n",
    "    eigs = np.abs(eigenvalues)\n",
    "    eigs = np.sort(eigs)\n",
    "    \n",
    "    # Compute gaps\n",
    "    gaps = np.diff(eigs)\n",
    "    \n",
    "    # Find largest gaps\n",
    "    gap_order = np.argsort(gaps)[::-1]\n",
    "    \n",
    "    # Statistics\n",
    "    mean_gap = gaps.mean()\n",
    "    std_gap = gaps.std()\n",
    "    \n",
    "    results = {\n",
    "        'n_eigenvalues': len(eigs),\n",
    "        'eigenvalues': eigs.tolist(),\n",
    "        'gaps': gaps.tolist(),\n",
    "        'mean_gap': float(mean_gap),\n",
    "        'std_gap': float(std_gap),\n",
    "        'largest_gaps': [],\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSpectral Gap Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Target b3: {target_b3}\")\n",
    "    print(f\"Mean gap: {mean_gap:.6e}\")\n",
    "    print(f\"Std gap: {std_gap:.6e}\")\n",
    "    print(\"\\nLargest gaps:\")\n",
    "    \n",
    "    for i, idx in enumerate(gap_order[:10]):\n",
    "        position = idx + 1  # Gap after eigenvalue #(idx+1)\n",
    "        gap_value = gaps[idx]\n",
    "        gap_ratio = gap_value / mean_gap\n",
    "        \n",
    "        marker = \"\" \n",
    "        if position == target_b3:\n",
    "            marker = \" <-- TARGET b3=77!\"\n",
    "        elif abs(position - target_b3) <= 3:\n",
    "            marker = f\" (near target, off by {position - target_b3})\"\n",
    "        \n",
    "        print(f\"  #{i+1}: Gap after eigenvalue {position}: {gap_value:.6e} ({gap_ratio:.1f}x mean){marker}\")\n",
    "        \n",
    "        results['largest_gaps'].append({\n",
    "            'position': int(position),\n",
    "            'gap_value': float(gap_value),\n",
    "            'gap_ratio': float(gap_ratio),\n",
    "        })\n",
    "    \n",
    "    # Check gap at target position\n",
    "    if target_b3 < len(gaps):\n",
    "        target_gap = gaps[target_b3 - 1]\n",
    "        target_ratio = target_gap / mean_gap\n",
    "        results['gap_at_target'] = {\n",
    "            'position': target_b3,\n",
    "            'gap_value': float(target_gap),\n",
    "            'gap_ratio': float(target_ratio),\n",
    "        }\n",
    "        print(f\"\\nGap at target b3={target_b3}: {target_gap:.6e} ({target_ratio:.1f}x mean)\")\n",
    "    \n",
    "    # Determine best candidate for b3\n",
    "    best_gap_pos = gap_order[0] + 1\n",
    "    results['best_candidate_b3'] = int(best_gap_pos)\n",
    "    results['distance_from_target'] = int(abs(best_gap_pos - target_b3))\n",
    "    \n",
    "    # Verdict\n",
    "    if best_gap_pos == target_b3:\n",
    "        results['verdict'] = 'EXACT_MATCH'\n",
    "        print(f\"\\n*** EXACT MATCH: Largest gap at position {target_b3} = b3 ***\")\n",
    "    elif abs(best_gap_pos - target_b3) <= 5:\n",
    "        results['verdict'] = 'CLOSE_MATCH'\n",
    "        print(f\"\\nClose match: Largest gap at {best_gap_pos}, target is {target_b3}\")\n",
    "    else:\n",
    "        results['verdict'] = 'MISMATCH'\n",
    "        print(f\"\\nMismatch: Largest gap at {best_gap_pos}, target is {target_b3}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Analyze spectrum\n",
    "spectral_results = analyze_spectral_gap(eigenvalues, target_b3=TARGET_B3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spectrum\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "eigs = np.abs(np.sort(eigenvalues))\n",
    "gaps = np.diff(eigs)\n",
    "\n",
    "# 1. Eigenvalue spectrum\n",
    "ax = axes[0, 0]\n",
    "ax.semilogy(range(1, len(eigs)+1), eigs, 'b.-', markersize=3)\n",
    "ax.axvline(x=TARGET_B3, color='r', linestyle='--', linewidth=2, label=f'Target b3={TARGET_B3}')\n",
    "ax.axvline(x=spectral_results['best_candidate_b3'], color='g', linestyle=':', linewidth=2, \n",
    "           label=f'Best gap at {spectral_results[\"best_candidate_b3\"]}')\n",
    "ax.set_xlabel('Eigenvalue index')\n",
    "ax.set_ylabel('Eigenvalue (log scale)')\n",
    "ax.set_title('Hodge Laplacian Spectrum')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Gaps\n",
    "ax = axes[0, 1]\n",
    "ax.bar(range(1, len(gaps)+1), gaps, width=1, alpha=0.7)\n",
    "ax.axvline(x=TARGET_B3, color='r', linestyle='--', linewidth=2, label=f'Target b3={TARGET_B3}')\n",
    "ax.set_xlabel('Gap position')\n",
    "ax.set_ylabel('Gap size')\n",
    "ax.set_title('Spectral Gaps')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Zoom around b3\n",
    "ax = axes[1, 0]\n",
    "zoom_start = max(0, TARGET_B3 - 30)\n",
    "zoom_end = min(len(eigs), TARGET_B3 + 30)\n",
    "ax.semilogy(range(zoom_start+1, zoom_end+1), eigs[zoom_start:zoom_end], 'b.-', markersize=5)\n",
    "ax.axvline(x=TARGET_B3, color='r', linestyle='--', linewidth=2, label=f'Target b3={TARGET_B3}')\n",
    "ax.set_xlabel('Eigenvalue index')\n",
    "ax.set_ylabel('Eigenvalue (log scale)')\n",
    "ax.set_title(f'Zoom: Eigenvalues {zoom_start+1}-{zoom_end}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gap ratios\n",
    "ax = axes[1, 1]\n",
    "gap_ratios = gaps / gaps.mean()\n",
    "ax.bar(range(1, len(gap_ratios)+1), gap_ratios, width=1, alpha=0.7)\n",
    "ax.axvline(x=TARGET_B3, color='r', linestyle='--', linewidth=2, label=f'Target b3={TARGET_B3}')\n",
    "ax.axhline(y=3, color='orange', linestyle=':', label='3x threshold')\n",
    "ax.set_xlabel('Gap position')\n",
    "ax.set_ylabel('Gap ratio (vs mean)')\n",
    "ax.set_title('Normalized Gap Ratios')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spectral_analysis_b3.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to spectral_analysis_b3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persistent Homology with Gudhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_persistent_homology(points: np.ndarray, max_dimension: int = 3, \n",
    "                                 max_edge_length: float = 1.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute persistent homology using Gudhi.\n",
    "    \n",
    "    Returns Betti numbers at various filtration scales.\n",
    "    \"\"\"\n",
    "    if not GUDHI_AVAILABLE:\n",
    "        return {'error': 'Gudhi not available'}\n",
    "    \n",
    "    print(f\"Computing persistent homology...\")\n",
    "    print(f\"  Points: {len(points)}\")\n",
    "    print(f\"  Max dimension: {max_dimension}\")\n",
    "    print(f\"  Max edge length: {max_edge_length}\")\n",
    "    \n",
    "    # Build Rips complex\n",
    "    rips = gd.RipsComplex(points=points, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rips.create_simplex_tree(max_dimension=max_dimension + 1)\n",
    "    \n",
    "    print(f\"  Simplices: {simplex_tree.num_simplices()}\")\n",
    "    \n",
    "    # Compute persistence\n",
    "    simplex_tree.compute_persistence()\n",
    "    \n",
    "    # Get Betti numbers\n",
    "    betti = simplex_tree.betti_numbers()\n",
    "    \n",
    "    # Get persistence pairs\n",
    "    persistence = simplex_tree.persistence()\n",
    "    \n",
    "    results = {\n",
    "        'betti_numbers': betti,\n",
    "        'num_simplices': simplex_tree.num_simplices(),\n",
    "        'persistence_pairs': [(dim, (birth, death)) for dim, (birth, death) in persistence],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBetti numbers: {betti}\")\n",
    "    print(f\"  b0 = {betti[0]} (connected components)\")\n",
    "    print(f\"  b1 = {betti[1] if len(betti) > 1 else 0} (1-cycles)\")\n",
    "    print(f\"  b2 = {betti[2] if len(betti) > 2 else 0} (2-cycles) [target: {TARGET_B2}]\")\n",
    "    print(f\"  b3 = {betti[3] if len(betti) > 3 else 0} (3-cycles) [target: {TARGET_B3}]\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if GUDHI_AVAILABLE:\n",
    "    # Use subset of points (Gudhi can be slow for large point clouds)\n",
    "    n_subset = min(2000, N_POINTS)\n",
    "    subset_idx = np.random.choice(N_POINTS, n_subset, replace=False)\n",
    "    points_subset = points[subset_idx].cpu().numpy()\n",
    "    \n",
    "    homology_results = compute_persistent_homology(\n",
    "        points_subset, \n",
    "        max_dimension=3,\n",
    "        max_edge_length=0.8\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping persistent homology (Gudhi not installed)\")\n",
    "    homology_results = {'error': 'Gudhi not available'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot persistence diagram\n",
    "if GUDHI_AVAILABLE and 'persistence_pairs' in homology_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Persistence diagram\n",
    "    ax = axes[0]\n",
    "    colors = ['blue', 'orange', 'green', 'red']\n",
    "    for dim, (birth, death) in homology_results['persistence_pairs']:\n",
    "        if death == float('inf'):\n",
    "            death = 2.0  # Cap for visualization\n",
    "        ax.scatter(birth, death, c=colors[dim % len(colors)], s=20, alpha=0.6, label=f'H{dim}' if birth == 0 else '')\n",
    "    \n",
    "    ax.plot([0, 2], [0, 2], 'k--', alpha=0.3)\n",
    "    ax.set_xlabel('Birth')\n",
    "    ax.set_ylabel('Death')\n",
    "    ax.set_title('Persistence Diagram')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Betti number bar chart\n",
    "    ax = axes[1]\n",
    "    betti = homology_results['betti_numbers']\n",
    "    x = range(len(betti))\n",
    "    ax.bar(x, betti, color=['blue', 'orange', 'green', 'red'][:len(betti)])\n",
    "    \n",
    "    # Add target lines\n",
    "    if len(betti) > 2:\n",
    "        ax.axhline(y=TARGET_B2, color='green', linestyle='--', alpha=0.5, label=f'Target b2={TARGET_B2}')\n",
    "    if len(betti) > 3:\n",
    "        ax.axhline(y=TARGET_B3, color='red', linestyle='--', alpha=0.5, label=f'Target b3={TARGET_B3}')\n",
    "    \n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_ylabel('Betti number')\n",
    "    ax.set_title('Betti Numbers')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'b{i}' for i in x])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('persistent_homology.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Three-Generation Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generation_structure(eigenvalues: np.ndarray, eigenvectors: np.ndarray,\n",
    "                                  b3: int = 77, n_gen: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze if eigenvalues cluster into 3 generations.\n",
    "    \n",
    "    For GIFT, the 77 harmonic 3-forms should cluster into 3 families\n",
    "    corresponding to 3 fermion generations.\n",
    "    \"\"\"\n",
    "    # Take first b3 eigenvalues (harmonic forms)\n",
    "    eigs = np.abs(eigenvalues[:b3])\n",
    "    \n",
    "    # K-means clustering into n_gen groups\n",
    "    kmeans = KMeans(n_clusters=n_gen, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(eigs.reshape(-1, 1))\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_sizes = [np.sum(labels == i) for i in range(n_gen)]\n",
    "    cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "    \n",
    "    # Sort by cluster center\n",
    "    order = np.argsort(cluster_centers)\n",
    "    cluster_sizes = [cluster_sizes[i] for i in order]\n",
    "    cluster_centers = cluster_centers[order]\n",
    "    \n",
    "    results = {\n",
    "        'n_generations': n_gen,\n",
    "        'cluster_sizes': cluster_sizes,\n",
    "        'cluster_centers': cluster_centers.tolist(),\n",
    "        'total_forms': sum(cluster_sizes),\n",
    "    }\n",
    "    \n",
    "    print(\"\\nThree-Generation Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Clustering first {b3} eigenvalues into {n_gen} groups:\\n\")\n",
    "    \n",
    "    for i, (size, center) in enumerate(zip(cluster_sizes, cluster_centers)):\n",
    "        print(f\"  Generation {i+1}: {size} forms (center: {center:.6e})\")\n",
    "    \n",
    "    # Check if roughly equal\n",
    "    expected_size = b3 // n_gen\n",
    "    size_variance = np.var(cluster_sizes)\n",
    "    balanced = all(abs(s - expected_size) <= expected_size * 0.3 for s in cluster_sizes)\n",
    "    \n",
    "    results['expected_size'] = expected_size\n",
    "    results['size_variance'] = float(size_variance)\n",
    "    results['balanced'] = balanced\n",
    "    \n",
    "    print(f\"\\nExpected ~{expected_size} forms per generation\")\n",
    "    print(f\"Balance: {'GOOD' if balanced else 'UNBALANCED'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Analyze generation structure\n",
    "n_harmonic = min(TARGET_B3, len(eigenvalues))\n",
    "generation_results = analyze_generation_structure(eigenvalues, eigenvectors, b3=n_harmonic, n_gen=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Topology Certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile topology certificate\n",
    "topology_certificate = {\n",
    "    'type': 'G2_TOPOLOGY_CERTIFICATE',\n",
    "    'version': '2.0',\n",
    "    'targets': {\n",
    "        'b2': TARGET_B2,\n",
    "        'b3': TARGET_B3,\n",
    "        'n_gen': 3,\n",
    "    },\n",
    "    'mesh': {\n",
    "        'n_points': N_POINTS,\n",
    "        'k_neighbors': K_NEIGHBORS,\n",
    "    },\n",
    "    'spectral_analysis': {\n",
    "        'n_eigenvalues': len(eigenvalues),\n",
    "        'best_gap_position': spectral_results['best_candidate_b3'],\n",
    "        'target_gap_position': TARGET_B3,\n",
    "        'distance_from_target': spectral_results['distance_from_target'],\n",
    "        'verdict': spectral_results['verdict'],\n",
    "        'gap_at_target': spectral_results.get('gap_at_target', {}),\n",
    "        'largest_gaps': spectral_results['largest_gaps'][:5],\n",
    "    },\n",
    "    'persistent_homology': homology_results if GUDHI_AVAILABLE else {'status': 'not_computed'},\n",
    "    'generation_structure': generation_results,\n",
    "}\n",
    "\n",
    "# Determine overall status\n",
    "if spectral_results['verdict'] == 'EXACT_MATCH':\n",
    "    topology_certificate['status'] = 'VERIFIED'\n",
    "    topology_certificate['conclusion'] = f\"b3 = {TARGET_B3} VERIFIED: Spectral gap at exact position.\"\n",
    "elif spectral_results['verdict'] == 'CLOSE_MATCH':\n",
    "    topology_certificate['status'] = 'LIKELY_VERIFIED'\n",
    "    topology_certificate['conclusion'] = (\n",
    "        f\"b3 = {TARGET_B3} LIKELY: Gap at position {spectral_results['best_candidate_b3']}, \"\n",
    "        f\"within {spectral_results['distance_from_target']} of target.\"\n",
    "    )\n",
    "else:\n",
    "    topology_certificate['status'] = 'INCONCLUSIVE'\n",
    "    topology_certificate['conclusion'] = (\n",
    "        f\"b3 = {TARGET_B3} INCONCLUSIVE: Best gap at {spectral_results['best_candidate_b3']}. \"\n",
    "        f\"May need higher resolution mesh.\"\n",
    "    )\n",
    "\n",
    "# Save certificate\n",
    "with open('topology_certificate.json', 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON\n",
    "    def convert(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert(v) for v in obj]\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return float(obj)\n",
    "        return obj\n",
    "    \n",
    "    json.dump(convert(topology_certificate), f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOPOLOGY CERTIFICATE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nStatus: {topology_certificate['status']}\")\n",
    "print(f\"\\n{topology_certificate['conclusion']}\")\n",
    "print(f\"\\nSaved to topology_certificate.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full certificate\n",
    "print(json.dumps(topology_certificate, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
