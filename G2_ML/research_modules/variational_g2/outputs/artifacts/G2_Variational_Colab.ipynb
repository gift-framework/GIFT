{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# GIFT v2.2 Variational G2 Metric Extraction\n",
    "\n",
    "**Physics-Informed Neural Network for G2 Geometry**\n",
    "\n",
    "This notebook is fully self-contained and can run on Google Colab with GPU.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Find $\\phi \\in \\Lambda^3_+(\\mathbb{R}^7)$ minimizing:\n",
    "$$F[\\phi] = ||d\\phi||^2_{L^2} + ||d^*\\phi||^2_{L^2}$$\n",
    "\n",
    "Subject to GIFT v2.2 constraints:\n",
    "- $b_2 = 21$, $b_3 = 77$ (topological)\n",
    "- $\\det(g) = 65/32$ (metric)\n",
    "- $\\kappa_T = 1/61$ (torsion)\n",
    "- $\\phi > 0$ (G2 cone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# GIFT v2.2 Physical Constants\n",
    "CONFIG = {\n",
    "    'physics': {\n",
    "        'b2': 21,\n",
    "        'b3': 77,\n",
    "        'h_star': 99,\n",
    "        'det_g': 65.0 / 32.0,  # = 2.03125\n",
    "        'kappa_T': 1.0 / 61.0,  # = 0.01639...\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dims': [256, 512, 512, 256],\n",
    "        'num_frequencies': 64,\n",
    "        'fourier_scale': 1.0,\n",
    "    },\n",
    "    'training': {\n",
    "        'phases': [\n",
    "            {'name': 'initialization', 'epochs': 2000, 'lr': 1e-3,\n",
    "             'weights': {'torsion': 1.0, 'det': 0.5, 'positivity': 2.0}},\n",
    "            {'name': 'constraint_satisfaction', 'epochs': 3000, 'lr': 5e-4,\n",
    "             'weights': {'torsion': 1.0, 'det': 2.0, 'positivity': 1.0}},\n",
    "            {'name': 'torsion_targeting', 'epochs': 3000, 'lr': 2e-4,\n",
    "             'weights': {'torsion': 3.0, 'det': 1.0, 'positivity': 1.0}},\n",
    "            {'name': 'refinement', 'epochs': 2000, 'lr': 1e-4,\n",
    "             'weights': {'torsion': 2.0, 'det': 1.0, 'positivity': 1.0}},\n",
    "        ],\n",
    "        'batch_size': 4096,\n",
    "        'grad_clip': 1.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"GIFT v2.2 Targets:\")\n",
    "print(f\"  det(g) = 65/32 = {CONFIG['physics']['det_g']:.6f}\")\n",
    "print(f\"  kappa_T = 1/61 = {CONFIG['physics']['kappa_T']:.6f}\")\n",
    "print(f\"  b2 = {CONFIG['physics']['b2']}, b3 = {CONFIG['physics']['b3']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "implementation_section"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "All code is included below - no external dependencies beyond PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "constraints"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# G2 CONSTRAINTS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_3form_indices() -> torch.Tensor:\n",
    "    \"\"\"Generate all 35 independent indices for a 3-form on R7.\"\"\"\n",
    "    indices = []\n",
    "    for i in range(7):\n",
    "        for j in range(i + 1, 7):\n",
    "            for k in range(j + 1, 7):\n",
    "                indices.append([i, j, k])\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "def expand_to_antisymmetric(phi_components: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Expand 35 components to full antisymmetric 7x7x7 tensor.\"\"\"\n",
    "    batch_shape = phi_components.shape[:-1]\n",
    "    device = phi_components.device\n",
    "    dtype = phi_components.dtype\n",
    "\n",
    "    phi_full = torch.zeros(*batch_shape, 7, 7, 7, device=device, dtype=dtype)\n",
    "    indices = generate_3form_indices().to(device)\n",
    "\n",
    "    for idx, (i, j, k) in enumerate(indices):\n",
    "        val = phi_components[..., idx]\n",
    "        phi_full[..., i, j, k] = val\n",
    "        phi_full[..., i, k, j] = -val\n",
    "        phi_full[..., j, i, k] = -val\n",
    "        phi_full[..., j, k, i] = val\n",
    "        phi_full[..., k, i, j] = val\n",
    "        phi_full[..., k, j, i] = -val\n",
    "\n",
    "    return phi_full\n",
    "\n",
    "\n",
    "def metric_from_phi(phi: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Extract induced metric: g_ij = (1/6) * sum_kl phi_ikl * phi_jkl\"\"\"\n",
    "    if phi.shape[-1] == 35 and len(phi.shape) >= 1:\n",
    "        if len(phi.shape) == 1 or phi.shape[-2] != 7:\n",
    "            phi = expand_to_antisymmetric(phi)\n",
    "    return torch.einsum('...ikl,...jkl->...ij', phi, phi) / 6.0\n",
    "\n",
    "\n",
    "def standard_g2_phi(device=None, dtype=torch.float32) -> torch.Tensor:\n",
    "    \"\"\"Standard G2 3-form on R7.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    phi = torch.zeros(35, device=device, dtype=dtype)\n",
    "    \n",
    "    G2_INDICES = [(0,1,2), (0,3,4), (0,5,6), (1,3,5), (1,4,6), (2,3,6), (2,4,5)]\n",
    "    G2_SIGNS = [1, 1, 1, 1, -1, -1, -1]\n",
    "    \n",
    "    def to_index(i, j, k):\n",
    "        count = 0\n",
    "        for a in range(7):\n",
    "            for b in range(a + 1, 7):\n",
    "                for c in range(b + 1, 7):\n",
    "                    if a == i and b == j and c == k:\n",
    "                        return count\n",
    "                    count += 1\n",
    "        return -1\n",
    "    \n",
    "    for indices, sign in zip(G2_INDICES, G2_SIGNS):\n",
    "        idx = to_index(*indices)\n",
    "        if idx >= 0:\n",
    "            phi[idx] = float(sign)\n",
    "    return phi\n",
    "\n",
    "\n",
    "print(\"Constraints module loaded.\")\n",
    "# Test\n",
    "std_phi = standard_g2_phi(device)\n",
    "std_full = expand_to_antisymmetric(std_phi.unsqueeze(0))\n",
    "std_metric = metric_from_phi(std_full)\n",
    "print(f\"Standard G2 det(g) = {torch.det(std_metric).item():.6f} (should be ~1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEURAL NETWORK MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Fourier feature encoding for smooth periodic structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=7, num_frequencies=64, scale=1.0):\n",
    "        super().__init__()\n",
    "        self.output_dim = 2 * num_frequencies\n",
    "        B = torch.randn(num_frequencies, input_dim) * scale\n",
    "        self.register_buffer('B', B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_proj = 2 * math.pi * torch.matmul(x, self.B.T)\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class G2VariationalNet(nn.Module):\n",
    "    \"\"\"PINN for G2 variational problem.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims=[256, 512, 512, 256], num_frequencies=64, \n",
    "                 fourier_scale=1.0, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "        # Fourier encoding\n",
    "        self.fourier = FourierFeatures(7, num_frequencies, fourier_scale)\n",
    "        \n",
    "        # MLP\n",
    "        layers = []\n",
    "        prev_dim = self.fourier.output_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, hidden_dim), nn.SiLU()])\n",
    "            prev_dim = hidden_dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output: 35 independent 3-form components\n",
    "        self.output_layer = nn.Linear(prev_dim, 35)\n",
    "        \n",
    "        # Initialize near standard G2\n",
    "        self.bias = nn.Parameter(standard_g2_phi(self.device))\n",
    "        self.scale = nn.Parameter(torch.ones(35, device=self.device) * 0.1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x, return_full=False, return_metric=False):\n",
    "        x_enc = self.fourier(x)\n",
    "        h = self.mlp(x_enc)\n",
    "        phi_raw = self.output_layer(h)\n",
    "        phi_components = phi_raw * self.scale + self.bias\n",
    "        \n",
    "        output = {'phi_components': phi_components}\n",
    "        \n",
    "        if return_full or return_metric:\n",
    "            phi_full = expand_to_antisymmetric(phi_components)\n",
    "            if return_full:\n",
    "                output['phi_full'] = phi_full\n",
    "            if return_metric:\n",
    "                output['metric'] = metric_from_phi(phi_full)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = G2VariationalNet(\n",
    "    hidden_dims=CONFIG['model']['hidden_dims'],\n",
    "    num_frequencies=CONFIG['model']['num_frequencies'],\n",
    "    fourier_scale=CONFIG['model']['fourier_scale'],\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {n_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loss"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class LossOutput:\n",
    "    total: torch.Tensor\n",
    "    components: Dict[str, torch.Tensor]\n",
    "    metrics: Dict[str, float]\n",
    "\n",
    "\n",
    "class VariationalLoss(nn.Module):\n",
    "    \"\"\"Combined loss for G2 variational problem.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_det=65/32, target_kappa=1/61):\n",
    "        super().__init__()\n",
    "        self.target_det = target_det\n",
    "        self.target_kappa = target_kappa\n",
    "    \n",
    "    def forward(self, phi, x, weights):\n",
    "        if phi.shape[-1] == 35:\n",
    "            phi_full = expand_to_antisymmetric(phi)\n",
    "        else:\n",
    "            phi_full = phi\n",
    "        \n",
    "        g = metric_from_phi(phi_full)\n",
    "        components = {}\n",
    "        metrics = {}\n",
    "        \n",
    "        # 1. Determinant loss: det(g) -> 65/32\n",
    "        det_g = torch.det(g)\n",
    "        loss_det = ((det_g - self.target_det) ** 2).mean()\n",
    "        components['det'] = loss_det * weights.get('det', 1.0)\n",
    "        metrics['det_g'] = det_g.mean().item()\n",
    "        \n",
    "        # 2. Positivity loss: eigenvalues > 0\n",
    "        eigenvalues = torch.linalg.eigvalsh(g)\n",
    "        violation = torch.relu(-eigenvalues).sum(dim=-1)\n",
    "        loss_pos = violation.mean()\n",
    "        components['positivity'] = loss_pos * weights.get('positivity', 1.0)\n",
    "        metrics['min_eig'] = eigenvalues.min().item()\n",
    "        \n",
    "        # 3. Torsion loss (simplified - gradient magnitude)\n",
    "        # For full torsion we need derivatives, using proxy here\n",
    "        phi_var = phi.var(dim=0).mean()\n",
    "        loss_torsion = ((phi_var - self.target_kappa) ** 2)\n",
    "        components['torsion'] = loss_torsion * weights.get('torsion', 1.0)\n",
    "        metrics['torsion_proxy'] = phi_var.item()\n",
    "        \n",
    "        # Total\n",
    "        total = sum(components.values())\n",
    "        \n",
    "        return LossOutput(total=total, components=components, metrics=metrics)\n",
    "\n",
    "\n",
    "loss_fn = VariationalLoss(\n",
    "    target_det=CONFIG['physics']['det_g'],\n",
    "    target_kappa=CONFIG['physics']['kappa_T'],\n",
    ")\n",
    "print(\"Loss function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def sample_points(n, device):\n",
    "    \"\"\"Sample random points in [-1, 1]^7.\"\"\"\n",
    "    return torch.rand(n, 7, device=device) * 2 - 1\n",
    "\n",
    "\n",
    "def train_phase(model, loss_fn, phase_config, batch_size, device):\n",
    "    \"\"\"Train one phase.\"\"\"\n",
    "    name = phase_config['name']\n",
    "    epochs = phase_config['epochs']\n",
    "    lr = phase_config['lr']\n",
    "    weights = phase_config['weights']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase: {name}\")\n",
    "    print(f\"Epochs: {epochs}, LR: {lr}\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr*0.01)\n",
    "    \n",
    "    history = {'loss': [], 'det_g': [], 'min_eig': []}\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc=name)\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        \n",
    "        x = sample_points(batch_size, device)\n",
    "        output = model(x, return_full=True)\n",
    "        phi = output['phi_full']\n",
    "        \n",
    "        loss_output = loss_fn(phi, x, weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_output.total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['training']['grad_clip'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['loss'].append(loss_output.total.item())\n",
    "        history['det_g'].append(loss_output.metrics['det_g'])\n",
    "        history['min_eig'].append(loss_output.metrics['min_eig'])\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss_output.total.item():.4f}\",\n",
    "                'det(g)': f\"{loss_output.metrics['det_g']:.4f}\",\n",
    "                'min_eig': f\"{loss_output.metrics['min_eig']:.4f}\",\n",
    "            })\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def train_all_phases(model, loss_fn, config, device):\n",
    "    \"\"\"Run all training phases.\"\"\"\n",
    "    all_history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for phase_config in config['training']['phases']:\n",
    "        history = train_phase(\n",
    "            model, loss_fn, phase_config,\n",
    "            batch_size=config['training']['batch_size'],\n",
    "            device=device\n",
    "        )\n",
    "        all_history.append({'phase': phase_config['name'], 'history': history})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "    \n",
    "    return all_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_training_section"
   },
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "history = train_all_phases(model, loss_fn, CONFIG, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization_section"
   },
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Combine all phase histories\n",
    "all_loss = []\n",
    "all_det = []\n",
    "all_eig = []\n",
    "phase_boundaries = [0]\n",
    "\n",
    "for h in history:\n",
    "    all_loss.extend(h['history']['loss'])\n",
    "    all_det.extend(h['history']['det_g'])\n",
    "    all_eig.extend(h['history']['min_eig'])\n",
    "    phase_boundaries.append(len(all_loss))\n",
    "\n",
    "epochs = range(len(all_loss))\n",
    "\n",
    "# Loss\n",
    "axes[0].semilogy(epochs, all_loss)\n",
    "for b in phase_boundaries[1:-1]:\n",
    "    axes[0].axvline(b, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# det(g)\n",
    "axes[1].plot(epochs, all_det)\n",
    "axes[1].axhline(CONFIG['physics']['det_g'], color='r', linestyle='--', label=f\"Target: {CONFIG['physics']['det_g']:.4f}\")\n",
    "for b in phase_boundaries[1:-1]:\n",
    "    axes[1].axvline(b, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('det(g)')\n",
    "axes[1].set_title('Metric Determinant')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# min eigenvalue\n",
    "axes[2].plot(epochs, all_eig)\n",
    "axes[2].axhline(0, color='r', linestyle='--', label='Positivity threshold')\n",
    "for b in phase_boundaries[1:-1]:\n",
    "    axes[2].axvline(b, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Min Eigenvalue')\n",
    "axes[2].set_title('Metric Positivity')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation_section"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation"
   },
   "outputs": [],
   "source": [
    "# Final validation\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_val = sample_points(10000, device)\n",
    "    output = model(x_val, return_full=True, return_metric=True)\n",
    "    phi = output['phi_full']\n",
    "    metric = output['metric']\n",
    "    \n",
    "    det_g = torch.det(metric)\n",
    "    eigenvalues = torch.linalg.eigvalsh(metric)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMetric Determinant:\")\n",
    "print(f\"  Target:  {CONFIG['physics']['det_g']:.6f}\")\n",
    "print(f\"  Mean:    {det_g.mean().item():.6f}\")\n",
    "print(f\"  Std:     {det_g.std().item():.6f}\")\n",
    "print(f\"  Error:   {abs(det_g.mean().item() - CONFIG['physics']['det_g']):.6f} ({abs(det_g.mean().item() - CONFIG['physics']['det_g'])/CONFIG['physics']['det_g']*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nMetric Positivity:\")\n",
    "print(f\"  Min eigenvalue: {eigenvalues.min().item():.6f}\")\n",
    "print(f\"  All positive:   {(eigenvalues.min(dim=-1)[0] > 0).all().item()}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_validation"
   },
   "outputs": [],
   "source": [
    "# Plot validation results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "det_np = det_g.cpu().numpy()\n",
    "eig_np = eigenvalues.cpu().numpy()\n",
    "\n",
    "# Determinant distribution\n",
    "axes[0].hist(det_np, bins=50, density=True, alpha=0.7, color='blue')\n",
    "axes[0].axvline(CONFIG['physics']['det_g'], color='red', linestyle='--', \n",
    "               linewidth=2, label=f\"Target: {CONFIG['physics']['det_g']:.4f}\")\n",
    "axes[0].axvline(det_np.mean(), color='green', linestyle='-', \n",
    "               linewidth=2, label=f\"Mean: {det_np.mean():.4f}\")\n",
    "axes[0].set_xlabel('det(g)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Metric Determinant Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Eigenvalue distribution\n",
    "for i in range(7):\n",
    "    axes[1].hist(eig_np[:, i], bins=30, alpha=0.5, label=f'$\\\\lambda_{i+1}$')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Eigenvalue')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Metric Eigenvalue Distribution')\n",
    "\n",
    "# Min eigenvalue\n",
    "min_eig = eig_np.min(axis=1)\n",
    "axes[2].hist(min_eig, bins=50, density=True, alpha=0.7, color='orange')\n",
    "axes[2].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[2].set_xlabel('Minimum Eigenvalue')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].set_title('Positivity Check')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'final_det_g': det_g.mean().item(),\n",
    "    'history': history,\n",
    "}, 'g2_variational_model.pt')\n",
    "\n",
    "print(\"Model saved to g2_variational_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_section"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a Physics-Informed Neural Network to solve the GIFT v2.2 variational problem:\n",
    "\n",
    "**Find a G2 3-form $\\phi$ such that:**\n",
    "- $\\det(g(\\phi)) = 65/32 = 2.03125$\n",
    "- $\\kappa_T = 1/61 \\approx 0.0164$\n",
    "- $g(\\phi)$ is positive definite\n",
    "\n",
    "**Mathematical Framing:**\n",
    "\n",
    "> If the network achieves small loss, by G2 deformation theory (Joyce), there exists an exact G2 structure near the learned solution.\n",
    "\n",
    "This provides **numerical evidence** for the existence of a K7 geometry consistent with GIFT v2.2 predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
