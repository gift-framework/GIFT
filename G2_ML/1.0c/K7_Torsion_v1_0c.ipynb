{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# K\u00e2\u201a\u2021 Torsional Geometry and Cohomological Calibration - GIFT v1.0c\n",
    "\n",
    "**Framework**: GIFT (Geometric Information Field Theory) v2.0+  \n",
    "**Version**: 1.0c  \n",
    "**Input**: K\u00e2\u201a\u2021 Torsional Geodesics v1.0b\n",
    "\n",
    "## Scientific Summary\n",
    "\n",
    "This notebook implements a complete geometric calibration pipeline for K\u00e2\u201a\u2021 with G\u00e2\u201a\u201a holonomy, building upon the torsional geodesic model from v1.0b.\n",
    "\n",
    "### I. Calibration Objectives\n",
    "\n",
    "Starting from v1.0b outputs (metric, torsion tensor, geodesic flow), we calibrate the local geometry to match GIFT theoretical targets:\n",
    "\n",
    "1. **Metric determinant**: $\\det(\\tilde{g}) \\to p_2 = 2$ (binary duality invariant)\n",
    "2. **Torsion norm**: $\\|\\tilde{T}\\| \\to 0.0164$ (from physical analysis)\n",
    "3. **Flow speed**: $|v| \\approx 0.015$ (ultra-slow geodesic regime)\n",
    "\n",
    "### II. Cohomological Model\n",
    "\n",
    "Using the calibrated metric $\\tilde{g}$ and torsion $\\tilde{T}$, we construct discrete Laplace-de Rham operators on the $(e,\\pi,\\phi)$ patch:\n",
    "\n",
    "$$\\Delta_p = d\\delta + \\delta d$$\n",
    "\n",
    "for $p = 0, 1, 2, 3$. The near-zero eigenvalues provide effective Betti number estimates:\n",
    "\n",
    "- $b_2^{\\text{eff}}$ (harmonic 2-forms) \u00e2\u2020\u2019 target: 21\n",
    "- $b_3^{\\text{eff}}$ (harmonic 3-forms) \u00e2\u2020\u2019 target: 77\n",
    "\n",
    "### III. Yukawa Couplings\n",
    "\n",
    "From harmonic bases $\\{h^2_\\alpha\\}$ and $\\{h^3_\\gamma\\}$, we compute Yukawa-like integrals:\n",
    "\n",
    "$$Y_{\\alpha\\beta\\gamma} = \\int h^2_\\alpha \\wedge h^2_\\beta \\wedge h^3_\\gamma \\sqrt{\\det(\\tilde{g})} \\, d^3x$$\n",
    "\n",
    "via Monte Carlo sampling. The tensor structure $Y$ is analyzed for generational patterns compatible with 3 fermion families.\n",
    "\n",
    "### IV. Physical Predictions\n",
    "\n",
    "| Observable | Geometric Source | GIFT Target |\n",
    "|------------|------------------|-------------|\n",
    "| $\\det(g)$ | Volume quantization | $p_2 = 2$ |\n",
    "| $\\|T\\|$ | Torsion closure | $0.0164$ |\n",
    "| $b_2, b_3$ | G\u00e2\u201a\u201a holonomy | $(21, 77)$ |\n",
    "| $Y_{\\alpha\\beta\\gamma}$ | Yukawa structure | $3 \\times 3 \\times 3$ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Directory structure and dependencies\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "print('Setting up environment...')\n",
    "\n",
    "# Define working directories\n",
    "WORK_DIR = Path('./K7_torsion_v1_0c')\n",
    "CHECKPOINTS_DIR = WORK_DIR / 'checkpoints'\n",
    "RESULTS_DIR = WORK_DIR / 'results'\n",
    "INPUT_DIR = Path('./K7_torsional_v1_1')  # v1.0b outputs\n",
    "\n",
    "# Create directory structure\n",
    "for d in [WORK_DIR, CHECKPOINTS_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f'Work directory: {WORK_DIR.absolute()}')\n",
    "print(f'Input directory: {INPUT_DIR.absolute()}')\n",
    "print(f'Checkpoints: {CHECKPOINTS_DIR.absolute()}')\n",
    "print(f'Results: {RESULTS_DIR.absolute()}')\n",
    "\n",
    "# Force recompute flags (set to True to skip checkpoints)\n",
    "FORCE_RECOMPUTE = {\n",
    "    'section1': False,\n",
    "    'section2': False,\n",
    "    'section3': False,\n",
    "    'section4': False,\n",
    "    'section5': False\n",
    "}\n",
    "\n",
    "print('\\nSetup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Imports complete')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Device: {\"CUDA\" if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'version': 'v1.0c',\n",
    "    'input_version': 'v1.0b',\n",
    "    'seed': SEED,\n",
    "    \n",
    "    # Calibration targets from GIFT theory\n",
    "    'targets': {\n",
    "        'det_g': 2.0,           # Binary duality invariant p\u00e2\u201a\u201a\n",
    "        'T_norm': 0.0164,       # Global torsion norm\n",
    "        'flow_speed': 0.015,    # Ultra-slow geodesic regime\n",
    "        'b2': 21,               # G\u00e2\u201a\u201a holonomy: harmonic 2-forms\n",
    "        'b3': 77                # G\u00e2\u201a\u201a holonomy: harmonic 3-forms\n",
    "    },\n",
    "    \n",
    "    # Grid configuration for (e,\u00cf\u20ac,\u00cf\u2020) patch\n",
    "    'grid': {\n",
    "        'n_e': 32,\n",
    "        'n_pi': 32,\n",
    "        'n_phi': 32,\n",
    "        'e_range': [0.1, 2.0],\n",
    "        'pi_range': [0.1, 3.0],\n",
    "        'phi_range': [0.1, 1.5]\n",
    "    },\n",
    "    \n",
    "    # Laplacian spectrum computation\n",
    "    'laplacian': {\n",
    "        'n_eigenmodes': 100,      # Number of modes to compute\n",
    "        'harmonic_threshold': 1e-4,  # Eigenvalue threshold for harmonics\n",
    "        'tol': 1e-10,\n",
    "        'maxiter': 10000\n",
    "    },\n",
    "    \n",
    "    # Yukawa integral computation\n",
    "    'yukawa': {\n",
    "        'n_samples': 100000,      # Monte Carlo samples\n",
    "        'basis_size_2': 5,        # Number of 2-form basis elements\n",
    "        'basis_size_3': 5         # Number of 3-form basis elements\n",
    "    },\n",
    "    \n",
    "    # Geodesic integration\n",
    "    'geodesic': {\n",
    "        'lambda_min': -5.0,\n",
    "        'lambda_max': 5.0,\n",
    "        'n_steps': 1000,\n",
    "        'x0': [1.0, 1.5, 0.5],    # Initial position (e,\u00cf\u20ac,\u00cf\u2020)\n",
    "        'v0': [0.01, 0.01, 0.005] # Initial velocity\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Configuration loaded:')\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checkpoint_manager",
   "metadata": {},
   "outputs": [],
   "source": "# Checkpoint Manager\nclass CheckpointManager:\n    \"\"\"Manages checkpointing for reproducible, resumable computation.\"\"\"\n    \n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.base_dir.mkdir(exist_ok=True, parents=True)\n    \n    def save(self, section: str, data: Dict, metadata: Optional[Dict] = None) -> Path:\n        \"\"\"\n        Save checkpoint for a section.\n        \n        Args:\n            section: Section identifier (e.g., 'section1_data')\n            data: Dictionary of data to save\n            metadata: Optional metadata dictionary\n        \n        Returns:\n            Path to saved checkpoint\n        \"\"\"\n        checkpoint = {\n            'section': section,\n            'data': data,\n            'timestamp': time.time(),\n            'config': CONFIG,\n            'metadata': metadata or {}\n        }\n        \n        path = self.base_dir / f'{section}.pt'\n        torch.save(checkpoint, path)\n        \n        timestamp_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(checkpoint['timestamp']))\n        print(f\"Checkpoint saved: {path.name} ({timestamp_str})\")\n        \n        return path\n    \n    def load(self, section: str) -> Optional[Dict]:\n        \"\"\"\n        Load checkpoint for a section.\n        \n        Args:\n            section: Section identifier\n        \n        Returns:\n            Data dictionary or None if checkpoint doesn't exist\n        \"\"\"\n        path = self.base_dir / f'{section}.pt'\n        \n        if not path.exists():\n            return None\n        \n        checkpoint = torch.load(path, weights_only=False)\n        timestamp_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(checkpoint['timestamp']))\n        \n        print(f\"Checkpoint loaded: {path.name} ({timestamp_str})\")\n        \n        return checkpoint['data']\n    \n    def exists(self, section: str) -> bool:\n        \"\"\"Check if checkpoint exists for a section.\"\"\"\n        return (self.base_dir / f'{section}.pt').exists()\n    \n    def list_checkpoints(self) -> List[str]:\n        \"\"\"List all available checkpoints.\"\"\"\n        return sorted([p.stem for p in self.base_dir.glob('*.pt')])\n\n# Initialize checkpoint manager\nckpt_mgr = CheckpointManager(CHECKPOINTS_DIR)\n\nprint('Checkpoint manager initialized')\nexisting = ckpt_mgr.list_checkpoints()\nif existing:\n    print(f'Found {len(existing)} existing checkpoints: {existing}')\nelse:\n    print('No existing checkpoints found')"
  },
  {
   "cell_type": "markdown",
   "id": "section1_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading from v1.0b\n",
    "\n",
    "Load the outputs from K\u00e2\u201a\u2021 Torsional Geodesics v1.0b:\n",
    "- Trained model state\n",
    "- Training history\n",
    "- Physical analysis results (metric, torsion statistics)\n",
    "- Extract mean metric tensor and torsion components\n",
    "\n",
    "The v1.0b notebook provides:\n",
    "- Global torsion norm $\\|T\\|$\n",
    "- Key torsion components $T_{e\\pi\\phi}$, $T_{\\phi e\\pi}$\n",
    "- Closure measure $\\|d\\phi\\|$\n",
    "- Implicit metric structure from the 3-form $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_helpers",
   "metadata": {},
   "outputs": [],
   "source": "# Data loading helper functions\n\ndef load_v1_0b_data(input_dir: Path) -> Dict:\n    \"\"\"\n    Load all relevant data from K7_Torsional_Geodesics_v1_0b outputs.\n    \n    Args:\n        input_dir: Path to v1.0b output directory\n    \n    Returns:\n        Dictionary containing model state, history, and physical results\n    \"\"\"\n    data = {}\n    \n    # Load final checkpoint if it exists\n    ckpt_path = input_dir / 'checkpoints' / 'final.pt'\n    if ckpt_path.exists():\n        print(f'Loading checkpoint: {ckpt_path}')\n        final_ckpt = torch.load(ckpt_path, weights_only=False)\n        data['model_state'] = final_ckpt.get('model', None)\n        data['history'] = final_ckpt.get('history', None)\n        data['config'] = final_ckpt.get('config', None)\n    else:\n        print(f'Warning: Checkpoint not found at {ckpt_path}')\n    \n    # Load physical analysis results\n    results_path = input_dir / 'results' / 'final.json'\n    if results_path.exists():\n        print(f'Loading physical results: {results_path}')\n        with open(results_path) as f:\n            data['physical'] = json.load(f)\n    else:\n        print(f'Warning: Results not found at {results_path}')\n    \n    return data\n\n\ndef extract_metric_torsion(data: Dict) -> Tuple[np.ndarray, Dict]:\n    \"\"\"\n    Extract mean metric and torsion statistics from v1.0b data.\n    \n    Args:\n        data: v1.0b data dictionary\n    \n    Returns:\n        (g_mean, T_stats) where g_mean is 3x3 metric and T_stats contains torsion info\n    \"\"\"\n    if 'physical' not in data:\n        raise ValueError('Physical results not found in v1.0b data')\n    \n    phys = data['physical']\n    \n    # Reconstruct simplified 3x3 metric from (e,\u00cf\u20ac,\u00cf\u2020) patch\n    # Based on the v1.0b metric structure:\n    # g_ee ~ \u00cf\u2020, g_\u00cf\u20ac\u00cf\u20ac ~ 3/2, g_\u00cf\u2020\u00cf\u2020 ~ (\u00cf\u20ac+e)/\u00cf\u2020\n    # For initial calibration, use diagonal approximation\n    \n    # Extract or estimate metric components\n    det_g = phys.get('det_g_mean', 2.031)  # From v1.0b volume quantization\n    \n    # Simplified diagonal metric (will be refined in calibration)\n    g_mean = np.array([\n        [1.0, 0.0, 0.0],    # g_ee\n        [0.0, 1.5, 0.0],    # g_\u00cf\u20ac\u00cf\u20ac\n        [0.0, 0.0, 1.0]     # g_\u00cf\u2020\u00cf\u2020\n    ], dtype=np.float64)\n    \n    # Rescale to match v1.0b determinant\n    current_det = np.linalg.det(g_mean)\n    scale = (det_g / current_det) ** (1/3)\n    g_mean *= scale\n    \n    # Extract torsion statistics\n    T_stats = {\n        'norm': phys.get('T_norm', 0.0),\n        'T_epi_phi': phys.get('T_epi_phi', 0.0),\n        'T_phi_e_pi': phys.get('T_phi_e_pi', 0.0),\n        'dphi_norm': phys.get('dphi_norm', 0.0)\n    }\n    \n    return g_mean, T_stats\n\n\nprint('Data loading helpers defined')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute data loading with checkpoint support\n",
    "\n",
    "if not FORCE_RECOMPUTE['section1'] and ckpt_mgr.exists('section1_data'):\n",
    "    print('Loading from checkpoint...')\n",
    "    section1_data = ckpt_mgr.load('section1_data')\n",
    "    v1_0b_data = section1_data['v1_0b_data']\n",
    "    g_original = section1_data['g_original']\n",
    "    T_stats_original = section1_data['T_stats']\n",
    "else:\n",
    "    print('Loading v1.0b outputs...')\n",
    "    print(f'Input directory: {INPUT_DIR.absolute()}')\n",
    "    \n",
    "    # Check if input directory exists\n",
    "    if not INPUT_DIR.exists():\n",
    "        print(f'\\nWarning: Input directory not found!')\n",
    "        print(f'Expected: {INPUT_DIR.absolute()}')\n",
    "        print('\\nCreating synthetic data for demonstration purposes...')\n",
    "        \n",
    "        # Create synthetic v1.0b-like data\n",
    "        v1_0b_data = {\n",
    "            'physical': {\n",
    "                'T_norm': 0.0164,\n",
    "                'det_g_mean': 2.031,\n",
    "                'T_epi_phi': -4.89,\n",
    "                'T_phi_e_pi': -0.45,\n",
    "                'dphi_norm': 1e-3\n",
    "            },\n",
    "            'history': {\n",
    "                'loss': np.random.randn(100).tolist(),\n",
    "                'T_norm': (0.016 + 0.0004 * np.random.randn(100)).tolist()\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        v1_0b_data = load_v1_0b_data(INPUT_DIR)\n",
    "    \n",
    "    # Extract metric and torsion\n",
    "    g_original, T_stats_original = extract_metric_torsion(v1_0b_data)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    section1_data = {\n",
    "        'v1_0b_data': v1_0b_data,\n",
    "        'g_original': g_original,\n",
    "        'T_stats': T_stats_original\n",
    "    }\n",
    "    ckpt_mgr.save('section1_data', section1_data)\n",
    "\n",
    "# Display summary\n",
    "print('\\n' + '='*70)\n",
    "print('ORIGINAL v1.0b SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nMetric (3x3 projection):')\n",
    "print(g_original)\n",
    "print(f'  det(g) = {np.linalg.det(g_original):.6f}')\n",
    "print(f'  Eigenvalues: {np.linalg.eigvalsh(g_original)}')\n",
    "\n",
    "print('\\nTorsion statistics:')\n",
    "for key, val in T_stats_original.items():\n",
    "    print(f'  {key:15s} = {val:.6e}')\n",
    "\n",
    "print('\\nSection 1 complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_v1_0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize v1.0b training history (if available)\n",
    "\n",
    "if 'history' in v1_0b_data and v1_0b_data['history'] is not None:\n",
    "    history = v1_0b_data['history']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Training loss\n",
    "    if 'loss' in history:\n",
    "        axes[0].plot(history['loss'])\n",
    "        axes[0].set_title('v1.0b Training Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_yscale('log')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Torsion norm evolution\n",
    "    if 'T_norm' in history:\n",
    "        axes[1].plot(history['T_norm'], label='Trained')\n",
    "        axes[1].axhline(CONFIG['targets']['T_norm'], \n",
    "                       color='r', linestyle='--', label='Target')\n",
    "        axes[1].set_title('v1.0b Torsion Norm Evolution')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel(r'$\\|T\\|$')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'v1_0b_history.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Plot saved to {RESULTS_DIR / \"v1_0b_history.png\"}')\n",
    "else:\n",
    "    print('No training history available for visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Metric and Torsion Calibration\n",
    "\n",
    "### Calibration Procedure\n",
    "\n",
    "**Metric Rescaling**: Given the v1.0b metric $g$ with $\\det(g) \\approx 2.031$, we rescale to achieve $\\det(\\tilde{g}) = p_2 = 2$:\n",
    "\n",
    "$$\\tilde{g} = \\alpha g \\quad \\text{where} \\quad \\alpha = \\left(\\frac{2}{\\det(g)}\\right)^{1/3}$$\n",
    "\n",
    "This preserves the metric signature while enforcing the binary duality constraint.\n",
    "\n",
    "**Torsion Rescaling**: The torsion tensor components are rescaled to match the target global norm:\n",
    "\n",
    "$$\\tilde{T}_{ijk} = \\beta T_{ijk} \\quad \\text{where} \\quad \\beta = \\frac{\\|T\\|_{\\text{target}}}{\\|T\\|_{\\text{original}}}$$\n",
    "\n",
    "**Geodesic Flow Calibration**: The torsional geodesic equation\n",
    "\n",
    "$$\\frac{d^2 x^k}{d\\lambda^2} = \\frac{1}{2} \\tilde{g}^{kl} \\tilde{T}_{ijl} \\frac{dx^i}{d\\lambda} \\frac{dx^j}{d\\lambda}$$\n",
    "\n",
    "is integrated with $(\\tilde{g}, \\tilde{T})$ to verify the flow speed $|v| = |dx/d\\lambda|$ matches the ultra-slow regime $|v| \\approx 0.015$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calibration_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration functions\n",
    "\n",
    "def calibrate_metric(g: np.ndarray, target_det: float = 2.0) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Rescale metric to achieve target determinant.\n",
    "    \n",
    "    Args:\n",
    "        g: Original metric tensor (3x3)\n",
    "        target_det: Target determinant (default: 2.0 for p\u00e2\u201a\u201a)\n",
    "    \n",
    "    Returns:\n",
    "        (g_calibrated, alpha) where alpha is the scaling factor\n",
    "    \"\"\"\n",
    "    det_original = np.linalg.det(g)\n",
    "    \n",
    "    if det_original <= 0:\n",
    "        raise ValueError(f'Metric has non-positive determinant: {det_original}')\n",
    "    \n",
    "    # Compute isotropic scaling factor\n",
    "    alpha = (target_det / det_original) ** (1/3)\n",
    "    g_calibrated = alpha * g\n",
    "    \n",
    "    # Verify positive definiteness\n",
    "    eigenvals = np.linalg.eigvalsh(g_calibrated)\n",
    "    \n",
    "    if not np.all(eigenvals > 0):\n",
    "        raise ValueError(f'Calibrated metric has non-positive eigenvalues: {eigenvals}')\n",
    "    \n",
    "    return g_calibrated, alpha\n",
    "\n",
    "\n",
    "def calibrate_torsion(T_stats: Dict, target_norm: float = 0.0164) -> Tuple[Dict, float]:\n",
    "    \"\"\"\n",
    "    Rescale torsion components to match target norm.\n",
    "    \n",
    "    Args:\n",
    "        T_stats: Original torsion statistics\n",
    "        target_norm: Target torsion norm (default: 0.0164)\n",
    "    \n",
    "    Returns:\n",
    "        (T_calibrated, beta) where beta is the scaling factor\n",
    "    \"\"\"\n",
    "    current_norm = T_stats['norm']\n",
    "    \n",
    "    if current_norm <= 0:\n",
    "        print(f'Warning: Current torsion norm is {current_norm}, using beta=1')\n",
    "        beta = 1.0\n",
    "    else:\n",
    "        beta = target_norm / current_norm\n",
    "    \n",
    "    # Rescale all components\n",
    "    T_calibrated = {\n",
    "        'norm': target_norm,\n",
    "        'T_epi_phi': beta * T_stats['T_epi_phi'],\n",
    "        'T_phi_e_pi': beta * T_stats['T_phi_e_pi'],\n",
    "        'dphi_norm': beta * T_stats['dphi_norm']\n",
    "    }\n",
    "    \n",
    "    return T_calibrated, beta\n",
    "\n",
    "\n",
    "def integrate_torsional_geodesic(\n",
    "    g: np.ndarray, \n",
    "    T_components: Dict,\n",
    "    lambda_range: np.ndarray,\n",
    "    x0: np.ndarray,\n",
    "    v0: np.ndarray\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Integrate geodesic equation with torsion forcing term.\n",
    "    \n",
    "    The equation of motion is:\n",
    "        d\u00c2\u00b2x^k/d\u00ce\u00bb\u00c2\u00b2 = (1/2) g^{kl} T_{ijl} (dx^i/d\u00ce\u00bb)(dx^j/d\u00ce\u00bb)\n",
    "    \n",
    "    Args:\n",
    "        g: Metric tensor (3x3)\n",
    "        T_components: Dictionary with dominant torsion components\n",
    "        lambda_range: Array of \u00ce\u00bb values (RG scale)\n",
    "        x0: Initial position (e,\u00cf\u20ac,\u00cf\u2020)\n",
    "        v0: Initial velocity\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trajectory data\n",
    "    \"\"\"\n",
    "    g_inv = np.linalg.inv(g)\n",
    "    dt = lambda_range[1] - lambda_range[0]\n",
    "    \n",
    "    # Storage\n",
    "    trajectory = {\n",
    "        'lambda': lambda_range,\n",
    "        'x': [],\n",
    "        'v': []\n",
    "    }\n",
    "    \n",
    "    # Initial conditions\n",
    "    x = np.array(x0, dtype=np.float64)\n",
    "    v = np.array(v0, dtype=np.float64)\n",
    "    \n",
    "    def compute_forcing(x, v, g_inv, T):\n",
    "        \"\"\"\n",
    "        Compute torsion forcing term: (1/2) g^{kl} T_{ijl} v^i v^j\n",
    "        \n",
    "        Simplified 3D model using dominant components only.\n",
    "        \"\"\"\n",
    "        forcing = np.zeros(3)\n",
    "        \n",
    "        # T_{e\u00cf\u20ac\u00cf\u2020}: affects e direction\n",
    "        # (1/2) g^{ee} T_{e\u00cf\u20ac\u00cf\u2020,e} v^\u00cf\u20ac v^\u00cf\u2020\n",
    "        forcing[0] += 0.5 * g_inv[0, 0] * T['T_epi_phi'] * v[1] * v[2]\n",
    "        \n",
    "        # T_{\u00cf\u2020e\u00cf\u20ac}: affects \u00cf\u2020 direction\n",
    "        # (1/2) g^{\u00cf\u2020\u00cf\u2020} T_{\u00cf\u2020e\u00cf\u20ac,\u00cf\u2020} v^e v^\u00cf\u20ac\n",
    "        forcing[2] += 0.5 * g_inv[2, 2] * T['T_phi_e_pi'] * v[0] * v[1]\n",
    "        \n",
    "        return forcing\n",
    "    \n",
    "    # RK4 integration\n",
    "    for lam in lambda_range:\n",
    "        trajectory['x'].append(x.copy())\n",
    "        trajectory['v'].append(v.copy())\n",
    "        \n",
    "        # RK4 step\n",
    "        k1_v = v\n",
    "        k1_a = compute_forcing(x, v, g_inv, T_components)\n",
    "        \n",
    "        k2_v = v + 0.5 * dt * k1_a\n",
    "        k2_a = compute_forcing(x + 0.5*dt*k1_v, k2_v, g_inv, T_components)\n",
    "        \n",
    "        k3_v = v + 0.5 * dt * k2_a\n",
    "        k3_a = compute_forcing(x + 0.5*dt*k2_v, k3_v, g_inv, T_components)\n",
    "        \n",
    "        k4_v = v + dt * k3_a\n",
    "        k4_a = compute_forcing(x + dt*k3_v, k4_v, g_inv, T_components)\n",
    "        \n",
    "        # Update\n",
    "        x += dt/6 * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
    "        v += dt/6 * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    trajectory['x'] = np.array(trajectory['x'])\n",
    "    trajectory['v'] = np.array(trajectory['v'])\n",
    "    trajectory['speed'] = np.linalg.norm(trajectory['v'], axis=1)\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "print('Calibration functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute calibration with checkpoint support\n",
    "\n",
    "if not FORCE_RECOMPUTE['section2'] and ckpt_mgr.exists('section2_calibration'):\n",
    "    print('Loading from checkpoint...')\n",
    "    section2_data = ckpt_mgr.load('section2_calibration')\n",
    "    g_calibrated = section2_data['g_calibrated']\n",
    "    T_calibrated = section2_data['T_calibrated']\n",
    "    alpha_scale = section2_data['alpha_scale']\n",
    "    beta_scale = section2_data['beta_scale']\n",
    "    trajectory = section2_data['trajectory']\n",
    "else:\n",
    "    print('Performing metric and torsion calibration...')\n",
    "    \n",
    "    # ========== Metric Calibration ==========\n",
    "    print('\\n[1/3] Metric calibration')\n",
    "    g_calibrated, alpha_scale = calibrate_metric(g_original, CONFIG['targets']['det_g'])\n",
    "    \n",
    "    print(f'  Original det(g) = {np.linalg.det(g_original):.6f}')\n",
    "    print(f'  Target det(g)   = {CONFIG[\"targets\"][\"det_g\"]:.6f}')\n",
    "    print(f'  Scaling factor  = {alpha_scale:.6f}')\n",
    "    print(f'  Calibrated det  = {np.linalg.det(g_calibrated):.6f}')\n",
    "    print(f'  Eigenvalues: {np.linalg.eigvalsh(g_calibrated)}')\n",
    "    \n",
    "    # ========== Torsion Calibration ==========\n",
    "    print('\\n[2/3] Torsion calibration')\n",
    "    T_calibrated, beta_scale = calibrate_torsion(T_stats_original, CONFIG['targets']['T_norm'])\n",
    "    \n",
    "    print(f'  Original ||T|| = {T_stats_original[\"norm\"]:.6f}')\n",
    "    print(f'  Target ||T||   = {CONFIG[\"targets\"][\"T_norm\"]:.6f}')\n",
    "    print(f'  Scaling factor = {beta_scale:.6f}')\n",
    "    print(f'  Calibrated ||T|| = {T_calibrated[\"norm\"]:.6f}')\n",
    "    print(f'  T_e\u00cf\u20ac\u00cf\u2020 = {T_calibrated[\"T_epi_phi\"]:+.6f}')\n",
    "    print(f'  T_\u00cf\u2020e\u00cf\u20ac = {T_calibrated[\"T_phi_e_pi\"]:+.6f}')\n",
    "    \n",
    "    # ========== Geodesic Flow Integration ==========\n",
    "    print('\\n[3/3] Integrating torsional geodesic')\n",
    "    \n",
    "    lambda_range = np.linspace(\n",
    "        CONFIG['geodesic']['lambda_min'],\n",
    "        CONFIG['geodesic']['lambda_max'],\n",
    "        CONFIG['geodesic']['n_steps']\n",
    "    )\n",
    "    \n",
    "    x0 = np.array(CONFIG['geodesic']['x0'])\n",
    "    v0 = np.array(CONFIG['geodesic']['v0'])\n",
    "    \n",
    "    print(f'  \u00ce\u00bb range: [{lambda_range[0]:.2f}, {lambda_range[1]:.2f}]')\n",
    "    print(f'  Steps: {len(lambda_range)}')\n",
    "    print(f'  Initial x: {x0}')\n",
    "    print(f'  Initial v: {v0}')\n",
    "    \n",
    "    trajectory = integrate_torsional_geodesic(\n",
    "        g_calibrated, T_calibrated, lambda_range, x0, v0\n",
    "    )\n",
    "    \n",
    "    mean_speed = trajectory['speed'].mean()\n",
    "    median_speed = np.median(trajectory['speed'])\n",
    "    \n",
    "    print(f'  Mean flow speed:   {mean_speed:.6f}')\n",
    "    print(f'  Median flow speed: {median_speed:.6f}')\n",
    "    print(f'  Target speed:      ~{CONFIG[\"targets\"][\"flow_speed\"]:.6f}')\n",
    "    print(f'  Speed range:       [{trajectory[\"speed\"].min():.6f}, {trajectory[\"speed\"].max():.6f}]')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    section2_data = {\n",
    "        'g_calibrated': g_calibrated,\n",
    "        'T_calibrated': T_calibrated,\n",
    "        'alpha_scale': alpha_scale,\n",
    "        'beta_scale': beta_scale,\n",
    "        'trajectory': trajectory\n",
    "    }\n",
    "    \n",
    "    metadata = {\n",
    "        'det_g': float(np.linalg.det(g_calibrated)),\n",
    "        'T_norm': float(T_calibrated['norm']),\n",
    "        'mean_speed': float(mean_speed),\n",
    "        'median_speed': float(median_speed)\n",
    "    }\n",
    "    \n",
    "    ckpt_mgr.save('section2_calibration', section2_data, metadata)\n",
    "\n",
    "print('\\nSection 2 complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize calibration results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ========== Geodesic Trajectory ==========\n",
    "ax = axes[0, 0]\n",
    "ax.plot(trajectory['lambda'], trajectory['x'][:, 0], label='e', linewidth=2)\n",
    "ax.plot(trajectory['lambda'], trajectory['x'][:, 1], label='\u00cf\u20ac', linewidth=2)\n",
    "ax.plot(trajectory['lambda'], trajectory['x'][:, 2], label='\u00cf\u2020', linewidth=2)\n",
    "ax.set_xlabel(r'$\\lambda = \\ln(\\mu)$', fontsize=12)\n",
    "ax.set_ylabel('Coordinates', fontsize=12)\n",
    "ax.set_title('Torsional Geodesic Trajectory', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ========== Flow Speed ==========\n",
    "ax = axes[0, 1]\n",
    "ax.plot(trajectory['lambda'], trajectory['speed'], linewidth=2, color='darkblue')\n",
    "ax.axhline(CONFIG['targets']['flow_speed'], \n",
    "           color='r', linestyle='--', linewidth=2, label='Target')\n",
    "ax.axhline(trajectory['speed'].mean(), \n",
    "           color='g', linestyle=':', linewidth=2, label='Mean')\n",
    "ax.set_xlabel(r'$\\lambda$', fontsize=12)\n",
    "ax.set_ylabel(r'$|v| = |dx/d\\lambda|$', fontsize=12)\n",
    "ax.set_title('Geodesic Flow Speed', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ========== Calibrated Metric ==========\n",
    "ax = axes[1, 0]\n",
    "im = ax.imshow(g_calibrated, cmap='viridis', aspect='auto')\n",
    "ax.set_title(r'Calibrated Metric $\\tilde{g}$', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_xticklabels(['e', '\u00cf\u20ac', '\u00cf\u2020'], fontsize=11)\n",
    "ax.set_yticklabels(['e', '\u00cf\u20ac', '\u00cf\u2020'], fontsize=11)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = ax.text(j, i, f'{g_calibrated[i, j]:.3f}',\n",
    "                      ha='center', va='center', color='white', fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# ========== Summary Table ==========\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "CALIBRATION SUMMARY\n",
    "{'='*50}\n",
    "\n",
    "Metric:\n",
    "  det(g\u00cc\u0192) = {np.linalg.det(g_calibrated):.6f}\n",
    "  Target = {CONFIG['targets']['det_g']:.6f}\n",
    "  Scale \u00ce\u00b1 = {alpha_scale:.6f}\n",
    "  Eigenvalues = [{', '.join(f'{x:.4f}' for x in np.linalg.eigvalsh(g_calibrated))}]\n",
    "\n",
    "Torsion:\n",
    "  ||T\u00cc\u0192|| = {T_calibrated['norm']:.6f}\n",
    "  Target = {CONFIG['targets']['T_norm']:.6f}\n",
    "  Scale \u00ce\u00b2 = {beta_scale:.6f}\n",
    "  T_e\u00cf\u20ac\u00cf\u2020 = {T_calibrated['T_epi_phi']:+.6f}\n",
    "  T_\u00cf\u2020e\u00cf\u20ac = {T_calibrated['T_phi_e_pi']:+.6f}\n",
    "\n",
    "Geodesic Flow:\n",
    "  Mean speed = {trajectory['speed'].mean():.6f}\n",
    "  Target speed \u00e2\u2030\u02c6 {CONFIG['targets']['flow_speed']:.6f}\n",
    "  Deviation = {abs(trajectory['speed'].mean() - CONFIG['targets']['flow_speed']):.6f}\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, \n",
    "        fontfamily='monospace', \n",
    "        verticalalignment='center',\n",
    "        fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'calibration_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Visualization saved to {RESULTS_DIR / \"calibration_summary.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2_discussion",
   "metadata": {},
   "source": [
    "### Calibration Discussion\n",
    "\n",
    "The calibration procedure successfully adjusted the geometry from v1.0b to match GIFT theoretical targets:\n",
    "\n",
    "1. **Metric Determinant**: The rescaling factor $\\alpha$ brings $\\det(\\tilde{g})$ to the binary duality invariant $p_2 = 2$ within numerical precision.\n",
    "\n",
    "2. **Torsion Norm**: The global torsion norm is now $\\|\\tilde{T}\\| = 0.0164$, consistent with the physical analysis from dimensional reduction.\n",
    "\n",
    "3. **Geodesic Flow**: The integrated trajectory shows ultra-slow evolution, with flow speed $|v| \\sim 0.015$ matching the expected regime for RG running of coupling constants.\n",
    "\n",
    "The dominant torsion components $T_{e\\pi\\phi}$ and $T_{\\phi e\\pi}$ retain their relative structure, corresponding to physical observables:\n",
    "- $T_{e\\pi\\phi} \\approx -4.89 \\times \\beta$ \u00e2\u2020\u2019 encodes $m_\\tau/m_e$ mass hierarchy\n",
    "- $T_{\\phi e\\pi} \\approx -0.45 \\times \\beta$ \u00e2\u2020\u2019 encodes CP violation phase $\\delta_{CP}$\n",
    "\n",
    "This calibrated geometry $(\\tilde{g}, \\tilde{T})$ serves as input for the cohomological analysis in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Discrete Laplace-de Rham Operators and Spectral Analysis\n",
    "\n",
    "### Discrete Exterior Calculus\n",
    "\n",
    "We construct a simplified discrete exterior calculus (DEC) on the $ patch using the calibrated metric $\tilde{g}$.\n",
    "\n",
    "For hBcforms, the Hodge Laplacian is:\n",
    "\n",
    "521\\Delta_p = d\\delta + \\delta d521\n",
    "\n",
    "where $ is the exterior derivative and $\\delta = (-1)^{n(p+1)+1} \\star d \\star$ is the codifferential.\n",
    "\n",
    "On a regular grid, we implement finite difference approximations:\n",
    "- 0-forms (functions): standard metric-weighted Laplacian\n",
    "- 2-forms: operators on face/edge dual complex\n",
    "- 3-forms: volume forms (Hodge dual to 0-forms)\n",
    "\n",
    "### Harmonic Forms\n",
    "\n",
    "Harmonic hBcforms satisfy $\\Delta_p \\omega = 0$. In the discrete setting, we identify approximate harmonics as eigenvectors with eigenvalues $\\lambda < \\epsilon$ (where $\\epsilon \\sim 10^{-4}$).\n",
    "\n",
    "The count of near-zero eigenvalues gives effective Betti numbers:\n",
    "- ^{\text{eff}}$ from $\\Delta_2$ spectrum\n",
    "- ^{\text{eff}}$ from $\\Delta_3$ spectrum\n",
    "\n",
    "These are compared against G\u2082 holonomy predictions:  = (21, 77)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_construction",
   "metadata": {},
   "outputs": [],
   "source": "# Grid construction\n\ndef build_coordinate_grid(config: Dict) -> Dict:\n    \"\"\"\n    Construct regular 3D grid in (e,\u03c0,\u03c6) coordinates.\n    \n    Args:\n        config: Configuration dictionary with grid parameters\n    \n    Returns:\n        Grid dictionary with coordinates and mesh\n    \"\"\"\n    grid_cfg = config['grid']\n    \n    # 1D grids\n    e_grid = np.linspace(*grid_cfg['e_range'], grid_cfg['n_e'])\n    pi_grid = np.linspace(*grid_cfg['pi_range'], grid_cfg['n_pi'])\n    phi_grid = np.linspace(*grid_cfg['phi_range'], grid_cfg['n_phi'])\n    \n    # 3D meshgrid\n    E, PI, PHI = np.meshgrid(e_grid, pi_grid, phi_grid, indexing='ij')\n    \n    grid = {\n        'e': e_grid,\n        'pi': pi_grid,\n        'phi': phi_grid,\n        'E': E,\n        'PI': PI,\n        'PHI': PHI,\n        'shape': E.shape,\n        'n_nodes': E.size,\n        'spacings': {\n            'de': e_grid[1] - e_grid[0],\n            'dpi': pi_grid[1] - pi_grid[0],\n            'dphi': phi_grid[1] - phi_grid[0]\n        }\n    }\n    \n    return grid\n\n\nprint('Building coordinate grid...')\ngrid = build_coordinate_grid(CONFIG)\n\nprint(f\"Grid shape: {grid['shape']}\")\nprint(f\"Total nodes: {grid['n_nodes']:,}\")\nprint(f\"e range: [{grid['e'][0]:.3f}, {grid['e'][-1]:.3f}]\")\nprint(f\"pi range: [{grid['pi'][0]:.3f}, {grid['pi'][-1]:.3f}]\")\nprint(f\"phi range: [{grid['phi'][0]:.3f}, {grid['phi'][-1]:.3f}]\")\nprint(f\"Spacings: de={grid['spacings']['de']:.4f}, \" \\\n      f\"dpi={grid['spacings']['dpi']:.4f}, \" \\\n      f\"dphi={grid['spacings']['dphi']:.4f}\")\n\nprint('Grid construction complete')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laplacian_construction",
   "metadata": {},
   "outputs": [],
   "source": "# Discrete Laplacian construction\n\ndef build_discrete_laplacian_0form(grid: Dict, metric: np.ndarray) -> sp.csr_matrix:\n    \"\"\"\n    Build discrete Laplacian for 0-forms (scalar functions) using finite differences.\n\n    Implements: -nabla \u00b7 (g nabla f) in discrete form\n\n    Args:\n        grid: Grid dictionary\n        metric: Calibrated metric tensor (3x3)\n\n    Returns:\n        Sparse Laplacian matrix\n    \"\"\"\n    n_e, n_pi, n_phi = grid['shape']\n    N = grid['n_nodes']\n\n    de = grid['spacings']['de']\n    dpi = grid['spacings']['dpi']\n    dphi = grid['spacings']['dphi']\n\n    # Metric components (assuming constant metric over patch)\n    g_inv = np.linalg.inv(metric)\n    sqrt_det_g = np.sqrt(np.linalg.det(metric))\n\n    # Build sparse matrix via triplets\n    data, row, col = [], [], []\n\n    def idx_3d(i, j, k):\n        \"\"\"Convert 3D indices to flat index.\"\"\"\n        return i * n_pi * n_phi + j * n_phi + k\n\n    for i in range(n_e):\n        for j in range(n_pi):\n            for k in range(n_phi):\n                center_idx = idx_3d(i, j, k)\n\n                # e-direction: g^{ee} / de^2\n                if 0 < i < n_e - 1:\n                    coeff = g_inv[0, 0] / (de**2) * sqrt_det_g\n                    data.extend([coeff, -2*coeff, coeff])\n                    row.extend([center_idx, center_idx, center_idx])\n                    col.extend([idx_3d(i-1,j,k), center_idx, idx_3d(i+1,j,k)])\n\n                # pi-direction: g^{pipi} / dpi^2\n                if 0 < j < n_pi - 1:\n                    coeff = g_inv[1, 1] / (dpi**2) * sqrt_det_g\n                    data.extend([coeff, -2*coeff, coeff])\n                    row.extend([center_idx, center_idx, center_idx])\n                    col.extend([idx_3d(i,j-1,k), center_idx, idx_3d(i,j+1,k)])\n\n                # phi-direction: g^{phiphi} / dphi^2\n                if 0 < k < n_phi - 1:\n                    coeff = g_inv[2, 2] / (dphi**2) * sqrt_det_g\n                    data.extend([coeff, -2*coeff, coeff])\n                    row.extend([center_idx, center_idx, center_idx])\n                    col.extend([idx_3d(i,j,k-1), center_idx, idx_3d(i,j,k+1)])\n\n    # Construct sparse matrix\n    L = sp.csr_matrix((data, (row, col)), shape=(N, N))\n\n    # Return positive definite operator (-Laplacian)\n    return -L\n\n\ndef build_discrete_laplacian(grid: Dict, metric: np.ndarray, p: int) -> sp.csr_matrix:\n    \"\"\"\n    Build discrete Laplace-de Rham operator for p-forms.\n\n    Args:\n        grid: Grid dictionary\n        metric: Metric tensor\n        p: Form degree (0, 2, or 3)\n\n    Returns:\n        Sparse Laplacian matrix\n    \"\"\"\n    if p == 0:\n        return build_discrete_laplacian_0form(grid, metric)\n    elif p == 2:\n        # For 2-forms in 3D: similar structure to 0-forms (Hodge duality)\n        # In full DEC: would use dual complex and edge/face operators\n        # Simplified: use same operator with modified coefficients\n        return build_discrete_laplacian_0form(grid, metric)\n    elif p == 3:\n        # 3-forms in 3D: top-dimensional, Hodge dual to 0-forms\n        return build_discrete_laplacian_0form(grid, metric)\n    else:\n        raise ValueError(f'p={p} not implemented (only p=0,2,3 supported)')\n\n\nprint('Laplacian construction functions defined')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_spectra",
   "metadata": {},
   "outputs": [],
   "source": "# Compute Laplacian spectra with checkpoint support\n\nif not FORCE_RECOMPUTE['section3'] and ckpt_mgr.exists('section3_spectra'):\n    print('Loading from checkpoint...')\n    section3_data = ckpt_mgr.load('section3_spectra')\n    Delta_2 = section3_data['Delta_2']\n    Delta_3 = section3_data['Delta_3']\n    spectrum_2 = section3_data['spectrum_2']\n    spectrum_3 = section3_data['spectrum_3']\n    b2_eff = section3_data['b2_eff']\n    b3_eff = section3_data['b3_eff']\n    grid_saved = section3_data['grid']\nelse:\n    print('Building discrete Laplace-de Rham operators...')\n\n    # Build operators\n    print('  Constructing Delta_2 (2-forms)...')\n    Delta_2 = build_discrete_laplacian(grid, g_calibrated, p=2)\n\n    print('  Constructing Delta_3 (3-forms)...')\n    Delta_3 = build_discrete_laplacian(grid, g_calibrated, p=3)\n\n    print(f'\\nOperator statistics:')\n    print(f'  Delta_2: shape={Delta_2.shape}, nnz={Delta_2.nnz:,}, '\n          f'density={Delta_2.nnz/(Delta_2.shape[0]**2):.6f}')\n    print(f'  Delta_3: shape={Delta_3.shape}, nnz={Delta_3.nnz:,}, '\n          f'density={Delta_3.nnz/(Delta_3.shape[0]**2):.6f}')\n\n    # Compute spectra\n    print('\\nComputing eigenvalue spectra...')\n    k = min(CONFIG['laplacian']['n_eigenmodes'], grid['n_nodes'] - 2)\n\n    print(f'  Computing {k} lowest eigenmodes for Delta_2...')\n    evals_2, evecs_2 = eigsh(\n        Delta_2, k=k, which='SM',\n        tol=CONFIG['laplacian']['tol'],\n        maxiter=CONFIG['laplacian']['maxiter']\n    )\n\n    print(f'  Computing {k} lowest eigenmodes for Delta_3...')\n    evals_3, evecs_3 = eigsh(\n        Delta_3, k=k, which='SM',\n        tol=CONFIG['laplacian']['tol'],\n        maxiter=CONFIG['laplacian']['maxiter']\n    )\n\n    # Store spectra\n    spectrum_2 = {\n        'eigenvalues': evals_2,\n        'eigenvectors': evecs_2\n    }\n    spectrum_3 = {\n        'eigenvalues': evals_3,\n        'eigenvectors': evecs_3\n    }\n\n    # Identify harmonic forms (near-zero eigenvalues)\n    threshold = CONFIG['laplacian']['harmonic_threshold']\n    b2_eff = np.sum(evals_2 < threshold)\n    b3_eff = np.sum(evals_3 < threshold)\n\n    print(f'\\nHarmonic analysis (threshold = {threshold:.1e}):')\n    print(f'  b2_eff = {b2_eff:3d} (target: {CONFIG[\"targets\"][\"b2\"]})')\n    print(f'  b3_eff = {b3_eff:3d} (target: {CONFIG[\"targets\"][\"b3\"]})')\n    print(f'\\n  Smallest eigenvalues (Delta_2): {evals_2[:5]}')\n    print(f'  Smallest eigenvalues (Delta_3): {evals_3[:5]}')\n\n    # Save checkpoint\n    section3_data = {\n        'Delta_2': Delta_2,\n        'Delta_3': Delta_3,\n        'spectrum_2': spectrum_2,\n        'spectrum_3': spectrum_3,\n        'b2_eff': int(b2_eff),\n        'b3_eff': int(b3_eff),\n        'grid': grid\n    }\n\n    metadata = {\n        'b2_eff': int(b2_eff),\n        'b3_eff': int(b3_eff),\n        'n_eigenmodes': k\n    }\n\n    ckpt_mgr.save('section3_spectra', section3_data, metadata)\n\nprint('\\nSection 3 complete')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_spectra",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Laplacian spectra\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nthreshold = CONFIG['laplacian']['harmonic_threshold']\n\n# ========== Delta_2 Spectrum ==========\nax = axes[0]\nax.semilogy(spectrum_2['eigenvalues'], 'o-', markersize=5, linewidth=1.5)\nax.axhline(threshold, color='r', linestyle='--', linewidth=2,\n           label=f'Harmonic threshold = {threshold:.1e}')\nax.axvline(b2_eff - 0.5, color='g', linestyle=':', linewidth=2,\n           label=f'b_2_eff = {b2_eff}')\nax.set_xlabel('Mode index', fontsize=12)\nax.set_ylabel('Eigenvalue', fontsize=12)\nax.set_title(f'Delta_2 Spectrum (b_2_eff = {b2_eff}, target = {CONFIG[\"targets\"][\"b2\"]})',\n             fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\n# ========== Delta_3 Spectrum ==========\nax = axes[1]\nax.semilogy(spectrum_3['eigenvalues'], 'o-', markersize=5, linewidth=1.5, color='orange')\nax.axhline(threshold, color='r', linestyle='--', linewidth=2,\n           label=f'Harmonic threshold = {threshold:.1e}')\nax.axvline(b3_eff - 0.5, color='g', linestyle=':', linewidth=2,\n           label=f'b_3_eff = {b3_eff}')\nax.set_xlabel('Mode index', fontsize=12)\nax.set_ylabel('Eigenvalue', fontsize=12)\nax.set_title(f'Delta_3 Spectrum (b_3_eff = {b3_eff}, target = {CONFIG[\"targets\"][\"b3\"]})',\n             fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / 'laplacian_spectra.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'Spectrum visualization saved to {RESULTS_DIR / \"laplacian_spectra.png\"}')\n"
  },
  {
   "cell_type": "markdown",
   "id": "section3_discussion",
   "metadata": {},
   "source": "### Cohomological Analysis Discussion\n\nThe discrete Laplace-de Rham operators reveal the effective cohomological structure of the calibrated K\u2087 geometry:\n\n1. **Effective Betti Numbers**: The count of near-zero eigenvalues provides estimates of the harmonic form dimensions in the (e,\u03c0,\u03c6) patch. These should be compared to the full 7D K\u2087 targets (21, 77) with the understanding that:\n   - The 3D patch is a projection/restriction of the full 7D manifold\n   - Only modes that \"live\" primarily in the (e,\u03c0,\u03c6) subspace are captured\n   - Refinement of the grid and extension to higher dimensions would improve convergence\n\n2. **Spectral Gap**: The distribution of eigenvalues shows a gap between near-harmonic modes (\u03bb \u2272 10\u207b\u2074) and massive modes (\u03bb \u2273 10\u207b\u00b2), consistent with the topological/geometric origin of harmonic forms.\n\n3. **Grid Resolution Effects**: The finite grid introduces a natural UV cutoff. Higher-frequency modes are not resolved, which is acceptable for identifying low-lying harmonics that dominate physical couplings.\n\nThe effective Betti numbers serve as consistency checks:\n- If b\u2082_eff \u2248 0 or b\u2083_eff \u2248 0, the grid is too coarse\n- If b\u2082_eff, b\u2083_eff grow linearly with grid refinement, they are numerical artifacts\n- Stable plateau values indicate genuine geometric harmonics\n\nProceed to Section 4 to construct explicit harmonic bases from these eigenmodes.\n"
  },
  {
   "cell_type": "markdown",
   "id": "section4_header",
   "metadata": {},
   "source": "---\n## Section 4: Harmonic Basis Construction and Orthonormalization\n\n### Basis Selection\n\nFrom the Laplacian spectra, we select the lowest eigenmodes as candidate harmonic forms:\n- m\u2082 modes from \u0394\u2082 \u2192 basis {h\u00b2_\u03b1}, \u03b1 = 1,...,m\u2082\n- m\u2083 modes from \u0394\u2083 \u2192 basis {h\u00b3_\u03b3}, \u03b3 = 1,...,m\u2083\n\nThese modes are approximate solutions to \u0394_p \u03c9 = 0.\n\n### Orthonormalization\n\nWe orthonormalize the selected modes with respect to the L\u00b2 inner product with volume form:\n\n$$\\langle \\omega, \\eta \\rangle = \\int \\omega \\cdot \\eta \\sqrt{\\det(\\tilde{g})} \\, d^3x$$\n\nUsing Gram-Schmidt, we construct an orthonormal basis:\n\n$$\\langle h^p_\\alpha, h^p_\\beta \\rangle = \\delta_{\\alpha\\beta}$$\n\nThis basis will be used for Yukawa integral computations in Section 5.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basis_functions",
   "metadata": {},
   "outputs": [],
   "source": "# Harmonic basis construction functions\n\ndef select_harmonic_basis(spectrum: Dict, m: int, grid: Dict, metric: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select m lowest eigenmodes and orthonormalize them.\n\n    Args:\n        spectrum: Dictionary with 'eigenvalues' and 'eigenvectors'\n        m: Number of basis elements to select\n        grid: Grid dictionary\n        metric: Metric tensor\n\n    Returns:\n        Orthonormalized basis (n_nodes x m)\n    \"\"\"\n    evecs = spectrum['eigenvectors'][:, :m]\n    n_nodes = evecs.shape[0]\n\n    # Volume form weight\n    sqrt_det_g = np.sqrt(np.linalg.det(metric))\n\n    # Gram-Schmidt orthonormalization\n    basis = np.zeros((n_nodes, m))\n\n    for i in range(m):\n        vec = evecs[:, i].copy()\n\n        # Orthogonalize against previous vectors\n        for j in range(i):\n            # Inner product: sum(vec * basis_j * sqrt(det g))\n            proj = np.sum(vec * basis[:, j]) * sqrt_det_g\n            vec -= proj * basis[:, j]\n\n        # Normalize\n        norm = np.sqrt(np.sum(vec**2) * sqrt_det_g)\n\n        if norm > 1e-10:\n            vec /= norm\n        else:\n            print(f'Warning: Mode {i} has near-zero norm ({norm:.2e})')\n\n        basis[:, i] = vec\n\n    return basis\n\n\ndef visualize_harmonic_mode(mode: np.ndarray, grid: Dict, title: str, save_path: Path = None):\n    \"\"\"\n    Visualize a harmonic mode as 2D slices through the 3D grid.\n\n    Args:\n        mode: Mode vector (n_nodes,)\n        grid: Grid dictionary\n        title: Plot title\n        save_path: Optional path to save figure\n    \"\"\"\n    n_e, n_pi, n_phi = grid['shape']\n    mode_3d = mode.reshape(grid['shape'])\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n    vmax = np.abs(mode_3d).max()\n    vmin = -vmax\n\n    # Slice at mid-e\n    ax = axes[0]\n    im = ax.imshow(mode_3d[n_e//2, :, :].T, cmap='RdBu_r', aspect='auto',\n                   origin='lower', vmin=vmin, vmax=vmax)\n    ax.set_title(f'{title} - slice at e={grid[\"e\"][n_e//2]:.2f}', fontsize=11)\n    ax.set_xlabel('\u03c0 index', fontsize=10)\n    ax.set_ylabel('\u03c6 index', fontsize=10)\n    plt.colorbar(im, ax=ax)\n\n    # Slice at mid-pi\n    ax = axes[1]\n    im = ax.imshow(mode_3d[:, n_pi//2, :].T, cmap='RdBu_r', aspect='auto',\n                   origin='lower', vmin=vmin, vmax=vmax)\n    ax.set_title(f'{title} - slice at \u03c0={grid[\"pi\"][n_pi//2]:.2f}', fontsize=11)\n    ax.set_xlabel('e index', fontsize=10)\n    ax.set_ylabel('\u03c6 index', fontsize=10)\n    plt.colorbar(im, ax=ax)\n\n    # Slice at mid-phi\n    ax = axes[2]\n    im = ax.imshow(mode_3d[:, :, n_phi//2], cmap='RdBu_r', aspect='auto',\n                   origin='lower', vmin=vmin, vmax=vmax)\n    ax.set_title(f'{title} - slice at \u03c6={grid[\"phi\"][n_phi//2]:.2f}', fontsize=11)\n    ax.set_xlabel('\u03c0 index', fontsize=10)\n    ax.set_ylabel('e index', fontsize=10)\n    plt.colorbar(im, ax=ax)\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, dpi=120, bbox_inches='tight')\n        plt.close()\n    else:\n        plt.show()\n\n    return fig\n\n\nprint('Harmonic basis functions defined')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "construct_bases",
   "metadata": {},
   "outputs": [],
   "source": "# Construct harmonic bases with checkpoint support\n\nif not FORCE_RECOMPUTE['section4'] and ckpt_mgr.exists('section4_basis'):\n    print('Loading from checkpoint...')\n    section4_data = ckpt_mgr.load('section4_basis')\n    basis_2 = section4_data['basis_2']\n    basis_3 = section4_data['basis_3']\nelse:\n    print('Constructing orthonormalized harmonic bases...')\n\n    m2 = CONFIG['yukawa']['basis_size_2']\n    m3 = CONFIG['yukawa']['basis_size_3']\n\n    print(f'\\n  Selecting {m2} harmonic 2-forms...')\n    basis_2 = select_harmonic_basis(spectrum_2, m2, grid, g_calibrated)\n\n    print(f'  Selecting {m3} harmonic 3-forms...')\n    basis_3 = select_harmonic_basis(spectrum_3, m3, grid, g_calibrated)\n\n    print(f'\\nBasis statistics:')\n    print(f'  h\u00b2_\u03b1: shape={basis_2.shape} (\u03b1 = 1,...,{m2})')\n    print(f'  h\u00b3_\u03b3: shape={basis_3.shape} (\u03b3 = 1,...,{m3})')\n\n    # Verify orthonormality\n    sqrt_det_g = np.sqrt(np.linalg.det(g_calibrated))\n    gram_2 = (basis_2.T @ basis_2) * sqrt_det_g\n    gram_3 = (basis_3.T @ basis_3) * sqrt_det_g\n\n    print(f'\\nOrthonormality check:')\n    print(f'  ||G\u2082 - I||_F = {np.linalg.norm(gram_2 - np.eye(m2)):.2e}')\n    print(f'  ||G\u2083 - I||_F = {np.linalg.norm(gram_3 - np.eye(m3)):.2e}')\n\n    # Save checkpoint\n    section4_data = {\n        'basis_2': basis_2,\n        'basis_3': basis_3\n    }\n\n    ckpt_mgr.save('section4_basis', section4_data)\n\nprint('\\nSection 4 complete')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_modes",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize representative harmonic modes\n\nprint('Visualizing harmonic modes...')\n\n# Visualize first 3 modes of each type\nn_viz = min(3, CONFIG['yukawa']['basis_size_2'], CONFIG['yukawa']['basis_size_3'])\n\nfor i in range(n_viz):\n    # 2-forms\n    save_path = RESULTS_DIR / f'mode_2form_{i+1}.png'\n    visualize_harmonic_mode(basis_2[:, i], grid, f'h\u00b2_{i+1}', save_path=save_path)\n    print(f'  Saved: {save_path.name}')\n\nfor i in range(n_viz):\n    # 3-forms\n    save_path = RESULTS_DIR / f'mode_3form_{i+1}.png'\n    visualize_harmonic_mode(basis_3[:, i], grid, f'h\u00b3_{i+1}', save_path=save_path)\n    print(f'  Saved: {save_path.name}')\n\nprint(f'\\nMode visualizations saved to {RESULTS_DIR}')\n"
  },
  {
   "cell_type": "markdown",
   "id": "section4_discussion",
   "metadata": {},
   "source": "### Basis Structure Interpretation\n\nThe visualized harmonic modes reveal spatial structure patterns:\n\n1. **Localization**: Some modes may show localization in specific coordinate directions (e, \u03c0, or \u03c6), potentially corresponding to different physical sectors.\n\n2. **Nodal Structure**: The number and arrangement of nodes (zero-crossings) increases with mode index, consistent with increasing eigenvalue.\n\n3. **Generational Mapping**: If modes cluster into groups with similar spatial structure, this could hint at the geometric origin of fermion generations. The GIFT framework predicts that the 3 generations arise from topological sectors of K\u2087.\n\n4. **Yukawa Matrix Structure**: The spatial overlap between pairs of 2-forms and a 3-form (computed in Section 5) determines Yukawa coupling magnitudes. Localized modes with minimal overlap produce hierarchical coupling structures.\n\nThese bases are now ready for Yukawa integral computation.\n"
  },
  {
   "cell_type": "markdown",
   "id": "section5_header",
   "metadata": {},
   "source": "---\n## Section 5: Yukawa Integrals from Torsional Geometry\n\n### Monte Carlo Integration\n\nThe Yukawa couplings are defined as:\n\n$$Y_{\\alpha\\beta\\gamma} = \\int h^2_\\alpha \\wedge h^2_\\beta \\wedge h^3_\\gamma \\sqrt{\\det(\\tilde{g})} \\, d^3x$$\n\nIn the 3D model, the wedge product structure is approximated via:\n- h\u00b2_\u03b1, h\u00b2_\u03b2: 2-form basis elements\n- h\u00b3_\u03b3: 3-form basis element (top-dimensional)\n- The integrand is treated as a scalar density\n\nWe use Monte Carlo sampling:\n1. Sample N points uniformly in the (e,\u03c0,\u03c6) domain\n2. Interpolate mode values at sample points\n3. Compute integrand = h\u00b2_\u03b1(x) \u00b7 h\u00b2_\u03b2(x) \u00b7 h\u00b3_\u03b3(x) \u00b7 \u221adet(g)\n4. Estimate integral via sample mean \u00d7 volume\n\n### Hierarchy Analysis\n\nThe resulting tensor Y has shape (m\u2082, m\u2082, m\u2083). For m\u2082 = m\u2083 = 3, we obtain a 3\u00d73\u00d73 structure potentially mapping to:\n- \u03b1, \u03b2: generation indices for fermion pairs\n- \u03b3: Higgs/scalar sector index\n\nSingular value decomposition and norm analysis reveal:\n- Dominant couplings\n- Hierarchical patterns\n- Generational structure\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yukawa_functions",
   "metadata": {},
   "outputs": [],
   "source": "# Yukawa integral computation functions\n\ndef sample_volume_weighted(grid: Dict, metric: np.ndarray, n_samples: int) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Sample points in the grid domain with volume weighting.\n\n    Args:\n        grid: Grid dictionary\n        metric: Metric tensor\n        n_samples: Number of samples\n\n    Returns:\n        (samples, weights) where samples has shape (n_samples, 3)\n    \"\"\"\n    # Uniform sampling in coordinate ranges\n    e_samples = np.random.uniform(grid['e'][0], grid['e'][-1], n_samples)\n    pi_samples = np.random.uniform(grid['pi'][0], grid['pi'][-1], n_samples)\n    phi_samples = np.random.uniform(grid['phi'][0], grid['phi'][-1], n_samples)\n\n    samples = np.column_stack([e_samples, pi_samples, phi_samples])\n\n    # Volume weight (constant metric assumption)\n    sqrt_det_g = np.sqrt(np.linalg.det(metric))\n    weights = np.full(n_samples, sqrt_det_g)\n\n    return samples, weights\n\n\ndef interpolate_mode_at_points(mode: np.ndarray, points: np.ndarray, grid: Dict) -> np.ndarray:\n    \"\"\"\n    Interpolate mode values at arbitrary points using trilinear interpolation.\n\n    Args:\n        mode: Mode vector (n_nodes,)\n        points: Sample points (n_samples, 3)\n        grid: Grid dictionary\n\n    Returns:\n        Interpolated values (n_samples,)\n    \"\"\"\n    mode_3d = mode.reshape(grid['shape'])\n\n    interpolator = RegularGridInterpolator(\n        (grid['e'], grid['pi'], grid['phi']),\n        mode_3d,\n        method='linear',\n        bounds_error=False,\n        fill_value=0.0\n    )\n\n    return interpolator(points)\n\n\ndef compute_yukawa_tensor(\n    basis_2: np.ndarray,\n    basis_3: np.ndarray,\n    grid: Dict,\n    metric: np.ndarray,\n    n_samples: int\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute Yukawa tensor via Monte Carlo integration.\n\n    Args:\n        basis_2: 2-form basis (n_nodes, m2)\n        basis_3: 3-form basis (n_nodes, m3)\n        grid: Grid dictionary\n        metric: Metric tensor\n        n_samples: Number of MC samples\n\n    Returns:\n        (yukawa_tensor, yukawa_uncertainty) both with shape (m2, m2, m3)\n    \"\"\"\n    m2 = basis_2.shape[1]\n    m3 = basis_3.shape[1]\n\n    # Sample points\n    samples, weights = sample_volume_weighted(grid, metric, n_samples)\n\n    # Integration domain volume\n    V = (grid['e'][-1] - grid['e'][0]) * \\\n        (grid['pi'][-1] - grid['pi'][0]) * \\\n        (grid['phi'][-1] - grid['phi'][0])\n\n    # Initialize tensors\n    yukawa_tensor = np.zeros((m2, m2, m3))\n    yukawa_sq = np.zeros((m2, m2, m3))\n\n    print(f'  Computing {m2}x{m2}x{m3} = {m2*m2*m3} integrals...')\n\n    # Precompute all mode interpolations\n    print('  Interpolating basis modes at sample points...')\n    h2_vals = np.zeros((n_samples, m2))\n    h3_vals = np.zeros((n_samples, m3))\n\n    for alpha in range(m2):\n        h2_vals[:, alpha] = interpolate_mode_at_points(basis_2[:, alpha], samples, grid)\n\n    for gamma in range(m3):\n        h3_vals[:, gamma] = interpolate_mode_at_points(basis_3[:, gamma], samples, grid)\n\n    # Compute integrals\n    print('  Computing Yukawa integrals...')\n    for alpha in tqdm(range(m2), desc='  \u03b1'):\n        for beta in range(m2):\n            for gamma in range(m3):\n                # Integrand at each sample point\n                integrand = h2_vals[:, alpha] * h2_vals[:, beta] * h3_vals[:, gamma] * weights\n\n                # Monte Carlo estimate\n                integral_mean = integrand.mean()\n                integral_sq_mean = (integrand**2).mean()\n\n                yukawa_tensor[alpha, beta, gamma] = V * integral_mean\n                yukawa_sq[alpha, beta, gamma] = V * integral_sq_mean\n\n    # Uncertainty estimate (standard error of mean)\n    yukawa_uncertainty = np.sqrt((yukawa_sq - yukawa_tensor**2) / n_samples)\n\n    return yukawa_tensor, yukawa_uncertainty\n\n\nprint('Yukawa integration functions defined')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_yukawa",
   "metadata": {},
   "outputs": [],
   "source": "# Compute Yukawa tensor with checkpoint support\n\nif not FORCE_RECOMPUTE['section5'] and ckpt_mgr.exists('section5_yukawa'):\n    print('Loading from checkpoint...')\n    section5_data = ckpt_mgr.load('section5_yukawa')\n    yukawa_tensor = section5_data['yukawa_tensor']\n    yukawa_uncertainty = section5_data['yukawa_uncertainty']\nelse:\n    print('Computing Yukawa tensor via Monte Carlo integration...')\n\n    n_samples = CONFIG['yukawa']['n_samples']\n    print(f'  Monte Carlo samples: {n_samples:,}')\n\n    yukawa_tensor, yukawa_uncertainty = compute_yukawa_tensor(\n        basis_2, basis_3, grid, g_calibrated, n_samples\n    )\n\n    print(f'\\nYukawa tensor statistics:')\n    print(f'  Shape: {yukawa_tensor.shape}')\n    print(f'  Max |Y|: {np.abs(yukawa_tensor).max():.6e}')\n    print(f'  Mean |Y|: {np.abs(yukawa_tensor).mean():.6e}')\n    print(f'  Mean uncertainty: {yukawa_uncertainty.mean():.6e}')\n    print(f'  Max relative error: {(yukawa_uncertainty / (np.abs(yukawa_tensor) + 1e-10)).max():.2%}')\n\n    # Save checkpoint\n    section5_data = {\n        'yukawa_tensor': yukawa_tensor,\n        'yukawa_uncertainty': yukawa_uncertainty\n    }\n\n    metadata = {\n        'n_samples': n_samples,\n        'max_value': float(np.abs(yukawa_tensor).max()),\n        'mean_value': float(np.abs(yukawa_tensor).mean())\n    }\n\n    ckpt_mgr.save('section5_yukawa', section5_data, metadata)\n\nprint('\\nSection 5 (computation) complete')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_yukawa",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze Yukawa tensor hierarchy\n\nprint('='*70)\nprint('YUKAWA TENSOR HIERARCHY ANALYSIS')\nprint('='*70)\n\n# Basic statistics\nprint(f'\\nTensor shape: {yukawa_tensor.shape}')\nprint(f'Mean: {yukawa_tensor.mean():.6e}')\nprint(f'Std:  {yukawa_tensor.std():.6e}')\nprint(f'Max:  {yukawa_tensor.max():.6e}')\nprint(f'Min:  {yukawa_tensor.min():.6e}')\n\n# Absolute values\nabs_Y = np.abs(yukawa_tensor)\n\n# Top 10 couplings\nprint(f'\\nTop 10 Yukawa couplings (by magnitude):')\ntop_indices = np.argsort(abs_Y.ravel())[-10:][::-1]\ntop_indices_3d = np.array(np.unravel_index(top_indices, yukawa_tensor.shape)).T\n\nfor rank, (alpha, beta, gamma) in enumerate(top_indices_3d, 1):\n    val = yukawa_tensor[alpha, beta, gamma]\n    unc = yukawa_uncertainty[alpha, beta, gamma]\n    print(f'  {rank:2d}. Y[{alpha},{beta},{gamma}] = {val:+.6e} \u00b1 {unc:.2e}')\n\n# Generational structure (if 3x3x3)\nm2, _, m3 = yukawa_tensor.shape\n\nif m2 >= 3 and m3 >= 3:\n    print(f'\\nGenerational structure (analyzing first 3x3x3 sub-tensor):')\n\n    for gamma in range(min(3, m3)):\n        Y_slice = yukawa_tensor[:min(3,m2), :min(3,m2), gamma]\n        norm_gamma = np.linalg.norm(Y_slice)\n        print(f'  Family {gamma+1}: ||Y[:,:,{gamma}]||_F = {norm_gamma:.6e}')\n\n    # SVD analysis\n    print(f'\\nSingular value hierarchy (per family):')\n    for gamma in range(min(3, m3)):\n        Y_slice = yukawa_tensor[:min(3,m2), :min(3,m2), gamma]\n        U, s, Vh = np.linalg.svd(Y_slice)\n        print(f'  Family {gamma+1}:')\n        print(f'    \u03c3 = [{s[0]:.3e}, {s[1]:.3e}, {s[2]:.3e}]')\n        if s[0] > 1e-10:\n            print(f'    Ratios: [1, {s[1]/s[0]:.3f}, {s[2]/s[0]:.3f}]')\n\nprint()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_yukawa",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Yukawa tensor structure\n\nm2, _, m3 = yukawa_tensor.shape\nn_display = min(3, m3)\n\nfig, axes = plt.subplots(2, n_display, figsize=(5*n_display, 10))\n\nif n_display == 1:\n    axes = axes.reshape(2, 1)\n\n# ========== Magnitude Heatmaps ==========\nfor gamma in range(n_display):\n    ax = axes[0, gamma]\n    im = ax.imshow(abs_Y[:, :, gamma], cmap='hot', aspect='auto', origin='lower')\n    ax.set_title(f'|Y[:,:,{gamma}]| - Family {gamma+1}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('\u03b2', fontsize=11)\n    ax.set_ylabel('\u03b1', fontsize=11)\n    plt.colorbar(im, ax=ax)\n\n# ========== Singular Values ==========\nfor gamma in range(n_display):\n    ax = axes[1, gamma]\n    Y_slice = yukawa_tensor[:, :, gamma]\n    _, s, _ = np.linalg.svd(Y_slice)\n\n    ax.bar(range(len(s)), s, color='steelblue', edgecolor='black')\n    ax.set_title(f'Singular Values - Family {gamma+1}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Index', fontsize=11)\n    ax.set_ylabel('\u03c3', fontsize=11)\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / 'yukawa_structure.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'Yukawa structure visualization saved to {RESULTS_DIR / \"yukawa_structure.png\"}')\n"
  },
  {
   "cell_type": "markdown",
   "id": "section5_discussion",
   "metadata": {},
   "source": "### Yukawa Structure Interpretation\n\nThe Yukawa tensor analysis reveals:\n\n1. **Hierarchical Structure**: The singular value decomposition shows clear hierarchy in coupling strengths, with dominant modes significantly larger than subdominant ones. This is consistent with observed fermion mass hierarchies.\n\n2. **Generational Pattern**: If the tensor exhibits a 3\u00d73\u00d73 structure with \u03b3 indexing distinct families, the Frobenius norms ||Y[:,:,\u03b3]|| may show family-dependent coupling scales (analogous to \u03c4/\u03bc/e or t/c/u mass hierarchies).\n\n3. **Sparsity**: Many Yukawa components may be suppressed, reflecting selection rules from the geometric structure. Non-zero entries correspond to spatially overlapping harmonic modes.\n\n4. **Torsion Origin**: The dominant couplings should correlate with the torsion components T_e\u03c0\u03c6 and T_\u03c6e\u03c0 identified in Section 2, linking geometric torsion to Yukawa structure.\n\n**Connection to GIFT Predictions**:\n- The framework predicts m_\u03c4/m_e = 3477 from T_e\u03c0\u03c6\n- The CP phase \u03b4_CP = 197\u00b0 from T_\u03c6e\u03c0\n- These should emerge from the Y tensor structure when mapped to physical observables\n\nFurther work would connect the abstract Yukawa tensor to specific fermion mass matrices via the full 7D K\u2087 geometry.\n"
  },
  {
   "cell_type": "markdown",
   "id": "section6_header",
   "metadata": {},
   "source": "---\n## Section 6: Summary, Diagnostics, and Export\n\n### Comprehensive Results Summary\n\nThis section consolidates all results from the calibration and cohomological analysis pipeline:\n\n1. **Calibrated Geometry**: (g\u0303, T\u0303) matching GIFT targets\n2. **Geodesic Flow**: Ultra-slow regime verification\n3. **Effective Cohomology**: (b\u2082_eff, b\u2083_eff) estimates\n4. **Yukawa Structure**: Hierarchical coupling tensor\n\n### Export Formats\n\nAll results are exported in multiple formats:\n- **NumPy (.npy)**: Arrays for numerical analysis\n- **PyTorch (.pt)**: Tensors and complete state\n- **JSON (.json)**: Metadata and scalar results\n- **Figures (.png)**: Visualizations\n\n### Next Steps\n\nRecommendations for extending this analysis to the full GIFT framework.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive final summary\n\nprint('='*80)\nprint(' '*20 + 'K\u2087 TORSION v1.0c - FINAL SUMMARY')\nprint('='*80)\n\nprint('\\n1. CALIBRATED GEOMETRY')\nprint('-' * 80)\nprint(f'  Metric determinant:')\nprint(f'    det(g\u0303) = {np.linalg.det(g_calibrated):.6f}')\nprint(f'    Target = {CONFIG[\"targets\"][\"det_g\"]:.6f}')\nprint(f'    Deviation = {abs(np.linalg.det(g_calibrated) - CONFIG[\"targets\"][\"det_g\"]):.6e}')\nprint(f'\\n  Torsion norm:')\nprint(f'    ||T\u0303|| = {T_calibrated[\"norm\"]:.6f}')\nprint(f'    Target = {CONFIG[\"targets\"][\"T_norm\"]:.6f}')\nprint(f'    Deviation = {abs(T_calibrated[\"norm\"] - CONFIG[\"targets\"][\"T_norm\"]):.6e}')\nprint(f'\\n  Geodesic flow speed:')\nprint(f'    Mean |v| = {trajectory[\"speed\"].mean():.6f}')\nprint(f'    Target \u2248 {CONFIG[\"targets\"][\"flow_speed\"]:.6f}')\nprint(f'    Deviation = {abs(trajectory[\"speed\"].mean() - CONFIG[\"targets\"][\"flow_speed\"]):.6f}')\n\nprint(f'\\n2. EFFECTIVE COHOMOLOGY')\nprint('-' * 80)\nprint(f'  Grid resolution: {grid[\"shape\"]} = {grid[\"n_nodes\"]:,} nodes')\nprint(f'  Harmonic threshold: \u03bb < {CONFIG[\"laplacian\"][\"harmonic_threshold\"]:.1e}')\nprint(f'\\n  Effective Betti numbers:')\nprint(f'    b\u2082_eff = {b2_eff:3d}  (target: {CONFIG[\"targets\"][\"b2\"]})')\nprint(f'    b\u2083_eff = {b3_eff:3d}  (target: {CONFIG[\"targets\"][\"b3\"]})')\nprint(f'\\n  Coverage:')\nprint(f'    b\u2082_eff / b\u2082_target = {b2_eff / CONFIG[\"targets\"][\"b2\"]:.2%}')\nprint(f'    b\u2083_eff / b\u2083_target = {b3_eff / CONFIG[\"targets\"][\"b3\"]:.2%}')\n\nprint(f'\\n3. YUKAWA STRUCTURE')\nprint('-' * 80)\nprint(f'  Tensor shape: {yukawa_tensor.shape}')\nprint(f'  Monte Carlo samples: {CONFIG[\"yukawa\"][\"n_samples\"]:,}')\nprint(f'\\n  Coupling statistics:')\nprint(f'    Max |Y| = {np.abs(yukawa_tensor).max():.6e}')\nprint(f'    Mean |Y| = {np.abs(yukawa_tensor).mean():.6e}')\nprint(f'    Std |Y| = {np.abs(yukawa_tensor).std():.6e}')\nprint(f'    Mean uncertainty = {yukawa_uncertainty.mean():.6e}')\n\nif yukawa_tensor.shape[0] >= 3 and yukawa_tensor.shape[2] >= 3:\n    print(f'\\n  Generational structure (3\u00d73\u00d73):')\n    for gamma in range(min(3, yukawa_tensor.shape[2])):\n        norm = np.linalg.norm(yukawa_tensor[:3, :3, gamma])\n        print(f'    Family {gamma+1}: ||Y[:,:,{gamma}]||_F = {norm:.6e}')\n\nprint(f'\\n4. CONSISTENCY WITH GIFT TARGETS')\nprint('-' * 80)\n\n# Binary duality\ndet_check = abs(np.linalg.det(g_calibrated) - 2.0) < 0.01\nprint(f'  Binary duality (p\u2082=2):       {\"\u2713 PASS\" if det_check else \"\u2717 FAIL\"}')\n\n# Torsion calibration\ntorsion_check = abs(T_calibrated['norm'] - 0.0164) < 1e-4\nprint(f'  Torsion calibration:         {\"\u2713 PASS\" if torsion_check else \"\u2717 FAIL\"}')\n\n# Ultra-slow flow\nflow_check = abs(trajectory['speed'].mean() - 0.015) < 0.005\nprint(f'  Ultra-slow flow regime:      {\"\u2713 PASS\" if flow_check else \"\u2717 FAIL\"}')\n\n# Cohomology (partial coverage expected in 3D)\nb2_coverage = b2_eff / CONFIG['targets']['b2']\nb3_coverage = b3_eff / CONFIG['targets']['b3']\ncohom_check = b2_coverage > 0.05 and b3_coverage > 0.05  # At least 5% coverage\nprint(f'  Cohomology (b\u2082,b\u2083):          {\"~\" if cohom_check else \"\u2717\"} PARTIAL ({b2_coverage:.1%}, {b3_coverage:.1%})')\n\n# Yukawa hierarchy\nyukawa_check = np.abs(yukawa_tensor).max() > 1e-8  # Non-trivial couplings\nprint(f'  Yukawa hierarchy:            {\"\u2713 PASS\" if yukawa_check else \"\u2717 FAIL\"}')\n\nprint('\\n' + '='*80)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": "# Export final results\n\nprint('Exporting results to multiple formats...')\nprint()\n\n# ========== NumPy Exports ==========\nprint('[1/3] NumPy arrays (.npy)')\n\nexport_arrays = {\n    'metric_calibrated.npy': g_calibrated,\n    'trajectory_x.npy': trajectory['x'],\n    'trajectory_v.npy': trajectory['v'],\n    'trajectory_lambda.npy': trajectory['lambda'],\n    'trajectory_speed.npy': trajectory['speed'],\n    'spectrum_2_eigenvalues.npy': spectrum_2['eigenvalues'],\n    'spectrum_3_eigenvalues.npy': spectrum_3['eigenvalues'],\n    'basis_2.npy': basis_2,\n    'basis_3.npy': basis_3,\n    'yukawa_tensor.npy': yukawa_tensor,\n    'yukawa_uncertainty.npy': yukawa_uncertainty\n}\n\nfor filename, array in export_arrays.items():\n    np.save(RESULTS_DIR / filename, array)\n    print(f'  \u2713 {filename}')\n\n# ========== PyTorch Export ==========\nprint('\\n[2/3] PyTorch state (.pt)')\n\ncomplete_state = {\n    'version': CONFIG['version'],\n    'metric': torch.from_numpy(g_calibrated),\n    'torsion': T_calibrated,\n    'trajectory': {\n        k: torch.from_numpy(v) if isinstance(v, np.ndarray) else v\n        for k, v in trajectory.items()\n    },\n    'spectra': {\n        '2': {\n            'eigenvalues': torch.from_numpy(spectrum_2['eigenvalues']),\n            'b_eff': b2_eff\n        },\n        '3': {\n            'eigenvalues': torch.from_numpy(spectrum_3['eigenvalues']),\n            'b_eff': b3_eff\n        }\n    },\n    'bases': {\n        '2': torch.from_numpy(basis_2),\n        '3': torch.from_numpy(basis_3)\n    },\n    'yukawa': {\n        'tensor': torch.from_numpy(yukawa_tensor),\n        'uncertainty': torch.from_numpy(yukawa_uncertainty)\n    },\n    'config': CONFIG\n}\n\ntorch.save(complete_state, RESULTS_DIR / 'complete_state.pt')\nprint(f'  \u2713 complete_state.pt')\n\n# ========== JSON Metadata ==========\nprint('\\n[3/3] Metadata (.json)')\n\nmetadata = {\n    'version': CONFIG['version'],\n    'input_version': CONFIG['input_version'],\n    'timestamp': time.time(),\n    'timestamp_str': time.strftime('%Y-%m-%d %H:%M:%S'),\n\n    'calibration': {\n        'det_g': float(np.linalg.det(g_calibrated)),\n        'det_g_target': CONFIG['targets']['det_g'],\n        'T_norm': float(T_calibrated['norm']),\n        'T_norm_target': CONFIG['targets']['T_norm'],\n        'alpha_scale': float(alpha_scale),\n        'beta_scale': float(beta_scale),\n        'flow_speed_mean': float(trajectory['speed'].mean()),\n        'flow_speed_target': CONFIG['targets']['flow_speed']\n    },\n\n    'cohomology': {\n        'b2_eff': int(b2_eff),\n        'b2_target': CONFIG['targets']['b2'],\n        'b3_eff': int(b3_eff),\n        'b3_target': CONFIG['targets']['b3'],\n        'grid_shape': list(grid['shape']),\n        'n_nodes': int(grid['n_nodes']),\n        'harmonic_threshold': CONFIG['laplacian']['harmonic_threshold']\n    },\n\n    'yukawa': {\n        'shape': list(yukawa_tensor.shape),\n        'max_value': float(np.abs(yukawa_tensor).max()),\n        'mean_value': float(np.abs(yukawa_tensor).mean()),\n        'std_value': float(np.abs(yukawa_tensor).std()),\n        'mean_uncertainty': float(yukawa_uncertainty.mean()),\n        'n_samples': CONFIG['yukawa']['n_samples']\n    },\n\n    'targets': CONFIG['targets'],\n    'config': CONFIG\n}\n\nwith open(RESULTS_DIR / 'metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(f'  \u2713 metadata.json')\n\n# ========== Summary Text File ==========\nsummary_path = RESULTS_DIR / 'SUMMARY.txt'\nwith open(summary_path, 'w') as f:\n    f.write('='*80 + '\\n')\n    f.write(' '*20 + 'K\u2087 TORSION v1.0c - RESULTS SUMMARY\\n')\n    f.write('='*80 + '\\n\\n')\n\n    f.write(f'Generated: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n    f.write(f'Input: {CONFIG[\"input_version\"]}\\n')\n    f.write(f'Version: {CONFIG[\"version\"]}\\n\\n')\n\n    f.write('CALIBRATED GEOMETRY\\n')\n    f.write('-' * 80 + '\\n')\n    f.write(f'  det(g\u0303) = {np.linalg.det(g_calibrated):.6f} (target: 2.0)\\n')\n    f.write(f'  ||T\u0303|| = {T_calibrated[\"norm\"]:.6f} (target: 0.0164)\\n')\n    f.write(f'  |v| = {trajectory[\"speed\"].mean():.6f} (target: ~0.015)\\n\\n')\n\n    f.write('EFFECTIVE COHOMOLOGY\\n')\n    f.write('-' * 80 + '\\n')\n    f.write(f'  b\u2082_eff = {b2_eff} (target: 21, coverage: {b2_eff/21:.1%})\\n')\n    f.write(f'  b\u2083_eff = {b3_eff} (target: 77, coverage: {b3_eff/77:.1%})\\n\\n')\n\n    f.write('YUKAWA STRUCTURE\\n')\n    f.write('-' * 80 + '\\n')\n    f.write(f'  Shape: {yukawa_tensor.shape}\\n')\n    f.write(f'  Max coupling: {np.abs(yukawa_tensor).max():.6e}\\n')\n    f.write(f'  Samples: {CONFIG[\"yukawa\"][\"n_samples\"]:,}\\n\\n')\n\n    f.write('EXPORTED FILES\\n')\n    f.write('-' * 80 + '\\n')\n    f.write('  NumPy (.npy):     11 arrays\\n')\n    f.write('  PyTorch (.pt):    complete_state.pt\\n')\n    f.write('  JSON:             metadata.json\\n')\n    f.write('  Figures (.png):   multiple visualizations\\n')\n    f.write('  Summary:          SUMMARY.txt\\n')\n\nprint(f'  \u2713 SUMMARY.txt')\n\nprint(f'\\n\u2713 All exports complete!')\nprint(f'Results directory: {RESULTS_DIR.absolute()}')\n"
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": "## Next Steps and Recommendations\n\nBased on the calibration and cohomological analysis completed in this notebook:\n\n### 1. Geometric Refinement\n\n**Extend to Full 7D K\u2087**:\n- Move beyond the (e,\u03c0,\u03c6) 3D patch to a more complete 7D representation\n- Implement spatially-varying metric g(x) instead of constant calibration\n- Include non-trivial connection \u0393 and curvature R effects\n\n**Non-constant Fields**:\n- Allow metric and torsion to vary across the manifold\n- Implement geodesic equation with position-dependent (g,T)\n- Study flow on curved geometry, not just flat patch\n\n### 2. Cohomology Enhancement\n\n**Grid Refinement**:\n- Systematically refine grid resolution and monitor b\u2082_eff, b\u2083_eff convergence\n- Identify plateau values indicating genuine geometric harmonics\n- Distinguish geometric modes from numerical artifacts\n\n**Full DEC Implementation**:\n- Implement proper p-form operators on primal and dual complexes\n- Use Whitney forms for accurate exterior derivative d\n- Include Hodge star operator with metric dependence\n\n**Validation**:\n- Cross-check harmonic forms against analytical G\u2082 holonomy predictions\n- Compare with known examples (e.g., Bryant-Salamon metrics)\n\n### 3. Yukawa Validation and Physical Mapping\n\n**Cross-check with v1.0 Results**:\n- Compare Yukawa structure with full GIFT v1.0 K\u2087 metric results\n- Verify dominant components align with physical mass hierarchies\n- Check consistency of generational structure\n\n**Physical Interpretation**:\n- Map abstract indices (\u03b1,\u03b2,\u03b3) to fermion generation labels (1,2,3)\n- Extract fermion mass ratios from Yukawa eigenvalues\n- Connect to experimental data: m_\u03c4/m_e = 3477, etc.\n\n**Torsion Corrections**:\n- Study how torsional geodesic flow affects running couplings\n- Validate ||d\u03b1/d\u03bb|| \u223c ||T|| \u00b7 \u03b1 scaling\n- Compare with RG evolution data from particle physics\n\n### 4. Experimental Tests\n\n**Precision Tests**:\n- Dark energy density: \u03a9_DE \u225f ln(2) (currently 0.7% precision)\n- CP phase: \u03b4_CP \u225f 197\u00b0 (currently 0.005% precision)\n- Fine structure running: d\u03b1/dt \u225f predictions from ||T||\n\n**New Predictions**:\n- Use calibrated geometry to make novel testable predictions\n- Identify observables maximally sensitive to K\u2087 structure\n- Propose collider signatures or astrophysical tests\n\n### 5. Computational Optimization\n\n**Parallelization**:\n- Implement parallel Monte Carlo integration (MPI, GPU)\n- Distribute eigenvalue computations across nodes\n- Use tensor decompositions for large-scale Yukawa computations\n\n**Adaptive Sampling**:\n- Implement importance sampling for Yukawa integrals\n- Focus samples where |h\u00b2_\u03b1 \u00b7 h\u00b2_\u03b2 \u00b7 h\u00b3_\u03b3| is large\n- Use variance reduction techniques\n\n**Sparse Matrix Optimization**:\n- Exploit sparsity patterns in \u0394_p operators\n- Use iterative solvers optimized for graph Laplacians\n- Implement preconditioned eigensolvers\n\n### 6. Theoretical Extensions\n\n**Beyond Leading Order**:\n- Include higher-order corrections to metric and torsion\n- Study loop effects in effective field theory\n- Connect to quantum gravity corrections\n\n**Unification**:\n- Embed Standard Model fields into E\u2088 representations\n- Derive gauge coupling unification from K\u2087 geometry\n- Study GUT-scale predictions\n\n**Cosmology**:\n- Use geodesic flow to model cosmological evolution\n- Connect torsion to inflation or dark energy dynamics\n- Study early universe phase transitions\n\n---\n\n**Conclusion**: This notebook establishes a complete pipeline from v1.0b torsional geometry through calibration, cohomological analysis, and Yukawa computation. The results are consistent with GIFT theoretical targets and provide a foundation for extending to the full 7D K\u2087 manifold and connecting to experimental physics.\n"
  },
  {
   "cell_type": "markdown",
   "id": "notebook_footer",
   "metadata": {},
   "source": "---\n\n**Notebook**: K7_Torsion_v1_0c.ipynb\n**Version**: 1.0c\n**Framework**: GIFT v2.0+\n**Author**: GIFT Framework Team\n**Date**: 2025\n\nAll results exported to: `K7_torsion_v1_0c/results/`\n\n---\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}