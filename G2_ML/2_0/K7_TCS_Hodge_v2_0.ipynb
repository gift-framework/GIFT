{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K7 TCS Hodge v2.0 - Proper TCS/Joyce Global Modes\n",
    "\n",
    "**Major upgrade from v1.9b:**\n",
    "- Replace polynomial/trigonometric global modes with proper TCS structure\n",
    "- 77 H3 modes = 35 local (fiber) + 42 global (TCS gluing)\n",
    "- 42 global = 14 left-weighted + 14 right-weighted + 14 neck-coupled\n",
    "- Profile functions: smooth plateaus and Gaussian bumps\n",
    "- Goal: achieve 43/77 visible/hidden split with tau = 3472/891\n",
    "\n",
    "## TCS (Twisted Connected Sum) Construction\n",
    "\n",
    "Joyce's G2 manifolds are built by gluing two asymptotically cylindrical Calabi-Yau 3-folds:\n",
    "- Left building block: M_L with asymptotic cylinder\n",
    "- Right building block: M_R with asymptotic cylinder  \n",
    "- Neck region: where the gluing happens with twist\n",
    "\n",
    "The 42 global modes arise from this gluing structure."
   ]
  },
  {
   "cell_type": "code",
   "source": "# @title Setup and Imports\n# @markdown Run this cell first. Works on Colab, local Jupyter, or any Python environment.\n\nimport os\nimport sys\nimport json\nimport time\nimport csv\nfrom pathlib import Path\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Optional, Tuple, Dict, List\nfrom itertools import combinations\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Check environment\nIN_COLAB = 'google.colab' in sys.modules\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(\"=\" * 60)\nprint(\"K7 TCS HODGE v2.0 - Setup\")\nprint(\"=\" * 60)\nprint(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(f\"PyTorch: {torch.__version__}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Configuration\n# @markdown Adjust parameters here. TCS-specific settings control the profile functions.\n\n@dataclass\nclass Config:\n    \"\"\"v2.0 Configuration with TCS parameters.\"\"\"\n    \n    # === Geometry ===\n    dim: int = 7\n    b2_K7: int = 21          # Harmonic 2-forms\n    b3_K7: int = 77          # Harmonic 3-forms\n    b3_local: int = 35       # Local fiber modes\n    b3_global: int = 42      # Global TCS modes\n    \n    # TCS global mode breakdown: 42 = 14 + 14 + 14\n    n_left: int = 14         # Left-weighted modes\n    n_right: int = 14        # Right-weighted modes\n    n_neck: int = 14         # Neck-coupled modes\n    \n    # === Targets ===\n    target_det_g: float = 2.03125        # 65/32\n    target_kappa_T: float = 0.01639344   # 1/61\n    tau_target: float = 3.8967452        # 3472/891\n    \n    # === TCS Profile Parameters ===\n    lambda_L: float = -1.0   # Left boundary\n    lambda_R: float = +1.0   # Right boundary\n    lambda_neck: float = 0.0 # Neck center\n    sigma_transition: float = 0.15  # Transition width for plateaus\n    sigma_neck: float = 0.2         # Neck bump width\n    \n    # === Network Architecture ===\n    hidden_dim: int = 256\n    n_layers: int = 4\n    \n    # === Training ===\n    n_epochs_h2: int = 3000\n    n_epochs_h3: int = 8000  # More epochs for TCS learning\n    lr_h2: float = 1e-3\n    lr_h3: float = 3e-4      # Lower LR for stability\n    batch_size: int = 2048\n    weight_decay: float = 1e-5\n    max_grad_norm: float = 1.0\n    scheduler_patience: int = 400\n    scheduler_factor: float = 0.5\n    \n    # === Loss Weights ===\n    w_closed: float = 1.0\n    w_coclosed: float = 1.0\n    w_orthonormal: float = 0.1\n    w_g2_compat: float = 0.5\n    w_tcs_profile: float = 0.3   # NEW: TCS profile regularization\n    \n    # === Checkpointing ===\n    checkpoint_every: int = 500\n    checkpoint_dir: str = \"checkpoints_v2_0\"\n    log_every: int = 100\n    output_dir: str = \"outputs_v2_0\"\n\nconfig = Config()\n\nprint(\"Configuration loaded:\")\nprint(f\"  Geometry: b2={config.b2_K7}, b3={config.b3_K7} ({config.b3_local} local + {config.b3_global} global)\")\nprint(f\"  TCS global: {config.n_left} left + {config.n_right} right + {config.n_neck} neck\")\nprint(f\"  Training: H2={config.n_epochs_h2} epochs, H3={config.n_epochs_h3} epochs\")\nprint(f\"  Targets: det(g)={config.target_det_g}, tau={config.tau_target:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title TCS Profile Functions\n# @markdown These profile functions encode the TCS (Twisted Connected Sum) geometry.\n# @markdown - left_plateau: ~1 on left building block, ~0 elsewhere\n# @markdown - right_plateau: ~1 on right building block, ~0 elsewhere  \n# @markdown - neck_bump: peaked at neck, decays away\n\ndef smooth_step(x: torch.Tensor, x0: float = 0.0, width: float = 0.1) -> torch.Tensor:\n    \"\"\"Smooth sigmoid transition centered at x0 with given width.\"\"\"\n    w = max(width, 1e-8)\n    t = (x - x0) / w\n    return torch.sigmoid(5.0 * t)  # Steepness factor 5 for sharp-ish transition\n\ndef left_plateau(lam: torch.Tensor, config: Config) -> torch.Tensor:\n    \"\"\"\n    Profile for left-weighted modes.\n    Returns ~1 for lambda < lambda_neck, ~0 for lambda > lambda_neck.\n    \"\"\"\n    return 1.0 - smooth_step(lam, x0=config.lambda_neck, width=config.sigma_transition)\n\ndef right_plateau(lam: torch.Tensor, config: Config) -> torch.Tensor:\n    \"\"\"\n    Profile for right-weighted modes.\n    Returns ~0 for lambda < lambda_neck, ~1 for lambda > lambda_neck.\n    \"\"\"\n    return smooth_step(lam, x0=config.lambda_neck, width=config.sigma_transition)\n\ndef neck_bump(lam: torch.Tensor, config: Config) -> torch.Tensor:\n    \"\"\"\n    Profile for neck-coupled modes.\n    Gaussian bump centered at neck, decays to both sides.\n    \"\"\"\n    t = (lam - config.lambda_neck) / max(config.sigma_neck, 1e-8)\n    return torch.exp(-t * t)\n\ndef get_tcs_profiles(lam: torch.Tensor, config: Config) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Return all three TCS profiles for given lambda coordinates.\"\"\"\n    return left_plateau(lam, config), right_plateau(lam, config), neck_bump(lam, config)\n\n# Test profiles\nprint(\"TCS Profile Functions defined.\")\nprint(\"\\nTesting on lambda in [-1, 1]:\")\ntest_lam = torch.linspace(-1, 1, 11)\nL, R, N = get_tcs_profiles(test_lam, config)\nprint(f\"{'lambda':>8} | {'left':>6} | {'right':>6} | {'neck':>6}\")\nprint(\"-\" * 40)\nfor i, l in enumerate(test_lam):\n    print(f\"{l.item():8.2f} | {L[i].item():6.3f} | {R[i].item():6.3f} | {N[i].item():6.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Load Data\n# @markdown Loads calibrated K7 metric samples from v1.8/v1.9 or generates synthetic data.\n# @markdown For TCS, the first coordinate x[0] is interpreted as the neck coordinate lambda.\n\ndef load_data(path: Optional[str] = None) -> Dict[str, torch.Tensor]:\n    \"\"\"Load K7 metric data from various possible locations.\"\"\"\n    paths_to_try = [\n        path,\n        \"samples.npz\",\n        \"../1_9b/outputs_v1_9b/samples.npz\",\n        \"../1_8/samples.npz\",\n        \"/content/samples.npz\",\n        \"/content/drive/MyDrive/GIFT/G2_ML/1_8/samples.npz\",\n        \"/content/drive/MyDrive/GIFT/G2_ML/1_9b/outputs_v1_9b/samples.npz\",\n    ]\n    \n    for p in paths_to_try:\n        if p and os.path.exists(p):\n            print(f\"Loading from: {p}\")\n            data = np.load(p)\n            result = {\n                'coords': torch.from_numpy(data['coords']).float(),\n                'metric': torch.from_numpy(data['metric']).float(),\n            }\n            # phi may be in different formats\n            if 'phi' in data:\n                phi = data['phi']\n                if phi.ndim == 2 and phi.shape[1] == 35:\n                    result['phi'] = torch.from_numpy(phi).float()\n                elif phi.ndim == 3:\n                    # Take diagonal or mean\n                    result['phi'] = torch.from_numpy(phi).float().mean(dim=-1)\n                else:\n                    result['phi'] = torch.from_numpy(phi).float()\n            return result\n    \n    # Generate synthetic data with TCS-aware coordinates\n    print(\"Data not found, generating synthetic with TCS structure...\")\n    n = 8000\n    \n    # First coordinate is lambda (neck coordinate) in [-1, 1]\n    # Others are angular coordinates in [0, 2pi]\n    coords = torch.zeros(n, 7)\n    coords[:, 0] = torch.rand(n) * 2 - 1  # lambda in [-1, 1]\n    coords[:, 1:] = torch.rand(n, 6) * 2 * np.pi  # xi in [0, 2pi]\n    \n    # Metric with det(g) ~ 65/32\n    scale = config.target_det_g ** (1/7)\n    metric = torch.eye(7).unsqueeze(0).expand(n, -1, -1).clone() * scale\n    metric = metric + 0.02 * torch.randn(n, 7, 7)\n    metric = 0.5 * (metric + metric.transpose(-1, -2))  # Symmetrize\n    \n    # Phi: G2 3-form components (35 basis elements)\n    phi = torch.randn(n, 35) * 0.3\n    # Normalize to ||phi||^2 ~ 7\n    phi = phi * np.sqrt(7.0) / (torch.norm(phi, dim=1, keepdim=True) + 1e-8)\n    \n    return {'coords': coords, 'metric': metric, 'phi': phi}\n\n# Load data\ndata = load_data()\nn_samples = data['coords'].shape[0]\n\n# Compute statistics\ndet_g = torch.det(data['metric'])\ndet_g_mean = det_g.mean().item()\ndet_g_std = det_g.std().item()\n\n# Lambda distribution\nlam = data['coords'][:, 0]\nlam_min, lam_max = lam.min().item(), lam.max().item()\n\nprint(f\"\\nData loaded:\")\nprint(f\"  Samples: {n_samples}\")\nprint(f\"  det(g): {det_g_mean:.6f} +/- {det_g_std:.4f} (target: {config.target_det_g})\")\nprint(f\"  Lambda range: [{lam_min:.2f}, {lam_max:.2f}]\")\nprint(f\"  Phi shape: {data['phi'].shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title H2 Network (21 harmonic 2-forms)\n# @markdown Standard architecture for 2-forms, unchanged from v1.9b.\n\nclass H2Network(nn.Module):\n    \"\"\"\n    Network for 21 harmonic 2-forms on K7.\n    Output: (batch, 21, 21) - 21 forms, each with 21 = C(7,2) components.\n    \"\"\"\n    \n    def __init__(self, config: Config):\n        super().__init__()\n        \n        # Shared feature extractor\n        layers = []\n        in_dim = config.dim\n        for _ in range(config.n_layers):\n            layers.extend([\n                nn.Linear(in_dim, config.hidden_dim),\n                nn.SiLU(),\n            ])\n            in_dim = config.hidden_dim\n        self.features = nn.Sequential(*layers)\n        \n        # 21 separate heads for each 2-form\n        # Each outputs 21 components (antisymmetric 2-tensor has C(7,2) = 21 components)\n        self.heads = nn.ModuleList([\n            nn.Linear(config.hidden_dim, 21) for _ in range(config.b2_K7)\n        ])\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, 7) coordinates\n        Returns:\n            omega: (batch, 21, 21) - 21 2-forms with 21 components each\n        \"\"\"\n        f = self.features(x)\n        forms = [head(f) for head in self.heads]\n        return torch.stack(forms, dim=1)\n\n# Test\n_test_h2 = H2Network(config)\n_test_out = _test_h2(torch.randn(4, 7))\nprint(f\"H2Network defined.\")\nprint(f\"  Parameters: {sum(p.numel() for p in _test_h2.parameters()):,}\")\nprint(f\"  Output shape: {_test_out.shape} (batch, 21 forms, 21 components)\")\ndel _test_h2, _test_out",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title H3 TCS Network (77 harmonic 3-forms with TCS structure)\n# @markdown **KEY v2.0 CHANGE**: Global modes use proper TCS profiles.\n# @markdown - 35 local modes: fiber modes, independent of neck coordinate\n# @markdown - 14 left-weighted: multiplied by left_plateau(lambda)\n# @markdown - 14 right-weighted: multiplied by right_plateau(lambda)\n# @markdown - 14 neck-coupled: multiplied by neck_bump(lambda)\n\nclass H3TCSNetwork(nn.Module):\n    \"\"\"\n    Network for 77 harmonic 3-forms with TCS (Twisted Connected Sum) structure.\n    \n    The 77 = 35 + 42 modes are:\n    - 35 local modes: fiber \u039b\u00b3(R\u2077) modes, no special profile\n    - 42 global TCS modes:\n        - 14 left-weighted: profile = left_plateau(\u03bb)\n        - 14 right-weighted: profile = right_plateau(\u03bb)\n        - 14 neck-coupled: profile = neck_bump(\u03bb)\n    \n    Output: (batch, 77, 35) - 77 forms, each with 35 = C(7,3) components.\n    \"\"\"\n    \n    def __init__(self, config: Config):\n        super().__init__()\n        self.config = config\n        \n        # Shared feature extractor\n        layers = []\n        in_dim = config.dim\n        for _ in range(config.n_layers):\n            layers.extend([\n                nn.Linear(in_dim, config.hidden_dim),\n                nn.SiLU(),\n            ])\n            in_dim = config.hidden_dim\n        self.features = nn.Sequential(*layers)\n        \n        # === Local modes (35) ===\n        # These are fiber modes, independent of lambda\n        self.local_heads = nn.ModuleList([\n            nn.Linear(config.hidden_dim, 35) for _ in range(config.b3_local)\n        ])\n        \n        # === Global TCS modes (42 = 14 + 14 + 14) ===\n        # Each global mode has a base form multiplied by a profile\n        \n        # Left-weighted modes (14)\n        self.left_heads = nn.ModuleList([\n            nn.Linear(config.hidden_dim, 35) for _ in range(config.n_left)\n        ])\n        \n        # Right-weighted modes (14)\n        self.right_heads = nn.ModuleList([\n            nn.Linear(config.hidden_dim, 35) for _ in range(config.n_right)\n        ])\n        \n        # Neck-coupled modes (14)\n        self.neck_heads = nn.ModuleList([\n            nn.Linear(config.hidden_dim, 35) for _ in range(config.n_neck)\n        ])\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, 7) coordinates, x[:, 0] is the neck coordinate lambda\n        Returns:\n            Phi: (batch, 77, 35) - 77 3-forms with 35 components each\n        \"\"\"\n        batch = x.shape[0]\n        lam = x[:, 0]  # Neck coordinate\n        \n        # Get TCS profiles\n        prof_left = left_plateau(lam, self.config)      # (batch,)\n        prof_right = right_plateau(lam, self.config)    # (batch,)\n        prof_neck = neck_bump(lam, self.config)         # (batch,)\n        \n        # Feature extraction\n        f = self.features(x)  # (batch, hidden_dim)\n        \n        # === Local modes (35) ===\n        local_forms = torch.stack([h(f) for h in self.local_heads], dim=1)  # (batch, 35, 35)\n        \n        # === Global TCS modes ===\n        # Left-weighted: base_form * left_plateau(lambda)\n        left_base = torch.stack([h(f) for h in self.left_heads], dim=1)  # (batch, 14, 35)\n        left_forms = left_base * prof_left.unsqueeze(-1).unsqueeze(-1)   # Apply profile\n        \n        # Right-weighted: base_form * right_plateau(lambda)\n        right_base = torch.stack([h(f) for h in self.right_heads], dim=1)  # (batch, 14, 35)\n        right_forms = right_base * prof_right.unsqueeze(-1).unsqueeze(-1)\n        \n        # Neck-coupled: base_form * neck_bump(lambda)\n        neck_base = torch.stack([h(f) for h in self.neck_heads], dim=1)  # (batch, 14, 35)\n        neck_forms = neck_base * prof_neck.unsqueeze(-1).unsqueeze(-1)\n        \n        # Concatenate: [local (35), left (14), right (14), neck (14)] = 77\n        Phi = torch.cat([local_forms, left_forms, right_forms, neck_forms], dim=1)\n        \n        return Phi\n\n# Test\n_test_h3 = H3TCSNetwork(config)\n_test_x = torch.randn(4, 7)\n_test_x[:, 0] = torch.tensor([-0.8, -0.2, 0.2, 0.8])  # Various lambda values\n_test_out = _test_h3(_test_x)\n\nprint(f\"H3TCSNetwork defined.\")\nprint(f\"  Parameters: {sum(p.numel() for p in _test_h3.parameters()):,}\")\nprint(f\"  Output shape: {_test_out.shape} (batch, 77 forms, 35 components)\")\nprint(f\"\\nTCS mode structure:\")\nprint(f\"  Modes 0-34: Local fiber modes\")\nprint(f\"  Modes 35-48: Left-weighted (left_plateau)\")\nprint(f\"  Modes 49-62: Right-weighted (right_plateau)\")\nprint(f\"  Modes 63-76: Neck-coupled (neck_bump)\")\n\n# Check profile effect\nprint(f\"\\nProfile effect at different lambda:\")\nfor i, l in enumerate([-0.8, -0.2, 0.2, 0.8]):\n    norms = _test_out[i].norm(dim=1)\n    print(f\"  lambda={l:+.1f}: local={norms[:35].mean():.3f}, left={norms[35:49].mean():.3f}, right={norms[49:63].mean():.3f}, neck={norms[63:].mean():.3f}\")\n\ndel _test_h3, _test_x, _test_out",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Loss Functions\n# @markdown Losses enforce: harmonicity (d=0, d*=0), orthonormality, G2 compatibility.\n\ndef gram_matrix(forms: torch.Tensor, metric: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute Gram matrix G_ij = <form_i, form_j> with metric weighting.\n    \n    Args:\n        forms: (batch, n_forms, n_components)\n        metric: (batch, 7, 7)\n    Returns:\n        G: (n_forms, n_forms) averaged Gram matrix\n    \"\"\"\n    det_g = torch.det(metric)\n    vol = torch.sqrt(det_g.abs()).unsqueeze(-1).unsqueeze(-1)  # (batch, 1, 1)\n    weighted = forms * vol  # Volume weighting\n    G = torch.einsum('bic,bjc->ij', weighted, forms) / forms.shape[0]\n    return G\n\ndef orthonormality_loss(G: torch.Tensor) -> torch.Tensor:\n    \"\"\"Loss for enforcing G = I (orthonormal forms).\"\"\"\n    I = torch.eye(G.shape[0], device=G.device)\n    return torch.mean((G - I) ** 2)\n\ndef closedness_loss_fd(x: torch.Tensor, model: nn.Module, eps: float = 1e-4) -> torch.Tensor:\n    \"\"\"\n    Finite-difference approximation of d(omega) = 0 (closedness).\n    Penalizes large gradients of form components.\n    \"\"\"\n    total = torch.tensor(0.0, device=x.device)\n    omega_0 = model(x)\n    \n    for c in range(7):\n        x_plus = x.clone()\n        x_minus = x.clone()\n        x_plus[:, c] += eps\n        x_minus[:, c] -= eps\n        \n        grad = (model(x_plus) - model(x_minus)) / (2 * eps)\n        total = total + torch.mean(grad ** 2)\n    \n    return total / 7\n\ndef g2_compatibility_loss(Phi: torch.Tensor, phi_ref: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Loss for G2 3-form compatibility.\n    The diagonal components should match the reference G2 3-form.\n    \n    Args:\n        Phi: (batch, 77, 35) - 77 3-forms\n        phi_ref: (batch, 35) - reference G2 form components\n    \"\"\"\n    # Extract diagonal from local modes (first 35)\n    local_diag = Phi[:, :35, :].diagonal(dim1=1, dim2=2)  # (batch, 35)\n    return torch.mean((local_diag - phi_ref) ** 2)\n\ndef tcs_profile_regularization(Phi: torch.Tensor, lam: torch.Tensor, config: Config) -> torch.Tensor:\n    \"\"\"\n    NEW v2.0: Regularize global modes to follow their expected profiles.\n    \n    This encourages:\n    - Left modes to be large when lambda < 0\n    - Right modes to be large when lambda > 0\n    - Neck modes to be large when lambda ~ 0\n    \"\"\"\n    batch = Phi.shape[0]\n    \n    # Get expected profiles\n    prof_left = left_plateau(lam, config)\n    prof_right = right_plateau(lam, config)\n    prof_neck = neck_bump(lam, config)\n    \n    # Global mode norms (modes 35-76)\n    left_norms = Phi[:, 35:49, :].norm(dim=2).mean(dim=1)   # (batch,)\n    right_norms = Phi[:, 49:63, :].norm(dim=2).mean(dim=1)  # (batch,)\n    neck_norms = Phi[:, 63:77, :].norm(dim=2).mean(dim=1)   # (batch,)\n    \n    # Normalize profiles to similar scale as norms\n    scale = (left_norms.mean() + right_norms.mean() + neck_norms.mean()) / 3 + 1e-8\n    \n    # Loss: mode norms should follow their profiles\n    loss_left = torch.mean((left_norms / scale - prof_left) ** 2)\n    loss_right = torch.mean((right_norms / scale - prof_right) ** 2)\n    loss_neck = torch.mean((neck_norms / scale - prof_neck) ** 2)\n    \n    return (loss_left + loss_right + loss_neck) / 3\n\nprint(\"Loss functions defined:\")\nprint(\"  - gram_matrix: Hodge inner product\")\nprint(\"  - orthonormality_loss: G = I\")\nprint(\"  - closedness_loss_fd: d(omega) = 0\")\nprint(\"  - g2_compatibility_loss: match G2 form\")\nprint(\"  - tcs_profile_regularization: TCS structure (NEW)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Checkpointing and Auto-Resume\n# @markdown Saves checkpoints regularly. Training resumes automatically if interrupted.\n\ndef save_checkpoint(path: str, epoch: int, model: nn.Module, optimizer: optim.Optimizer,\n                   scheduler, losses: Dict, best_loss: float, phase: str):\n    \"\"\"Save training checkpoint.\"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n        'losses': losses,\n        'best_loss': best_loss,\n        'phase': phase,\n        'config': asdict(config),\n    }, path)\n\ndef load_checkpoint(path: str, model: nn.Module, optimizer: optim.Optimizer,\n                   scheduler=None) -> Dict:\n    \"\"\"Load checkpoint and restore state.\"\"\"\n    ckpt = torch.load(path, map_location=device)\n    model.load_state_dict(ckpt['model_state_dict'])\n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    if scheduler and ckpt.get('scheduler_state_dict'):\n        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n    print(f\"  Resumed from epoch {ckpt['epoch']} (best loss: {ckpt['best_loss']:.2e})\")\n    return ckpt\n\ndef find_latest_checkpoint(checkpoint_dir: str, phase: str) -> Optional[str]:\n    \"\"\"Find most recent checkpoint for a phase.\"\"\"\n    if not os.path.exists(checkpoint_dir):\n        return None\n    pattern = f\"{phase}_epoch_*.pt\"\n    ckpts = sorted(Path(checkpoint_dir).glob(pattern))\n    return str(ckpts[-1]) if ckpts else None\n\ndef format_time(seconds: float) -> str:\n    \"\"\"Format seconds as HH:MM:SS.\"\"\"\n    h = int(seconds // 3600)\n    m = int((seconds % 3600) // 60)\n    s = int(seconds % 60)\n    return f\"{h:02d}:{m:02d}:{s:02d}\"\n\n# Create checkpoint directory\nos.makedirs(config.checkpoint_dir, exist_ok=True)\nprint(f\"Checkpointing configured:\")\nprint(f\"  Directory: {config.checkpoint_dir}\")\nprint(f\"  Save every: {config.checkpoint_every} epochs\")\nprint(f\"  Log every: {config.log_every} epochs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Phase 1: Train H2 (21 harmonic 2-forms)\n# @markdown Text-only monitoring. Auto-resumes from checkpoint if available.\n\ndef train_h2(config: Config, data: Dict[str, torch.Tensor], resume: bool = True) -> Tuple[nn.Module, Dict]:\n    \"\"\"Train H2 network with auto-resume capability.\"\"\"\n    \n    print(\"=\" * 70)\n    print(\"PHASE 1: Training H2 (21 harmonic 2-forms)\")\n    print(\"=\" * 70)\n    \n    # Initialize\n    model = H2Network(config).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=config.lr_h2, weight_decay=config.weight_decay)\n    scheduler = ReduceLROnPlateau(optimizer, patience=config.scheduler_patience, \n                                   factor=config.scheduler_factor, min_lr=1e-6)\n    \n    coords = data['coords'].to(device)\n    metric = data['metric'].to(device)\n    n = coords.shape[0]\n    \n    start_epoch = 0\n    best_loss = float('inf')\n    all_losses = {'total': [], 'ortho': [], 'closed': []}\n    best_state = model.state_dict().copy()\n    \n    # Try to resume\n    if resume:\n        latest = find_latest_checkpoint(config.checkpoint_dir, 'h2')\n        if latest:\n            print(f\"Found checkpoint: {latest}\")\n            ckpt = load_checkpoint(latest, model, optimizer, scheduler)\n            start_epoch = ckpt['epoch'] + 1\n            best_loss = ckpt['best_loss']\n            all_losses = ckpt.get('losses', all_losses)\n            best_state = model.state_dict().copy()\n    \n    if start_epoch == 0:\n        print(\"Starting fresh training...\")\n    \n    # Header\n    print(f\"\\n{'Epoch':>6} | {'Loss':>10} | {'Ortho':>10} | {'Closed':>10} | {'Best':>10} | {'LR':>8} | {'Time':>8}\")\n    print(\"-\" * 80)\n    \n    start_time = time.time()\n    \n    for epoch in range(start_epoch, config.n_epochs_h2):\n        model.train()\n        \n        # Sample batch\n        idx = torch.randperm(n)[:config.batch_size]\n        x, g = coords[idx], metric[idx]\n        \n        # Forward\n        omega = model(x)\n        \n        # Losses\n        G = gram_matrix(omega, g)\n        loss_ortho = orthonormality_loss(G)\n        loss_closed = closedness_loss_fd(x, model)\n        total = config.w_orthonormal * loss_ortho + config.w_closed * loss_closed\n        \n        # Backward\n        optimizer.zero_grad()\n        total.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n        optimizer.step()\n        scheduler.step(total)\n        \n        # Track\n        all_losses['total'].append(total.item())\n        all_losses['ortho'].append(loss_ortho.item())\n        all_losses['closed'].append(loss_closed.item())\n        \n        if total.item() < best_loss:\n            best_loss = total.item()\n            best_state = model.state_dict().copy()\n        \n        # Log\n        if (epoch + 1) % config.log_every == 0:\n            elapsed = time.time() - start_time\n            lr = optimizer.param_groups[0]['lr']\n            print(f\"{epoch+1:6d} | {total.item():10.2e} | {loss_ortho.item():10.2e} | \"\n                  f\"{loss_closed.item():10.2e} | {best_loss:10.2e} | {lr:8.1e} | {format_time(elapsed)}\")\n        \n        # Checkpoint\n        if (epoch + 1) % config.checkpoint_every == 0:\n            ckpt_path = f\"{config.checkpoint_dir}/h2_epoch_{epoch+1:05d}.pt\"\n            save_checkpoint(ckpt_path, epoch, model, optimizer, scheduler, all_losses, best_loss, 'h2')\n    \n    # Restore best\n    model.load_state_dict(best_state)\n    \n    # Final checkpoint\n    final_path = f\"{config.checkpoint_dir}/h2_final.pt\"\n    save_checkpoint(final_path, config.n_epochs_h2 - 1, model, optimizer, scheduler, all_losses, best_loss, 'h2')\n    \n    print(f\"\\nH2 training complete.\")\n    print(f\"  Best loss: {best_loss:.2e}\")\n    print(f\"  Saved: {final_path}\")\n    \n    return model, all_losses\n\n# Train H2\nh2_model, h2_losses = train_h2(config, data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Phase 2: Train H3 with TCS structure (77 harmonic 3-forms)\n# @markdown **KEY v2.0**: Includes TCS profile regularization for global modes.\n\ndef train_h3_tcs(config: Config, data: Dict[str, torch.Tensor], resume: bool = True) -> Tuple[nn.Module, Dict]:\n    \"\"\"Train H3 TCS network with proper global mode structure.\"\"\"\n    \n    print(\"=\" * 70)\n    print(\"PHASE 2: Training H3 with TCS Structure (77 harmonic 3-forms)\")\n    print(\"=\" * 70)\n    print(\"  35 local modes + 42 global TCS modes (14 left + 14 right + 14 neck)\")\n    \n    # Initialize\n    model = H3TCSNetwork(config).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=config.lr_h3, weight_decay=config.weight_decay)\n    scheduler = ReduceLROnPlateau(optimizer, patience=config.scheduler_patience,\n                                   factor=config.scheduler_factor, min_lr=1e-6)\n    \n    coords = data['coords'].to(device)\n    metric = data['metric'].to(device)\n    phi = data['phi'].to(device)\n    n = coords.shape[0]\n    \n    start_epoch = 0\n    best_loss = float('inf')\n    all_losses = {'total': [], 'ortho': [], 'closed': [], 'g2': [], 'tcs': []}\n    best_state = model.state_dict().copy()\n    \n    # Try to resume\n    if resume:\n        latest = find_latest_checkpoint(config.checkpoint_dir, 'h3')\n        if latest:\n            print(f\"Found checkpoint: {latest}\")\n            ckpt = load_checkpoint(latest, model, optimizer, scheduler)\n            start_epoch = ckpt['epoch'] + 1\n            best_loss = ckpt['best_loss']\n            all_losses = ckpt.get('losses', all_losses)\n            best_state = model.state_dict().copy()\n    \n    if start_epoch == 0:\n        print(\"Starting fresh training...\")\n    \n    # Header\n    print(f\"\\n{'Epoch':>6} | {'Loss':>9} | {'Ortho':>9} | {'Closed':>9} | {'G2':>9} | {'TCS':>9} | {'Best':>9} | {'Time':>8}\")\n    print(\"-\" * 95)\n    \n    start_time = time.time()\n    \n    for epoch in range(start_epoch, config.n_epochs_h3):\n        model.train()\n        \n        # Sample batch\n        idx = torch.randperm(n)[:config.batch_size]\n        x, g, p = coords[idx], metric[idx], phi[idx]\n        lam = x[:, 0]  # Neck coordinate\n        \n        # Forward\n        Phi = model(x)  # (batch, 77, 35)\n        \n        # Losses\n        G = gram_matrix(Phi, g)\n        loss_ortho = orthonormality_loss(G)\n        loss_closed = closedness_loss_fd(x, model)\n        loss_g2 = g2_compatibility_loss(Phi, p)\n        loss_tcs = tcs_profile_regularization(Phi, lam, config)\n        \n        total = (config.w_orthonormal * loss_ortho + \n                config.w_closed * loss_closed + \n                config.w_g2_compat * loss_g2 +\n                config.w_tcs_profile * loss_tcs)\n        \n        # Backward\n        optimizer.zero_grad()\n        total.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n        optimizer.step()\n        scheduler.step(total)\n        \n        # Track\n        all_losses['total'].append(total.item())\n        all_losses['ortho'].append(loss_ortho.item())\n        all_losses['closed'].append(loss_closed.item())\n        all_losses['g2'].append(loss_g2.item())\n        all_losses['tcs'].append(loss_tcs.item())\n        \n        if total.item() < best_loss:\n            best_loss = total.item()\n            best_state = model.state_dict().copy()\n        \n        # Log\n        if (epoch + 1) % config.log_every == 0:\n            elapsed = time.time() - start_time\n            print(f\"{epoch+1:6d} | {total.item():9.2e} | {loss_ortho.item():9.2e} | \"\n                  f\"{loss_closed.item():9.2e} | {loss_g2.item():9.2e} | {loss_tcs.item():9.2e} | \"\n                  f\"{best_loss:9.2e} | {format_time(elapsed)}\")\n        \n        # Checkpoint\n        if (epoch + 1) % config.checkpoint_every == 0:\n            ckpt_path = f\"{config.checkpoint_dir}/h3_epoch_{epoch+1:05d}.pt\"\n            save_checkpoint(ckpt_path, epoch, model, optimizer, scheduler, all_losses, best_loss, 'h3')\n    \n    # Restore best\n    model.load_state_dict(best_state)\n    \n    # Final checkpoint\n    final_path = f\"{config.checkpoint_dir}/h3_final.pt\"\n    save_checkpoint(final_path, config.n_epochs_h3 - 1, model, optimizer, scheduler, all_losses, best_loss, 'h3')\n    \n    print(f\"\\nH3 TCS training complete.\")\n    print(f\"  Best loss: {best_loss:.2e}\")\n    print(f\"  Saved: {final_path}\")\n    \n    return model, all_losses\n\n# Train H3 with TCS structure\nh3_model, h3_losses = train_h3_tcs(config, data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Phase 3: Compute Yukawa Tensor\n# @markdown Y_ijk = integral(omega_i wedge omega_j wedge Phi_k) using proper Levi-Civita.\n\ndef levi_civita_7(indices: Tuple[int, ...]) -> int:\n    \"\"\"Compute Levi-Civita symbol for 7D indices.\"\"\"\n    if len(set(indices)) != 7:\n        return 0\n    inv = sum(1 for i in range(7) for j in range(i+1, 7) if indices[i] > indices[j])\n    return 1 if inv % 2 == 0 else -1\n\ndef build_yukawa_coefficients() -> List[Tuple[int, int, int, int]]:\n    \"\"\"Build list of non-zero Yukawa wedge coefficients.\"\"\"\n    pairs = list(combinations(range(7), 2))    # 21 pairs for 2-forms\n    triples = list(combinations(range(7), 3))  # 35 triples for 3-forms\n    \n    coeffs = []\n    for i1, p1 in enumerate(pairs):\n        for i2, p2 in enumerate(pairs):\n            if i2 < i1:\n                continue  # Symmetry\n            for i3, t in enumerate(triples):\n                all_idx = p1 + p2 + t\n                if len(set(all_idx)) != 7:\n                    continue  # Not a valid wedge\n                sign = levi_civita_7(all_idx)\n                if sign != 0:\n                    coeffs.append((i1, i2, i3, sign))\n    \n    return coeffs\n\n# Build coefficients once\nYUKAWA_COEFFS = build_yukawa_coefficients()\nprint(f\"Built {len(YUKAWA_COEFFS)} Yukawa wedge coefficients\")\n\ndef compute_yukawa(h2_model: nn.Module, h3_model: nn.Module, \n                   data: Dict[str, torch.Tensor], \n                   coeffs: List[Tuple[int, int, int, int]],\n                   n_pts: int = 5000) -> Dict[str, np.ndarray]:\n    \"\"\"Compute Yukawa tensor Y_ijk and Gram matrix M.\"\"\"\n    \n    print(\"=\" * 70)\n    print(\"PHASE 3: Computing Yukawa Tensor (proper wedge product)\")\n    print(\"=\" * 70)\n    \n    h2_model.eval()\n    h3_model.eval()\n    \n    coords = data['coords'].to(device)\n    metric = data['metric'].to(device)\n    n = min(n_pts, coords.shape[0])\n    idx = torch.randperm(coords.shape[0])[:n]\n    x, g = coords[idx], metric[idx]\n    \n    # Volume element\n    det_g = torch.det(g)\n    vol = torch.sqrt(det_g.abs())\n    total_vol = vol.sum()\n    \n    with torch.no_grad():\n        omega = h2_model(x)  # (n, 21, 21)\n        Phi = h3_model(x)    # (n, 77, 35)\n    \n    print(f\"Integration points: {n}\")\n    print(f\"omega shape: {omega.shape}\")\n    print(f\"Phi shape: {Phi.shape}\")\n    print(\"Computing Y_ijk...\")\n    \n    Y = torch.zeros(21, 21, 77, device=device)\n    \n    for a in range(21):\n        if (a + 1) % 7 == 0:\n            print(f\"  H2 mode {a+1}/21\")\n        omega_a = omega[:, a, :]  # (n, 21)\n        \n        for b in range(a, 21):\n            omega_b = omega[:, b, :]  # (n, 21)\n            \n            for c in range(77):\n                Phi_c = Phi[:, c, :]  # (n, 35)\n                \n                # Compute wedge integral\n                integral = torch.zeros(n, device=device)\n                for i1, i2, i3, sign in coeffs:\n                    integral += sign * omega_a[:, i1] * omega_b[:, i2] * Phi_c[:, i3]\n                \n                Y[a, b, c] = (integral * vol).sum() / total_vol\n                if a != b:\n                    Y[b, a, c] = -Y[a, b, c]  # Antisymmetry\n    \n    print(\"Computing Gram matrix M = Y^T Y...\")\n    # M_kl = sum_ij Y_ijk * Y_ijl\n    M = torch.einsum('ijk,ijl->kl', Y, Y)\n    \n    print(\"Eigendecomposition...\")\n    eigenvalues, eigenvectors = torch.linalg.eigh(M)\n    idx_sort = torch.argsort(eigenvalues, descending=True)\n    eigenvalues = eigenvalues[idx_sort]\n    eigenvectors = eigenvectors[:, idx_sort]\n    \n    print(\"Done.\")\n    \n    return {\n        'Y': Y.cpu().numpy(),\n        'M': M.cpu().numpy(),\n        'eigenvalues': eigenvalues.cpu().numpy(),\n        'eigenvectors': eigenvectors.cpu().numpy(),\n        'omega': omega.cpu().numpy(),\n        'Phi': Phi.cpu().numpy(),\n    }\n\n# Compute Yukawa\nyukawa = compute_yukawa(h2_model, h3_model, data, YUKAWA_COEFFS)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Spectral Analysis\n# @markdown Analyze Yukawa spectrum for 43/77 split and tau = 3472/891.\n\ndef analyze_spectrum(eigs: np.ndarray, tau_target: float = 3472/891) -> Dict:\n    \"\"\"Comprehensive spectral analysis of Yukawa Gram matrix.\"\"\"\n    \n    print(\"=\" * 70)\n    print(\"YUKAWA SPECTRAL ANALYSIS\")\n    print(\"=\" * 70)\n    \n    # Non-zero count\n    nonzero_mask = np.abs(eigs) > 1e-10\n    nonzero = nonzero_mask.sum()\n    \n    print(f\"\\n[EIGENVALUES]\")\n    print(f\"  Total: {len(eigs)}, Non-zero: {nonzero}\")\n    print(f\"  Top 5: {eigs[:5].round(4)}\")\n    print(f\"  Around 35: eigs[32:38] = {eigs[32:38].round(6)}\")\n    print(f\"  Around 43: eigs[40:46] = {eigs[40:46]}\")\n    \n    # Gaps\n    gaps = np.abs(np.diff(eigs))\n    mean_gap = gaps.mean() if len(gaps) > 0 else 1.0\n    \n    print(f\"\\n[TOP 5 GAPS]\")\n    gap_order = np.argsort(gaps)[::-1]\n    for i, idx in enumerate(gap_order[:5]):\n        ratio = gaps[idx] / mean_gap if mean_gap > 0 else 0\n        print(f\"  #{i+1}: gap {idx}->{idx+1}: {gaps[idx]:.6f} ({ratio:.1f}x mean)\")\n    \n    # Key positions\n    print(f\"\\n[KEY POSITIONS]\")\n    for pos in [20, 21, 34, 35, 42, 43]:\n        if pos < len(gaps):\n            ratio = gaps[pos] / mean_gap if mean_gap > 0 else 0\n            print(f\"  Gap {pos}->{pos+1}: {gaps[pos]:.6f} ({ratio:.1f}x mean)\")\n    \n    # Cumulative\n    cumsum = np.cumsum(eigs)\n    total = eigs.sum() if eigs.sum() > 0 else 1.0\n    \n    print(f\"\\n[CUMULATIVE VARIANCE]\")\n    for n in [21, 35, 43, 77]:\n        if n <= len(eigs):\n            pct = 100 * cumsum[n-1] / total\n            print(f\"  First {n}: {pct:.1f}%\")\n    \n    # Tau search\n    print(f\"\\n[TAU SEARCH] (target: {tau_target:.4f})\")\n    best_n, best_ratio, best_err = 0, 0, float('inf')\n    \n    for n in range(20, 55):\n        if n < len(eigs):\n            visible = cumsum[n-1]\n            hidden = total - visible\n            if hidden > 1e-8:\n                ratio = visible / hidden\n                err = 100 * abs(ratio - tau_target) / tau_target\n                if err < best_err:\n                    best_n, best_ratio, best_err = n, ratio, err\n    \n    if best_err < float('inf'):\n        print(f\"  Best: n={best_n}, tau={best_ratio:.4f}, error={best_err:.1f}%\")\n    else:\n        print(f\"  No valid tau (hidden sum ~ 0)\")\n    \n    # Check specific tau values\n    print(f\"\\n[TAU AT KEY POSITIONS]\")\n    for n in [35, 42, 43]:\n        if n < len(eigs):\n            visible = cumsum[n-1]\n            hidden = total - visible\n            if hidden > 1e-8:\n                ratio = visible / hidden\n                err = 100 * abs(ratio - tau_target) / tau_target\n                print(f\"  n={n}: tau={ratio:.4f}, error={err:.1f}%\")\n    \n    # Verdict\n    largest_gap_idx = np.argmax(gaps) if len(gaps) > 0 else 0\n    n_visible = largest_gap_idx + 1\n    \n    gap_43 = gaps[42] if len(gaps) > 42 else 0\n    gap_43_ratio = gap_43 / mean_gap if mean_gap > 0 else 0\n    \n    gap_35 = gaps[34] if len(gaps) > 34 else 0\n    gap_35_ratio = gap_35 / mean_gap if mean_gap > 0 else 0\n    \n    print(f\"\\n[VERDICT]\")\n    print(f\"  Largest gap at: {largest_gap_idx}->{largest_gap_idx+1}\")\n    print(f\"  Suggested n_visible: {n_visible}\")\n    print(f\"  Gap at 35: {gap_35_ratio:.2f}x mean\")\n    print(f\"  Gap at 43: {gap_43_ratio:.2f}x mean\")\n    \n    return {\n        'n_visible': int(n_visible),\n        'nonzero_count': int(nonzero),\n        'largest_gap_idx': int(largest_gap_idx),\n        'gap_35_ratio': float(gap_35_ratio),\n        'gap_43_ratio': float(gap_43_ratio),\n        'tau_best_n': int(best_n) if best_err < float('inf') else -1,\n        'tau_estimate': float(best_ratio) if best_err < float('inf') else 0.0,\n        'tau_error_pct': float(best_err) if best_err < float('inf') else -1.0,\n    }\n\n# Analyze\nanalysis = analyze_spectrum(yukawa['eigenvalues'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Save Outputs\n# @markdown Save all results: models, Yukawa tensor, metrics, samples.\n\nos.makedirs(config.output_dir, exist_ok=True)\nprint(f\"Saving outputs to: {config.output_dir}\")\n\n# 1. Yukawa tensor and spectrum\nnp.savez(f\"{config.output_dir}/yukawa.npz\",\n         Y=yukawa['Y'],\n         M=yukawa['M'],\n         eigenvalues=yukawa['eigenvalues'],\n         eigenvectors=yukawa['eigenvectors'])\nprint(\"  yukawa.npz\")\n\n# 2. Models\ntorch.save({\n    'h2': h2_model.state_dict(),\n    'h3': h3_model.state_dict(),\n    'config': asdict(config),\n}, f\"{config.output_dir}/models.pt\")\nprint(\"  models.pt\")\n\n# 3. Metrics JSON\nis_43 = 41 <= analysis['n_visible'] <= 45\nis_35 = 33 <= analysis['n_visible'] <= 37\ntau_ok = 0 < analysis['tau_error_pct'] < 15\n\ndet_g_mean = torch.det(data['metric']).mean().item()\n\nmetrics = {\n    'version': '2.0',\n    'tcs_structure': True,\n    'geometry': {\n        'det_g_mean': float(det_g_mean),\n        'det_g_target': float(config.target_det_g),\n        'det_g_error_pct': float(100 * abs(det_g_mean - config.target_det_g) / config.target_det_g),\n    },\n    'training': {\n        'h2_epochs': int(config.n_epochs_h2),\n        'h3_epochs': int(config.n_epochs_h3),\n        'h2_final_loss': float(h2_losses['total'][-1]) if h2_losses['total'] else None,\n        'h3_final_loss': float(h3_losses['total'][-1]) if h3_losses['total'] else None,\n    },\n    'tcs_modes': {\n        'n_local': int(config.b3_local),\n        'n_left': int(config.n_left),\n        'n_right': int(config.n_right),\n        'n_neck': int(config.n_neck),\n    },\n    'yukawa': {\n        'n_visible': int(analysis['n_visible']),\n        'nonzero_count': int(analysis['nonzero_count']),\n        'largest_gap_idx': int(analysis['largest_gap_idx']),\n        'gap_35_ratio': float(analysis['gap_35_ratio']),\n        'gap_43_ratio': float(analysis['gap_43_ratio']),\n        'tau_best_n': int(analysis['tau_best_n']),\n        'tau_estimate': float(analysis['tau_estimate']),\n        'tau_target': float(config.tau_target),\n        'tau_error_pct': float(analysis['tau_error_pct']) if analysis['tau_error_pct'] >= 0 else None,\n    },\n    'verdict': {\n        '43_77_structure': bool(is_43),\n        '35_42_structure': bool(is_35),\n        'tau_emerged': bool(tau_ok),\n    },\n}\n\nwith open(f\"{config.output_dir}/metrics.json\", 'w') as f:\n    json.dump(metrics, f, indent=2)\nprint(\"  metrics.json\")\n\n# 4. Eigenvalues CSV\nwith open(f\"{config.output_dir}/eigenvalues.csv\", 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['index', 'eigenvalue', 'cumulative', 'gap'])\n    cumsum = np.cumsum(yukawa['eigenvalues'])\n    gaps = np.abs(np.diff(yukawa['eigenvalues']))\n    for i, ev in enumerate(yukawa['eigenvalues']):\n        g = float(gaps[i]) if i < len(gaps) else 0.0\n        writer.writerow([i, float(ev), float(cumsum[i]), g])\nprint(\"  eigenvalues.csv\")\n\n# 5. Samples with forms\nwith torch.no_grad():\n    sample_coords = data['coords'][:1000].to(device)\n    sample_omega = h2_model(sample_coords).cpu().numpy()\n    sample_Phi = h3_model(sample_coords).cpu().numpy()\n\nnp.savez(f\"{config.output_dir}/samples.npz\",\n         coords=data['coords'][:1000].numpy(),\n         metric=data['metric'][:1000].numpy(),\n         phi=data['phi'][:1000].numpy(),\n         omega=sample_omega,\n         Phi=sample_Phi)\nprint(\"  samples.npz\")\n\nprint(\"\\nAll outputs saved.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Final Summary\n# @markdown Complete report of v2.0 TCS Hodge training results.\n\nprint(\"=\" * 70)\nprint(\"K7 TCS HODGE v2.0 - FINAL REPORT\")\nprint(\"=\" * 70)\n\nprint(f\"\\n[GEOMETRY]\")\nprint(f\"  det(g): {metrics['geometry']['det_g_mean']:.6f} (target: {metrics['geometry']['det_g_target']}, error: {metrics['geometry']['det_g_error_pct']:.2f}%)\")\n\nprint(f\"\\n[TCS MODE STRUCTURE]\")\nprint(f\"  Local (fiber): {metrics['tcs_modes']['n_local']} modes\")\nprint(f\"  Left-weighted: {metrics['tcs_modes']['n_left']} modes\")\nprint(f\"  Right-weighted: {metrics['tcs_modes']['n_right']} modes\")\nprint(f\"  Neck-coupled: {metrics['tcs_modes']['n_neck']} modes\")\nprint(f\"  Total: {config.b3_K7} modes\")\n\nprint(f\"\\n[TRAINING]\")\nprint(f\"  H2: {metrics['training']['h2_epochs']} epochs, final loss: {metrics['training']['h2_final_loss']:.2e}\")\nprint(f\"  H3: {metrics['training']['h3_epochs']} epochs, final loss: {metrics['training']['h3_final_loss']:.2e}\")\n\nprint(f\"\\n[YUKAWA SPECTRUM]\")\nprint(f\"  Non-zero eigenvalues: {metrics['yukawa']['nonzero_count']}\")\nprint(f\"  Largest gap at: {metrics['yukawa']['largest_gap_idx']}->{metrics['yukawa']['largest_gap_idx']+1}\")\nprint(f\"  Suggested n_visible: {metrics['yukawa']['n_visible']}\")\nprint(f\"  Gap at 35: {metrics['yukawa']['gap_35_ratio']:.2f}x mean\")\nprint(f\"  Gap at 43: {metrics['yukawa']['gap_43_ratio']:.2f}x mean\")\n\nprint(f\"\\n[TAU PARAMETER]\")\nprint(f\"  Target: {metrics['yukawa']['tau_target']:.4f} (3472/891)\")\nif metrics['yukawa']['tau_error_pct'] is not None and metrics['yukawa']['tau_error_pct'] >= 0:\n    print(f\"  Best match at n={metrics['yukawa']['tau_best_n']}: tau={metrics['yukawa']['tau_estimate']:.4f}\")\n    print(f\"  Error: {metrics['yukawa']['tau_error_pct']:.1f}%\")\nelse:\n    print(f\"  Could not compute tau (hidden sum ~ 0)\")\n\nprint(f\"\\n[VERDICT]\")\nv = metrics['verdict']\nprint(f\"  43/77 visible/hidden structure: {'YES' if v['43_77_structure'] else 'NO'}\")\nprint(f\"  35/42 local/global structure: {'YES' if v['35_42_structure'] else 'NO'}\")\nprint(f\"  Tau = 3472/891 emerged: {'YES' if v['tau_emerged'] else 'NO'}\")\n\nprint(f\"\\n[OUTPUT FILES]\")\nprint(f\"  {config.output_dir}/models.pt\")\nprint(f\"  {config.output_dir}/yukawa.npz\")\nprint(f\"  {config.output_dir}/metrics.json\")\nprint(f\"  {config.output_dir}/eigenvalues.csv\")\nprint(f\"  {config.output_dir}/samples.npz\")\n\nprint(\"=\" * 70)\n\n# Success criteria\nsuccess = v['43_77_structure'] or v['35_42_structure'] or v['tau_emerged']\nif success:\n    print(\"\\nTCS structure has influence on Yukawa spectrum.\")\nelse:\n    print(\"\\nTCS structure not yet visible. May need more training or parameter tuning.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}