{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K7 Hodge Pure v1.9b - Harmonic Forms with Proper Wedge\n",
    "\n",
    "**Fixes from v1.9:**\n",
    "- Proper wedge product (105 non-zero Levi-Civita terms)\n",
    "- Text-based training output (no matplotlib refresh)\n",
    "- Correct JSON serialization\n",
    "- Tau division by zero handling\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Train 21 harmonic 2-forms (H2)\n",
    "2. Train 77 harmonic 3-forms (H3: 35 local + 42 global)\n",
    "3. Compute proper Yukawa tensor with correct wedge\n",
    "4. Check if 43/77 or 35/42 structure emerges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup and Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Geometry\n",
    "    dim: int = 7\n",
    "    b2_K7: int = 21\n",
    "    b3_K7: int = 77\n",
    "    b3_local: int = 35\n",
    "    b3_global: int = 42\n",
    "    \n",
    "    # Targets\n",
    "    target_det_g: float = 2.03125\n",
    "    target_kappa_T: float = 0.01639344262295082\n",
    "    tau_target: float = 3.8967452300785634\n",
    "    \n",
    "    # Network\n",
    "    hidden_dim: int = 256\n",
    "    n_layers: int = 4\n",
    "    \n",
    "    # Training\n",
    "    n_epochs_h2: int = 3000\n",
    "    n_epochs_h3: int = 5000\n",
    "    lr_h2: float = 1e-3\n",
    "    lr_h3: float = 5e-4\n",
    "    batch_size: int = 2048\n",
    "    weight_decay: float = 1e-5\n",
    "    max_grad_norm: float = 1.0\n",
    "    scheduler_patience: int = 300\n",
    "    scheduler_factor: float = 0.5\n",
    "    \n",
    "    # Loss weights\n",
    "    w_closed: float = 1.0\n",
    "    w_coclosed: float = 1.0\n",
    "    w_orthonormal: float = 0.1\n",
    "    w_g2_compat: float = 0.5\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_every: int = 500\n",
    "    checkpoint_dir: str = \"checkpoints_v1_9b\"\n",
    "    log_every: int = 100\n",
    "    output_dir: str = \"outputs_v1_9b\"\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  H2 epochs: {config.n_epochs_h2}, H3 epochs: {config.n_epochs_h3}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Data\n",
    "\n",
    "def load_data(path: Optional[str] = None) -> Dict[str, torch.Tensor]:\n",
    "    paths_to_try = [\n",
    "        path, \"samples.npz\", \"../1_8/samples.npz\", \"/content/samples.npz\",\n",
    "        \"/content/drive/MyDrive/GIFT/G2_ML/1_8/samples.npz\",\n",
    "    ]\n",
    "    \n",
    "    for p in paths_to_try:\n",
    "        if p and os.path.exists(p):\n",
    "            print(f\"Loading from {p}\")\n",
    "            data = np.load(p)\n",
    "            return {\n",
    "                'coords': torch.from_numpy(data['coords']).float(),\n",
    "                'metric': torch.from_numpy(data['metric']).float(),\n",
    "                'phi': torch.from_numpy(data['phi']).float(),\n",
    "            }\n",
    "    \n",
    "    print(\"Data not found, generating synthetic...\")\n",
    "    n = 5000\n",
    "    coords = torch.rand(n, 7) * 2 * np.pi\n",
    "    metric = torch.eye(7).unsqueeze(0).expand(n, -1, -1).clone() * (config.target_det_g ** (1/7))\n",
    "    metric = metric + 0.01 * torch.randn(n, 7, 7)\n",
    "    metric = 0.5 * (metric + metric.transpose(-1, -2))\n",
    "    phi = torch.randn(n, 35) * 0.5\n",
    "    phi = phi * np.sqrt(7.0) / (torch.norm(phi, dim=1, keepdim=True) + 1e-8)\n",
    "    return {'coords': coords, 'metric': metric, 'phi': phi}\n",
    "\n",
    "data = load_data()\n",
    "n_samples = data['coords'].shape[0]\n",
    "det_g_mean = torch.det(data['metric']).mean().item()\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"det(g) mean: {det_g_mean:.6f} (target: {config.target_det_g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Networks\n",
    "\n",
    "class H2Network(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = config.dim\n",
    "        for _ in range(config.n_layers):\n",
    "            layers.extend([nn.Linear(in_dim, config.hidden_dim), nn.SiLU()])\n",
    "            in_dim = config.hidden_dim\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.heads = nn.ModuleList([nn.Linear(config.hidden_dim, 21) for _ in range(21)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        return torch.stack([h(f) for h in self.heads], dim=1)\n",
    "\n",
    "\n",
    "class H3Network(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = config.dim\n",
    "        for _ in range(config.n_layers):\n",
    "            layers.extend([nn.Linear(in_dim, config.hidden_dim), nn.SiLU()])\n",
    "            in_dim = config.hidden_dim\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.local_heads = nn.ModuleList([nn.Linear(config.hidden_dim, 35) for _ in range(35)])\n",
    "        self.global_neck = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim + 1, config.hidden_dim), nn.SiLU(),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "        )\n",
    "        self.global_heads = nn.ModuleList([nn.Linear(config.hidden_dim, 35) for _ in range(42)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lam = x[:, 0:1]\n",
    "        f = self.features(x)\n",
    "        local_out = torch.stack([h(f) for h in self.local_heads], dim=1)\n",
    "        gf = self.global_neck(torch.cat([f, lam], dim=-1))\n",
    "        global_out = torch.stack([h(gf) for h in self.global_heads], dim=1)\n",
    "        return torch.cat([local_out, global_out], dim=1)\n",
    "\n",
    "print(\"Networks defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Loss Functions\n",
    "\n",
    "def gram_matrix(forms, metric):\n",
    "    det_g = torch.det(metric)\n",
    "    vol = torch.sqrt(det_g.abs()).unsqueeze(-1).unsqueeze(-1)\n",
    "    weighted = forms * vol\n",
    "    return torch.einsum('bic,bjc->ij', weighted, forms) / forms.shape[0]\n",
    "\n",
    "def orthonormality_loss(G):\n",
    "    I = torch.eye(G.shape[0], device=G.device)\n",
    "    return torch.mean((G - I) ** 2)\n",
    "\n",
    "def closedness_loss_fd(x, model, eps=1e-4):\n",
    "    omega = model(x)\n",
    "    total = torch.tensor(0.0, device=x.device)\n",
    "    for c in range(7):\n",
    "        x_p, x_m = x.clone(), x.clone()\n",
    "        x_p[:, c] += eps\n",
    "        x_m[:, c] -= eps\n",
    "        grad = (model(x_p) - model(x_m)) / (2 * eps)\n",
    "        total += torch.mean(grad ** 2)\n",
    "    return total / 7\n",
    "\n",
    "def g2_compatibility_loss(Phi, phi_ref):\n",
    "    diag = Phi[:, :35, :].diagonal(dim1=1, dim2=2)\n",
    "    return torch.mean((diag - phi_ref) ** 2)\n",
    "\n",
    "print(\"Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Checkpointing\n",
    "\n",
    "def save_checkpoint(path, epoch, model, optimizer, scheduler, losses, best_loss, phase):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'losses': losses, 'best_loss': best_loss, 'phase': phase,\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scheduler=None):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    if scheduler and ckpt['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "    print(f\"Resumed from epoch {ckpt['epoch']}\")\n",
    "    return ckpt\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir, phase):\n",
    "    pattern = f\"{phase}_epoch_*.pt\"\n",
    "    ckpts = sorted(Path(checkpoint_dir).glob(pattern)) if os.path.exists(checkpoint_dir) else []\n",
    "    return str(ckpts[-1]) if ckpts else None\n",
    "\n",
    "print(\"Checkpointing ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 1: Train H2\n",
    "\n",
    "def train_h2(config, data, resume=True):\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 1: Training H2 (21 harmonic 2-forms)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Epoch':>6} | {'Loss':>10} | {'Ortho':>10} | {'Closed':>10} | {'Best':>10}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    model = H2Network(config).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr_h2, weight_decay=config.weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=config.scheduler_patience, factor=config.scheduler_factor)\n",
    "    \n",
    "    coords = data['coords'].to(device)\n",
    "    metric = data['metric'].to(device)\n",
    "    n = coords.shape[0]\n",
    "    \n",
    "    start_epoch, best_loss = 0, float('inf')\n",
    "    all_losses = {'total': [], 'ortho': [], 'closed': []}\n",
    "    \n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    latest = find_latest_checkpoint(config.checkpoint_dir, 'h2')\n",
    "    if resume and latest:\n",
    "        ckpt = load_checkpoint(latest, model, optimizer, scheduler)\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        best_loss = ckpt['best_loss']\n",
    "        all_losses = ckpt.get('losses', all_losses)\n",
    "    \n",
    "    best_state = model.state_dict().copy()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.n_epochs_h2):\n",
    "        model.train()\n",
    "        idx = torch.randperm(n)[:config.batch_size]\n",
    "        x, g = coords[idx], metric[idx]\n",
    "        \n",
    "        omega = model(x)\n",
    "        G = gram_matrix(omega, g)\n",
    "        loss_ortho = orthonormality_loss(G)\n",
    "        loss_closed = closedness_loss_fd(x, model)\n",
    "        total = config.w_orthonormal * loss_ortho + config.w_closed * loss_closed\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step(total)\n",
    "        \n",
    "        all_losses['total'].append(total.item())\n",
    "        all_losses['ortho'].append(loss_ortho.item())\n",
    "        all_losses['closed'].append(loss_closed.item())\n",
    "        \n",
    "        if total.item() < best_loss:\n",
    "            best_loss = total.item()\n",
    "            best_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % config.log_every == 0:\n",
    "            print(f\"{epoch+1:6d} | {total.item():10.2e} | {loss_ortho.item():10.2e} | {loss_closed.item():10.2e} | {best_loss:10.2e}\")\n",
    "        \n",
    "        if (epoch + 1) % config.checkpoint_every == 0:\n",
    "            save_checkpoint(f\"{config.checkpoint_dir}/h2_epoch_{epoch+1:05d}.pt\",\n",
    "                          epoch, model, optimizer, scheduler, all_losses, best_loss, 'h2')\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nH2 complete. Best loss: {best_loss:.2e}\")\n",
    "    return model, all_losses\n",
    "\n",
    "h2_model, h2_losses = train_h2(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 2: Train H3\n",
    "\n",
    "def train_h3(config, data, resume=True):\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 2: Training H3 (77 harmonic 3-forms)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Epoch':>6} | {'Loss':>10} | {'Ortho':>10} | {'Closed':>10} | {'G2':>10} | {'Best':>10}\")\n",
    "    print(\"-\"*75)\n",
    "    \n",
    "    model = H3Network(config).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr_h3, weight_decay=config.weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=config.scheduler_patience, factor=config.scheduler_factor)\n",
    "    \n",
    "    coords = data['coords'].to(device)\n",
    "    metric = data['metric'].to(device)\n",
    "    phi = data['phi'].to(device)\n",
    "    n = coords.shape[0]\n",
    "    \n",
    "    start_epoch, best_loss = 0, float('inf')\n",
    "    all_losses = {'total': [], 'ortho': [], 'closed': [], 'g2': []}\n",
    "    \n",
    "    latest = find_latest_checkpoint(config.checkpoint_dir, 'h3')\n",
    "    if resume and latest:\n",
    "        ckpt = load_checkpoint(latest, model, optimizer, scheduler)\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        best_loss = ckpt['best_loss']\n",
    "        all_losses = ckpt.get('losses', all_losses)\n",
    "    \n",
    "    best_state = model.state_dict().copy()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.n_epochs_h3):\n",
    "        model.train()\n",
    "        idx = torch.randperm(n)[:config.batch_size]\n",
    "        x, g, p = coords[idx], metric[idx], phi[idx]\n",
    "        \n",
    "        Phi = model(x)\n",
    "        G = gram_matrix(Phi, g)\n",
    "        loss_ortho = orthonormality_loss(G)\n",
    "        loss_closed = closedness_loss_fd(x, model)\n",
    "        loss_g2 = g2_compatibility_loss(Phi, p)\n",
    "        total = config.w_orthonormal * loss_ortho + config.w_closed * loss_closed + config.w_g2_compat * loss_g2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step(total)\n",
    "        \n",
    "        for k, v in [('total', total), ('ortho', loss_ortho), ('closed', loss_closed), ('g2', loss_g2)]:\n",
    "            all_losses[k].append(v.item())\n",
    "        \n",
    "        if total.item() < best_loss:\n",
    "            best_loss = total.item()\n",
    "            best_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % config.log_every == 0:\n",
    "            print(f\"{epoch+1:6d} | {total.item():10.2e} | {loss_ortho.item():10.2e} | {loss_closed.item():10.2e} | {loss_g2.item():10.2e} | {best_loss:10.2e}\")\n",
    "        \n",
    "        if (epoch + 1) % config.checkpoint_every == 0:\n",
    "            save_checkpoint(f\"{config.checkpoint_dir}/h3_epoch_{epoch+1:05d}.pt\",\n",
    "                          epoch, model, optimizer, scheduler, all_losses, best_loss, 'h3')\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nH3 complete. Best loss: {best_loss:.2e}\")\n",
    "    return model, all_losses\n",
    "\n",
    "h3_model, h3_losses = train_h3(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Proper Wedge Product\n",
    "\n",
    "def levi_civita_7(indices):\n",
    "    if len(set(indices)) != 7:\n",
    "        return 0\n",
    "    inv = sum(1 for i in range(7) for j in range(i+1, 7) if indices[i] > indices[j])\n",
    "    return 1 if inv % 2 == 0 else -1\n",
    "\n",
    "def build_yukawa_coefficients():\n",
    "    pairs = list(combinations(range(7), 2))\n",
    "    triples = list(combinations(range(7), 3))\n",
    "    coeffs = []\n",
    "    for i1, p1 in enumerate(pairs):\n",
    "        for i2, p2 in enumerate(pairs):\n",
    "            if i2 < i1:\n",
    "                continue\n",
    "            for i3, t in enumerate(triples):\n",
    "                all_idx = p1 + p2 + t\n",
    "                if len(set(all_idx)) != 7:\n",
    "                    continue\n",
    "                sign = levi_civita_7(all_idx)\n",
    "                if sign != 0:\n",
    "                    coeffs.append((i1, i2, i3, sign))\n",
    "    print(f\"Built {len(coeffs)} Yukawa coefficients\")\n",
    "    return coeffs\n",
    "\n",
    "YUKAWA_COEFFS = build_yukawa_coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 3: Compute Yukawa\n",
    "\n",
    "def compute_yukawa(h2_model, h3_model, data, coeffs, n_pts=5000):\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: Computing Yukawa Tensor (proper wedge)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    h2_model.eval()\n",
    "    h3_model.eval()\n",
    "    \n",
    "    coords = data['coords'].to(device)\n",
    "    metric = data['metric'].to(device)\n",
    "    n = min(n_pts, coords.shape[0])\n",
    "    idx = torch.randperm(coords.shape[0])[:n]\n",
    "    x, g = coords[idx], metric[idx]\n",
    "    \n",
    "    det_g = torch.det(g)\n",
    "    vol = torch.sqrt(det_g.abs())\n",
    "    total_vol = vol.sum()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        omega = h2_model(x)  # (n, 21, 21)\n",
    "        Phi = h3_model(x)    # (n, 77, 35)\n",
    "    \n",
    "    print(f\"Integration points: {n}\")\n",
    "    print(f\"Computing Y_ijk...\")\n",
    "    \n",
    "    Y = torch.zeros(21, 21, 77, device=device)\n",
    "    \n",
    "    for a in range(21):\n",
    "        if (a + 1) % 5 == 0:\n",
    "            print(f\"  H2 mode {a+1}/21\")\n",
    "        for b in range(a, 21):\n",
    "            omega_a = omega[:, a, :]\n",
    "            omega_b = omega[:, b, :]\n",
    "            for c in range(77):\n",
    "                Phi_c = Phi[:, c, :]\n",
    "                integral = torch.zeros(n, device=device)\n",
    "                for i1, i2, i3, sign in coeffs:\n",
    "                    integral += sign * omega_a[:, i1] * omega_b[:, i2] * Phi_c[:, i3]\n",
    "                Y[a, b, c] = (integral * vol).sum() / total_vol\n",
    "                if a != b:\n",
    "                    Y[b, a, c] = -Y[a, b, c]\n",
    "    \n",
    "    print(\"Computing Gram matrix M = Y^T Y...\")\n",
    "    M = torch.einsum('ijk,ijl->kl', Y, Y)\n",
    "    \n",
    "    print(\"Eigendecomposition...\")\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(M)\n",
    "    idx_sort = torch.argsort(eigenvalues, descending=True)\n",
    "    eigenvalues = eigenvalues[idx_sort]\n",
    "    eigenvectors = eigenvectors[:, idx_sort]\n",
    "    \n",
    "    return {\n",
    "        'Y': Y.cpu().numpy(),\n",
    "        'M': M.cpu().numpy(),\n",
    "        'eigenvalues': eigenvalues.cpu().numpy(),\n",
    "        'eigenvectors': eigenvectors.cpu().numpy(),\n",
    "        'omega': omega.cpu().numpy(),\n",
    "        'Phi': Phi.cpu().numpy(),\n",
    "    }\n",
    "\n",
    "yukawa = compute_yukawa(h2_model, h3_model, data, YUKAWA_COEFFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Analyze Spectrum\n",
    "\n",
    "def analyze_spectrum(eigs, tau_target=3472/891):\n",
    "    print(\"=\"*70)\n",
    "    print(\"YUKAWA SPECTRAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Non-zero\n",
    "    nonzero = (np.abs(eigs) > 1e-10).sum()\n",
    "    print(f\"\\n[EIGENVALUES]\")\n",
    "    print(f\"  Total: {len(eigs)}, Non-zero: {nonzero}\")\n",
    "    print(f\"  Top 5: {eigs[:5].round(4)}\")\n",
    "    print(f\"  Around 35: {eigs[32:38].round(6)}\")\n",
    "    print(f\"  Around 43: {eigs[40:46]}\")\n",
    "    \n",
    "    # Gaps\n",
    "    gaps = np.abs(np.diff(eigs))\n",
    "    mean_gap = gaps.mean() if len(gaps) > 0 else 1.0\n",
    "    print(f\"\\n[TOP 5 GAPS]\")\n",
    "    for i, idx in enumerate(np.argsort(gaps)[::-1][:5]):\n",
    "        ratio = gaps[idx] / mean_gap if mean_gap > 0 else 0\n",
    "        print(f\"  #{i+1}: gap {idx}->{idx+1}: {gaps[idx]:.6f} ({ratio:.1f}x mean)\")\n",
    "    \n",
    "    # Key gaps\n",
    "    print(f\"\\n[KEY POSITIONS]\")\n",
    "    for pos in [20, 21, 34, 35, 42, 43]:\n",
    "        if pos < len(gaps):\n",
    "            print(f\"  Gap {pos}->{pos+1}: {gaps[pos]:.6f}\")\n",
    "    \n",
    "    # Cumulative\n",
    "    cumsum = np.cumsum(eigs)\n",
    "    total = eigs.sum()\n",
    "    print(f\"\\n[CUMULATIVE]\")\n",
    "    for n in [21, 35, 43, 77]:\n",
    "        pct = 100 * cumsum[n-1] / total if total > 0 else 0\n",
    "        print(f\"  First {n}: {pct:.1f}%\")\n",
    "    \n",
    "    # Tau\n",
    "    print(f\"\\n[TAU SEARCH] (target: {tau_target:.4f})\")\n",
    "    best_n, best_ratio, best_err = 0, 0, float('inf')\n",
    "    for n in range(20, 55):\n",
    "        if n < len(eigs):\n",
    "            hidden = total - cumsum[n-1]\n",
    "            if hidden > 1e-8:\n",
    "                ratio = cumsum[n-1] / hidden\n",
    "                err = 100 * abs(ratio - tau_target) / tau_target\n",
    "                if err < best_err:\n",
    "                    best_n, best_ratio, best_err = n, ratio, err\n",
    "    \n",
    "    if best_err < float('inf'):\n",
    "        print(f\"  Best: n={best_n}, ratio={best_ratio:.4f}, error={best_err:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  No valid tau (hidden sum ~ 0)\")\n",
    "    \n",
    "    # Suggested split\n",
    "    largest_gap_idx = np.argmax(gaps) if len(gaps) > 0 else 0\n",
    "    n_visible = largest_gap_idx + 1\n",
    "    gap_43 = gaps[42] if len(gaps) > 42 else 0\n",
    "    gap_43_ratio = gap_43 / mean_gap if mean_gap > 0 else 0\n",
    "    \n",
    "    print(f\"\\n[VERDICT]\")\n",
    "    print(f\"  Suggested n_visible: {n_visible}\")\n",
    "    print(f\"  Gap at 43: {gap_43_ratio:.2f}x mean\")\n",
    "    \n",
    "    return {\n",
    "        'n_visible': int(n_visible),\n",
    "        'nonzero_count': int(nonzero),\n",
    "        'gap_43_ratio': float(gap_43_ratio),\n",
    "        'tau_estimate': float(best_ratio) if best_err < float('inf') else 0.0,\n",
    "        'tau_error_pct': float(best_err) if best_err < float('inf') else -1.0,\n",
    "    }\n",
    "\n",
    "analysis = analyze_spectrum(yukawa['eigenvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize Spectrum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eigs = yukawa['eigenvalues']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Spectrum\n",
    "ax = axes[0, 0]\n",
    "ax.semilogy(np.abs(eigs) + 1e-15)\n",
    "ax.axvline(x=34, color='r', ls='--', label='35 boundary')\n",
    "ax.axvline(x=42, color='g', ls=':', label='43 boundary')\n",
    "ax.set_xlabel('Mode'); ax.set_ylabel('|Eigenvalue|')\n",
    "ax.set_title('Yukawa Gram Spectrum'); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gaps\n",
    "ax = axes[0, 1]\n",
    "gaps = np.abs(np.diff(eigs))\n",
    "ax.bar(range(len(gaps)), gaps)\n",
    "ax.axvline(x=34, color='r', ls='--'); ax.axvline(x=42, color='g', ls=':')\n",
    "ax.set_xlabel('Gap index'); ax.set_ylabel('Gap'); ax.set_title('Eigenvalue Gaps')\n",
    "\n",
    "# Cumulative\n",
    "ax = axes[1, 0]\n",
    "cumsum = np.cumsum(eigs)\n",
    "total = eigs.sum() if eigs.sum() > 0 else 1\n",
    "ax.plot(100 * cumsum / total)\n",
    "ax.axhline(y=100*35/77, color='r', ls='--', label='35/77')\n",
    "ax.axhline(y=100*43/77, color='g', ls=':', label='43/77')\n",
    "ax.set_xlabel('Mode'); ax.set_ylabel('Cumulative %'); ax.set_title('Cumulative Variance')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# First 40 detailed\n",
    "ax = axes[1, 1]\n",
    "ax.bar(range(min(40, len(eigs))), eigs[:40])\n",
    "ax.set_xlabel('Mode'); ax.set_ylabel('Eigenvalue'); ax.set_title('Top 40 Eigenvalues')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "plt.savefig(f\"{config.output_dir}/yukawa_spectrum.png\", dpi=150)\n",
    "plt.show()\n",
    "print(f\"Saved: {config.output_dir}/yukawa_spectrum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Outputs\n",
    "\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "print(\"Saving outputs...\")\n",
    "\n",
    "# NPZ\n",
    "np.savez(f\"{config.output_dir}/yukawa.npz\",\n",
    "         Y=yukawa['Y'], M=yukawa['M'],\n",
    "         eigenvalues=yukawa['eigenvalues'], eigenvectors=yukawa['eigenvectors'])\n",
    "print(f\"  yukawa.npz\")\n",
    "\n",
    "# Models\n",
    "torch.save({'h2': h2_model.state_dict(), 'h3': h3_model.state_dict()}, f\"{config.output_dir}/models.pt\")\n",
    "print(f\"  models.pt\")\n",
    "\n",
    "# Metrics JSON\n",
    "is_43 = 41 <= analysis['n_visible'] <= 45\n",
    "is_35 = 33 <= analysis['n_visible'] <= 37\n",
    "tau_ok = 0 < analysis['tau_error_pct'] < 10\n",
    "\n",
    "metrics = {\n",
    "    'geometry': {'det_g_mean': float(torch.det(data['metric']).mean().item()), 'det_g_target': float(config.target_det_g)},\n",
    "    'training': {\n",
    "        'h2_epochs': int(config.n_epochs_h2), 'h3_epochs': int(config.n_epochs_h3),\n",
    "        'h2_final_loss': float(h2_losses['total'][-1]) if h2_losses['total'] else None,\n",
    "        'h3_final_loss': float(h3_losses['total'][-1]) if h3_losses['total'] else None,\n",
    "    },\n",
    "    'yukawa': {\n",
    "        'n_visible': int(analysis['n_visible']),\n",
    "        'nonzero_count': int(analysis['nonzero_count']),\n",
    "        'gap_43_ratio': float(analysis['gap_43_ratio']),\n",
    "        'tau_estimate': float(analysis['tau_estimate']),\n",
    "        'tau_error_pct': float(analysis['tau_error_pct']) if analysis['tau_error_pct'] >= 0 else None,\n",
    "    },\n",
    "    'verdict': {'43_77_confirmed': bool(is_43), '35_42_confirmed': bool(is_35), 'tau_emerged': bool(tau_ok)},\n",
    "}\n",
    "with open(f\"{config.output_dir}/final_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"  final_metrics.json\")\n",
    "\n",
    "# CSV\n",
    "with open(f\"{config.output_dir}/eigenvalues.csv\", 'w', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['index', 'eigenvalue', 'cumulative', 'gap'])\n",
    "    cs = np.cumsum(yukawa['eigenvalues'])\n",
    "    gaps = np.abs(np.diff(yukawa['eigenvalues']))\n",
    "    for i, ev in enumerate(yukawa['eigenvalues']):\n",
    "        w.writerow([i, float(ev), float(cs[i]), float(gaps[i]) if i < len(gaps) else 0.0])\n",
    "print(f\"  eigenvalues.csv\")\n",
    "\n",
    "# Samples\n",
    "with torch.no_grad():\n",
    "    c = data['coords'][:1000].to(device)\n",
    "    o, P = h2_model(c), h3_model(c)\n",
    "np.savez(f\"{config.output_dir}/samples.npz\",\n",
    "         coords=data['coords'][:1000].numpy(), metric=data['metric'][:1000].numpy(),\n",
    "         phi=data['phi'][:1000].numpy(), omega=o.cpu().numpy(), Phi=P.cpu().numpy())\n",
    "print(f\"  samples.npz\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Final Summary\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"V1.9b HODGE PURE - FINAL REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n[GEOMETRY]\")\n",
    "print(f\"  det(g): {metrics['geometry']['det_g_mean']:.6f} (target: {metrics['geometry']['det_g_target']})\")\n",
    "print(f\"\\n[TRAINING]\")\n",
    "print(f\"  H2: {metrics['training']['h2_epochs']} epochs, final loss: {metrics['training']['h2_final_loss']:.2e}\")\n",
    "print(f\"  H3: {metrics['training']['h3_epochs']} epochs, final loss: {metrics['training']['h3_final_loss']:.2e}\")\n",
    "print(f\"\\n[YUKAWA SPECTRUM]\")\n",
    "print(f\"  Non-zero eigenvalues: {metrics['yukawa']['nonzero_count']}\")\n",
    "print(f\"  Suggested n_visible: {metrics['yukawa']['n_visible']}\")\n",
    "print(f\"  Gap at 43: {metrics['yukawa']['gap_43_ratio']:.2f}x mean\")\n",
    "if metrics['yukawa']['tau_error_pct'] is not None:\n",
    "    print(f\"  Tau estimate: {metrics['yukawa']['tau_estimate']:.4f} (error: {metrics['yukawa']['tau_error_pct']:.1f}%)\")\n",
    "else:\n",
    "    print(f\"  Tau: N/A (hidden sum ~ 0)\")\n",
    "print(f\"\\n[VERDICT]\")\n",
    "print(f\"  43/77 structure: {'YES' if metrics['verdict']['43_77_confirmed'] else 'NO'}\")\n",
    "print(f\"  35/42 structure: {'YES' if metrics['verdict']['35_42_confirmed'] else 'NO'}\")\n",
    "print(f\"  Tau emerged: {'YES' if metrics['verdict']['tau_emerged'] else 'NO'}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {\"gpuType\": \"T4\"},
  "kernelspec": {\"display_name\": \"Python 3\", \"name\": \"python3\"},
  "language_info": {\"name\": \"python\", \"version\": \"3.10.12\"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
