{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GIFT v2.2 - Variational G2 Metric Learning\n",
        "\n",
        "**Simplified approach**: Learn the metric g directly (not via phi extraction).\n",
        "\n",
        "## GIFT v2.2 Constraints\n",
        "\n",
        "| Constraint | Value | Origin |\n",
        "|------------|-------|--------|\n",
        "| det(g) | 65/32 = 2.03125 | From h* = 99 |\n",
        "| kappa_T | 1/61 | Torsion magnitude |\n",
        "| g > 0 | Positive definite | Riemannian |\n",
        "| Smooth | Low gradient energy | Variational |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GIFT v2.2 targets\n",
        "DET_TARGET = 65/32      # = 2.03125\n",
        "KAPPA_TARGET = 1/61     # ~ 0.01639\n",
        "DIM = 7\n",
        "\n",
        "print(f\"GIFT v2.2 Targets:\")\n",
        "print(f\"  det(g) = {DET_TARGET} = 65/32\")\n",
        "print(f\"  kappa_T = {KAPPA_TARGET:.6f} = 1/61\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MetricNetwork(nn.Module):\n",
        "    \"\"\"Learn a 7x7 SPD metric field g(x).\n",
        "    \n",
        "    Uses Cholesky parametrization: g = L @ L^T\n",
        "    where L is lower triangular with positive diagonal.\n",
        "    This guarantees g is symmetric positive definite.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, hidden=256, n_layers=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Fourier features\n",
        "        self.B = nn.Parameter(torch.randn(DIM, 64) * 2.0, requires_grad=False)\n",
        "        \n",
        "        # MLP\n",
        "        layers = []\n",
        "        in_dim = DIM + 128  # coords + fourier\n",
        "        for _ in range(n_layers):\n",
        "            layers.extend([nn.Linear(in_dim, hidden), nn.SiLU()])\n",
        "            in_dim = hidden\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "        \n",
        "        # Output: 28 components for lower triangular L\n",
        "        # 7 diagonal + 21 off-diagonal\n",
        "        self.L_head = nn.Linear(hidden, 28)\n",
        "        \n",
        "        # Initialize near identity\n",
        "        nn.init.zeros_(self.L_head.weight)\n",
        "        # Bias: L = I gives g = I, det = 1\n",
        "        # We want det = 2.03125, so scale L by 2.03125^(1/14) ~ 1.052\n",
        "        scale = DET_TARGET ** (1/14)\n",
        "        bias = torch.zeros(28)\n",
        "        bias[:7] = math.log(scale)  # log for softplus\n",
        "        self.L_head.bias = nn.Parameter(bias)\n",
        "        \n",
        "        # Indices for building L matrix\n",
        "        self.register_buffer('tril_indices', torch.tril_indices(7, 7))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch = x.shape[0]\n",
        "        \n",
        "        # Fourier features\n",
        "        proj = 2 * math.pi * x @ self.B\n",
        "        fourier = torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)\n",
        "        \n",
        "        # MLP\n",
        "        h = self.backbone(torch.cat([x, fourier], dim=-1))\n",
        "        L_flat = self.L_head(h)\n",
        "        \n",
        "        # Build lower triangular L\n",
        "        L = torch.zeros(batch, 7, 7, device=x.device)\n",
        "        L[:, self.tril_indices[0], self.tril_indices[1]] = L_flat\n",
        "        \n",
        "        # Positive diagonal via softplus\n",
        "        diag_idx = torch.arange(7, device=x.device)\n",
        "        L[:, diag_idx, diag_idx] = torch.nn.functional.softplus(L[:, diag_idx, diag_idx]) + 0.1\n",
        "        \n",
        "        # g = L @ L^T (SPD by construction)\n",
        "        g = L @ L.transpose(-1, -2)\n",
        "        \n",
        "        return g, L\n",
        "\n",
        "# Test\n",
        "model = MetricNetwork().to(device)\n",
        "x_test = torch.randn(100, 7, device=device)\n",
        "g_test, L_test = model(x_test)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"g shape: {g_test.shape}\")\n",
        "print(f\"det(g) mean: {torch.det(g_test).mean().item():.4f}\")\n",
        "print(f\"min eigenvalue: {torch.linalg.eigvalsh(g_test).min().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_losses(model, x):\n",
        "    \"\"\"Compute all loss components.\"\"\"\n",
        "    g, L = model(x)\n",
        "    batch = x.shape[0]\n",
        "    \n",
        "    # 1. Determinant constraint: det(g) = 65/32\n",
        "    # det(g) = det(L)^2, det(L) = prod(diagonal)\n",
        "    diag = torch.diagonal(L, dim1=-2, dim2=-1)\n",
        "    log_det_L = torch.log(diag).sum(dim=-1)\n",
        "    log_det_g = 2 * log_det_L\n",
        "    det_g = torch.exp(log_det_g)\n",
        "    \n",
        "    det_loss = ((det_g - DET_TARGET) ** 2).mean()\n",
        "    \n",
        "    # 2. Smoothness: penalize spatial gradients of g\n",
        "    eps = 1e-3\n",
        "    grad_loss = torch.zeros(1, device=x.device)\n",
        "    for i in range(DIM):\n",
        "        x_p = x.clone()\n",
        "        x_p[:, i] += eps\n",
        "        g_p, _ = model(x_p)\n",
        "        grad_loss = grad_loss + ((g_p - g) ** 2).mean()\n",
        "    grad_loss = grad_loss / DIM\n",
        "    \n",
        "    # 3. Torsion proxy: variation of metric ~ kappa_T\n",
        "    # Simplified: we want some non-zero but small variation\n",
        "    torsion = torch.sqrt(grad_loss + 1e-10)\n",
        "    torsion_loss = ((torsion - KAPPA_TARGET) ** 2)\n",
        "    \n",
        "    return {\n",
        "        'det': det_loss,\n",
        "        'det_val': det_g.mean().item(),\n",
        "        'smooth': grad_loss.squeeze(),\n",
        "        'torsion': torsion_loss.squeeze(),\n",
        "        'torsion_val': torsion.item(),\n",
        "        'min_eig': torch.linalg.eigvalsh(g).min().item(),\n",
        "    }\n",
        "\n",
        "# Test\n",
        "losses = compute_losses(model, x_test)\n",
        "print(\"Initial losses:\")\n",
        "for k, v in losses.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        print(f\"  {k}: {v.item():.6f}\")\n",
        "    else:\n",
        "        print(f\"  {k}: {v:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, n_epochs=3000, batch_size=1024, lr=1e-3):\n",
        "    \"\"\"Train with phased loss weights.\"\"\"\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
        "    \n",
        "    history = {'loss': [], 'det': [], 'torsion': []}\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        # Sample random coordinates in [-1, 1]^7\n",
        "        x = 2 * torch.rand(batch_size, DIM, device=device) - 1\n",
        "        \n",
        "        losses = compute_losses(model, x)\n",
        "        \n",
        "        # Phase-dependent weights\n",
        "        if epoch < n_epochs // 3:\n",
        "            # Phase 1: Focus on det constraint\n",
        "            w_det, w_smooth, w_torsion = 10.0, 0.1, 0.1\n",
        "        elif epoch < 2 * n_epochs // 3:\n",
        "            # Phase 2: Balance det and smoothness\n",
        "            w_det, w_smooth, w_torsion = 5.0, 1.0, 1.0\n",
        "        else:\n",
        "            # Phase 3: Fine-tune torsion\n",
        "            w_det, w_smooth, w_torsion = 3.0, 0.5, 3.0\n",
        "        \n",
        "        total_loss = (w_det * losses['det'] + \n",
        "                      w_smooth * losses['smooth'] + \n",
        "                      w_torsion * losses['torsion'])\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        history['loss'].append(total_loss.item())\n",
        "        history['det'].append(losses['det_val'])\n",
        "        history['torsion'].append(losses['torsion_val'])\n",
        "        \n",
        "        if epoch % 500 == 0:\n",
        "            det_err = 100 * abs(losses['det_val'] - DET_TARGET) / DET_TARGET\n",
        "            print(f\"[{epoch:4d}] L={total_loss.item():.4f} | \"\n",
        "                  f\"det={losses['det_val']:.4f} ({det_err:.1f}%) | \"\n",
        "                  f\"kappa={losses['torsion_val']:.4f} | \"\n",
        "                  f\"min_eig={losses['min_eig']:.4f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "print(\"Training function ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train!\n",
        "print(\"=\"*60)\n",
        "print(\"GIFT v2.2 Metric Learning\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Target: det(g) = {DET_TARGET}\")\n",
        "\n",
        "model = MetricNetwork().to(device)\n",
        "history = train(model, n_epochs=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "axes[0].semilogy(history['loss'])\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Total Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history['det'])\n",
        "axes[1].axhline(DET_TARGET, color='r', linestyle='--', label=f'Target: {DET_TARGET}')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('det(g)')\n",
        "axes[1].set_title('Determinant')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(history['torsion'])\n",
        "axes[2].axhline(KAPPA_TARGET, color='r', linestyle='--', label=f'Target: 1/61')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Torsion proxy')\n",
        "axes[2].set_title('Torsion')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation\n",
        "model.eval()\n",
        "n_val = 5000\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = 2 * torch.rand(n_val, DIM, device=device) - 1\n",
        "    g, L = model(x)\n",
        "    \n",
        "    # Compute det via L diagonal\n",
        "    diag = torch.diagonal(L, dim1=-2, dim2=-1)\n",
        "    det_g = (diag ** 2).prod(dim=-1)\n",
        "    \n",
        "    # Eigenvalues\n",
        "    eigs = torch.linalg.eigvalsh(g)\n",
        "\n",
        "det_np = det_g.cpu().numpy()\n",
        "min_eig = eigs[:, 0].cpu().numpy()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\")\n",
        "print(f\"det(g):\")\n",
        "print(f\"  Mean:   {det_np.mean():.6f}\")\n",
        "print(f\"  Std:    {det_np.std():.6f}\")\n",
        "print(f\"  Target: {DET_TARGET:.6f}\")\n",
        "print(f\"  Error:  {100*abs(det_np.mean() - DET_TARGET)/DET_TARGET:.2f}%\")\n",
        "print(f\"\")\n",
        "print(f\"Positivity (g > 0):\")\n",
        "print(f\"  Min eigenvalue: {min_eig.mean():.6f} +/- {min_eig.std():.6f}\")\n",
        "print(f\"  All positive:   {100*(min_eig > 0).mean():.1f}%\")\n",
        "print(f\"\")\n",
        "print(f\"Eigenvalue spectrum:\")\n",
        "for i in range(7):\n",
        "    e = eigs[:, i].cpu().numpy()\n",
        "    print(f\"  lambda_{i}: {e.mean():.4f} +/- {e.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].hist(det_np, bins=50, density=True, alpha=0.7, color='blue')\n",
        "axes[0].axvline(DET_TARGET, color='r', linestyle='--', linewidth=2, label=f'Target: {DET_TARGET}')\n",
        "axes[0].axvline(det_np.mean(), color='g', linestyle='--', linewidth=2, label=f'Mean: {det_np.mean():.4f}')\n",
        "axes[0].set_xlabel('det(g)')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Metric Determinant Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(min_eig, bins=50, density=True, alpha=0.7, color='green')\n",
        "axes[1].axvline(0, color='r', linestyle='--', linewidth=2)\n",
        "axes[1].set_xlabel('Minimum eigenvalue')\n",
        "axes[1].set_ylabel('Density')\n",
        "axes[1].set_title('Positivity Check (all should be > 0)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "det_err = 100 * abs(det_np.mean() - DET_TARGET) / DET_TARGET\n",
        "pos_ok = (min_eig > 0).mean() > 0.99\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GIFT v2.2 METRIC LEARNING - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\")\n",
        "print(f\"Constraints:\")\n",
        "print(f\"  det(g) = {det_np.mean():.4f} (target: {DET_TARGET}, error: {det_err:.1f}%)\")\n",
        "print(f\"  g > 0:  {'YES' if pos_ok else 'NO'} (min eig = {min_eig.min():.4f})\")\n",
        "print(f\"\")\n",
        "print(f\"GIFT v2.2 values used:\")\n",
        "print(f\"  det(g) = 65/32 = {DET_TARGET}\")\n",
        "print(f\"  kappa_T = 1/61 = {KAPPA_TARGET:.6f}\")\n",
        "print(f\"\")\n",
        "status = 'PASSED' if det_err < 1.0 and pos_ok else 'NEEDS TUNING'\n",
        "print(f\"Status: {status}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.11.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
