{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prime-Spectral Mollifier: 2M-Zero Extension\n",
    "\n",
    "**Purpose**: Validate the parameter-free mollified Dirichlet polynomial on\n",
    "Odlyzko's 2,001,052 zeros (zeros6 table), extending the 100K verification\n",
    "to T ~ 2,400,000.\n",
    "\n",
    "**Runtime**: ~15 min on Colab A100 (GPU accelerates permutation tests only).\n",
    "CPU-only is fine for the core analysis (~25 min).\n",
    "\n",
    "**Key questions**:\n",
    "1. Does α remain ≈ 1 at θ* = 0.9941 over 2M zeros?\n",
    "2. Does N(T) counting stay 100% correct?\n",
    "3. Does localization stay ≥ 97%?\n",
    "4. How does the residual PSD/ACF evolve at large T?\n",
    "\n",
    "**Reference**: `research/PRIME_SPECTRAL_K7_METRIC.md`, Section 7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 1: Environment & GPU detection\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import numpy as np\n",
    "import os, sys, time, json, warnings\n",
    "from scipy.special import loggamma, lambertw\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU detection (optional — accelerates permutation tests)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU = True\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "    gpu_mem = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / 1e9\n",
    "    print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "except Exception:\n",
    "    GPU = False\n",
    "    print(\"No GPU detected — CPU mode (fine for core analysis)\")\n",
    "\n",
    "print(f\"NumPy {np.__version__}\")\n",
    "print(f\"Python {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ═══════════════════════════════════════════════════════════════\n# Cell 1b: Mount Google Drive early (insurance against idle timeout)\n#          Run this BEFORE the long computations so caches survive.\n# ═══════════════════════════════════════════════════════════════\nDRIVE_DIR = '/content/drive/MyDrive/GIFT_results'\n\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    os.makedirs(DRIVE_DIR, exist_ok=True)\n    print(f\"Google Drive mounted -> {DRIVE_DIR}\")\n    print(\"  .npy caches and JSON will be saved here automatically.\")\nexcept Exception:\n    DRIVE_DIR = None\n    print(\"Not in Colab or Drive unavailable — local storage only.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════\n# Cell 2: Download 2M genuine Riemann zeros (Odlyzko zeros6)\n#          Auto-saves .npy to Drive as soon as download completes.\n# ═══════════════════════════════════════════════════════════════\nimport urllib.request, shutil\n\nCACHE_100K = 'riemann_zeros_100k_genuine.npy'\nCACHE_2M   = 'riemann_zeros_2M_genuine.npy'\n\ndef download_odlyzko(url, cache_file, description):\n    # Check Drive cache first (survives Colab restarts)\n    drive_cache = os.path.join(DRIVE_DIR, cache_file) if DRIVE_DIR else None\n    if os.path.exists(cache_file):\n        print(f\"  Loading cached {description} (local)...\")\n        return np.load(cache_file)\n    if drive_cache and os.path.exists(drive_cache):\n        print(f\"  Loading cached {description} (Drive)...\")\n        shutil.copy2(drive_cache, cache_file)\n        return np.load(cache_file)\n    print(f\"  Downloading {description}...\")\n    t0 = time.time()\n    try:\n        response = urllib.request.urlopen(url, timeout=300)\n        raw = response.read().decode('utf-8')\n        lines = raw.strip().split('\\n')\n        zeros = np.array([float(l.strip()) for l in lines if l.strip()])\n        elapsed = time.time() - t0\n        print(f\"    Got {len(zeros):,} zeros in {elapsed:.1f}s\")\n        np.save(cache_file, zeros)\n        # Immediately copy to Drive (insurance)\n        if drive_cache:\n            shutil.copy2(cache_file, drive_cache)\n            print(f\"    Backed up to Drive: {drive_cache}\")\n        return zeros\n    except Exception as e:\n        print(f\"    Download failed: {e}\")\n        return None\n\nprint(\"=\" * 70)\nprint(\"DOWNLOADING GENUINE RIEMANN ZEROS\")\nprint(\"=\" * 70)\n\n# Primary: 100k zeros (for train/test baseline)\ngamma_100k = download_odlyzko(\n    'https://www-users.cse.umn.edu/~odlyzko/zeta_tables/zeros1',\n    CACHE_100K, \"100,000 zeros (Odlyzko zeros1)\")\n\n# Extended: 2M zeros\ngamma_2M = download_odlyzko(\n    'https://www-users.cse.umn.edu/~odlyzko/zeta_tables/zeros6',\n    CACHE_2M, \"2,001,052 zeros (Odlyzko zeros6)\")\n\nif gamma_2M is None:\n    raise RuntimeError(\"Could not download 2M zeros. Check network.\")\n\ngamma_n = gamma_2M\nN_ZEROS = len(gamma_n)\nprint(f\"\\nLoaded {N_ZEROS:,} zeros, range [{gamma_n[0]:.3f}, {gamma_n[-1]:.3f}]\")\n\n# Validation\nKNOWN = [14.134725142, 21.022039639, 25.010857580, 30.424876126, 32.935061588]\nprint(f\"\\nValidation (first 5 zeros vs known):\")\nfor i, k in enumerate(KNOWN):\n    err = abs(gamma_n[i] - k)\n    status = \"OK\" if err < 1e-6 else \"MISMATCH\"\n    print(f\"  gamma_{i+1} = {gamma_n[i]:.9f}  (known: {k:.9f}, err: {err:.2e}) [{status}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 3: Infrastructure — theta, smooth zeros, primes, mollifier\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def theta_vec(t):\n",
    "    \"\"\"Riemann-Siegel theta function (vectorized).\"\"\"\n",
    "    t = np.asarray(t, dtype=np.float64)\n",
    "    return np.imag(loggamma(0.25 + 0.5j * t)) - 0.5 * t * np.log(np.pi)\n",
    "\n",
    "def theta_deriv(t):\n",
    "    \"\"\"d/dt theta(t) = (1/2) log(t/2pi) + O(1/t^2).\"\"\"\n",
    "    return 0.5 * np.log(np.maximum(np.asarray(t, dtype=np.float64), 1.0) / (2 * np.pi))\n",
    "\n",
    "def smooth_zeros(N):\n",
    "    \"\"\"Compute gamma_n^(0) from theta(t) alone (40 Newton iterations).\"\"\"\n",
    "    ns = np.arange(1, N + 1, dtype=np.float64)\n",
    "    targets = (ns - 1.5) * np.pi\n",
    "    w = np.real(lambertw(ns / np.e))\n",
    "    t = np.maximum(2 * np.pi * ns / w, 2.0)\n",
    "    for _ in range(40):\n",
    "        dt = (theta_vec(t) - targets) / np.maximum(np.abs(theta_deriv(t)), 1e-15)\n",
    "        t -= dt\n",
    "        if np.max(np.abs(dt)) < 1e-12:\n",
    "            break\n",
    "    return t\n",
    "\n",
    "def sieve(N):\n",
    "    \"\"\"Sieve of Eratosthenes up to N.\"\"\"\n",
    "    is_p = np.ones(N + 1, dtype=bool); is_p[:2] = False\n",
    "    for i in range(2, int(N**0.5) + 1):\n",
    "        if is_p[i]: is_p[i*i::i] = False\n",
    "    return np.where(is_p)[0]\n",
    "\n",
    "def w_cosine(x):\n",
    "    \"\"\"Raised cosine mollifier: cos^2(pi*x/2) for x < 1.\"\"\"\n",
    "    return np.where(x < 1.0, np.cos(np.pi * x / 2)**2, 0.0)\n",
    "\n",
    "def w_selberg(x):\n",
    "    \"\"\"Selberg mollifier: (1 - x^2)_+.\"\"\"\n",
    "    return np.maximum(1.0 - x**2, 0.0)\n",
    "\n",
    "def w_linear(x):\n",
    "    \"\"\"Linear taper: (1 - x)_+.\"\"\"\n",
    "    return np.maximum(1.0 - x, 0.0)\n",
    "\n",
    "print(\"Infrastructure loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 4: Compute smooth zeros and corrections for 2M zeros\n",
    "#          (this is the expensive step: ~3 min on A100, ~8 min CPU)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "print(\"Computing smooth zeros for 2M zeros...\")\n",
    "t0 = time.time()\n",
    "\n",
    "gamma0 = smooth_zeros(N_ZEROS)\n",
    "delta  = gamma_n - gamma0\n",
    "tp     = theta_deriv(gamma0)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Done in {elapsed:.1f}s\")\n",
    "print(f\"  delta stats: mean={np.mean(delta):.6f}, std={np.std(delta):.4f}, \"\n",
    "      f\"max|delta|={np.max(np.abs(delta)):.4f}\")\n",
    "print(f\"  T range: [{gamma0[0]:.1f}, {gamma0[-1]:.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════\n# Cell 5: Core analysis — mollified prime sum with theta* = 0.9941\n#          Chunked computation with incremental Drive checkpointing\n# ═══════════════════════════════════════════════════════════════\nimport shutil\n\nTHETA_STAR = 0.9941\nK_MAX = 3\n\n# Sieve primes — for 2M zeros (T ~ 2.4M), we need primes up to ~2.4M\n# But in practice, the mollifier suppresses primes beyond T^theta.\n# We sieve up to a generous upper bound.\nP_MAX = 3_000_000\nprint(f\"Sieving primes up to {P_MAX:,}...\")\nt0 = time.time()\nprimes = sieve(P_MAX)\nprint(f\"  Found {len(primes):,} primes in {time.time()-t0:.1f}s\")\n\ndef prime_sum_adaptive_chunked(gamma0_chunk, tp_chunk, primes, k_max, theta, w_func):\n    \"\"\"\n    Mollified prime sum with adaptive cutoff X(T) = T^theta.\n    Processes one chunk of zeros at a time.\n    \"\"\"\n    S = np.zeros_like(gamma0_chunk)\n    log_gamma0 = np.log(np.maximum(gamma0_chunk, 2.0))\n    log_X = theta * log_gamma0\n\n    for p in primes:\n        logp = np.log(float(p))\n        # Early termination: if smallest log_X can't include this prime\n        if logp / log_X[-1] > 3.0:  # well beyond any mollifier support\n            break\n        for m in range(1, k_max + 1):\n            x = m * logp / log_X\n            weight = w_func(x)\n            if np.max(weight) < 1e-15:\n                continue\n            S -= weight * np.sin(gamma0_chunk * m * logp) / (m * p**(m / 2.0))\n\n    return -S / tp_chunk\n\n\n# ── Check for existing checkpoint on Drive ──\nCHECKPOINT_LOCAL = 'delta_pred_checkpoint.npy'\nCHECKPOINT_DRIVE = os.path.join(DRIVE_DIR, CHECKPOINT_LOCAL) if DRIVE_DIR else None\nstart_chunk = 0\n\nif CHECKPOINT_DRIVE and os.path.exists(CHECKPOINT_DRIVE):\n    print(f\"  Found Drive checkpoint: {CHECKPOINT_DRIVE}\")\n    delta_pred = np.load(CHECKPOINT_DRIVE)\n    # Figure out how far we got (find last non-zero chunk boundary)\n    for i in range(N_ZEROS - 1, 0, -1):\n        if delta_pred[i] != 0.0:\n            start_chunk = ((i // CHUNK_SIZE) + 1) * CHUNK_SIZE\n            break\n    print(f\"  Resuming from index {start_chunk:,} ({100*start_chunk/N_ZEROS:.1f}%)\")\nelif os.path.exists(CHECKPOINT_LOCAL):\n    delta_pred = np.load(CHECKPOINT_LOCAL)\n    for i in range(N_ZEROS - 1, 0, -1):\n        if delta_pred[i] != 0.0:\n            start_chunk = ((i // CHUNK_SIZE) + 1) * CHUNK_SIZE\n            break\n    print(f\"  Found local checkpoint, resuming from index {start_chunk:,}\")\nelse:\n    delta_pred = np.zeros(N_ZEROS)\n\n# ── Process in chunks with checkpointing ──\nCHUNK_SIZE = 100_000\n\nprint(f\"\\nComputing mollified prime sum (theta*={THETA_STAR}, k_max={K_MAX})...\")\nprint(f\"  Processing {N_ZEROS:,} zeros in chunks of {CHUNK_SIZE:,}\")\nif start_chunk > 0:\n    print(f\"  Skipping {start_chunk:,} already-computed zeros\")\nt0 = time.time()\n\nfor i in range(start_chunk, N_ZEROS, CHUNK_SIZE):\n    j = min(i + CHUNK_SIZE, N_ZEROS)\n    chunk_t0 = time.time()\n    delta_pred[i:j] = prime_sum_adaptive_chunked(\n        gamma0[i:j], tp[i:j], primes, K_MAX, THETA_STAR, w_cosine)\n    chunk_elapsed = time.time() - chunk_t0\n    pct = 100 * j / N_ZEROS\n    print(f\"    [{i:>8,}:{j:>8,}) ({pct:5.1f}%) — {chunk_elapsed:.1f}s\")\n\n    # Checkpoint to Drive after each chunk (insurance against timeout)\n    np.save(CHECKPOINT_LOCAL, delta_pred)\n    if CHECKPOINT_DRIVE:\n        shutil.copy2(CHECKPOINT_LOCAL, CHECKPOINT_DRIVE)\n\ntotal_elapsed = time.time() - t0\nprint(f\"\\nTotal computation: {total_elapsed:.1f}s\")\nprint(\"  Checkpoints saved to Drive after each chunk.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 6: Global metrics — alpha, R^2, localization, N(T) counting\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "# Alpha and R^2 (global, alpha FIXED to 1)\n",
    "residuals = delta - delta_pred\n",
    "R2_global = float(1.0 - np.var(residuals) / np.var(delta))\n",
    "alpha_OLS = float(np.dot(delta, delta_pred) / np.dot(delta_pred, delta_pred))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GLOBAL METRICS (2M ZEROS, alpha=1 fixed, theta*=0.9941)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  alpha (OLS, would-be): {alpha_OLS:+.6f}\")\n",
    "print(f\"  |alpha - 1|:          {abs(alpha_OLS - 1):.6f}\")\n",
    "print(f\"  R^2 (alpha=1):        {R2_global:.4f}\")\n",
    "print(f\"  E_rms:                {np.sqrt(np.mean(residuals**2)):.4f}\")\n",
    "print(f\"  E_max:                {np.max(np.abs(residuals)):.4f}\")\n",
    "\n",
    "# Localization\n",
    "half_gaps = np.diff(gamma_n) / 2.0\n",
    "n_loc = min(len(residuals) - 1, len(half_gaps))\n",
    "localized = np.abs(residuals[1:n_loc+1]) < half_gaps[:n_loc]\n",
    "loc_rate = float(np.mean(localized))\n",
    "print(f\"  Localization:         {loc_rate*100:.2f}%\")\n",
    "\n",
    "# N(T) counting at midpoints\n",
    "T_mid = (gamma_n[:-1] + gamma_n[1:]) / 2.0\n",
    "N_actual = np.arange(1, len(T_mid) + 1, dtype=np.float64)\n",
    "theta_mid = theta_vec(T_mid)\n",
    "N_smooth = theta_mid / np.pi + 1\n",
    "err_smooth = np.abs(N_actual - N_smooth)\n",
    "frac_correct = float(np.mean(err_smooth < 0.5))\n",
    "print(f\"  N(T) correct (smooth only): {frac_correct*100:.2f}%\")\n",
    "print(f\"  N(T) mean |error|:          {np.mean(err_smooth):.4f}\")\n",
    "print(f\"  N(T) max |error|:           {np.max(err_smooth):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 7: Window-by-window analysis\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "WINDOWS = [\n",
    "    (0, 100_000),\n",
    "    (100_000, 200_000),\n",
    "    (200_000, 500_000),\n",
    "    (500_000, 1_000_000),\n",
    "    (1_000_000, 1_500_000),\n",
    "    (1_500_000, N_ZEROS),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WINDOW-BY-WINDOW ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Window':>20} | {'T range':>25} | {'alpha':>8} | {'R^2':>8} | {'Loc%':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "window_results = []\n",
    "for (a, b) in WINDOWS:\n",
    "    d_w = delta[a:b]\n",
    "    dp_w = delta_pred[a:b]\n",
    "    r_w = d_w - dp_w\n",
    "    alpha_w = float(np.dot(d_w, dp_w) / np.dot(dp_w, dp_w))\n",
    "    R2_w = float(1.0 - np.var(r_w) / np.var(d_w))\n",
    "\n",
    "    # Localization in this window\n",
    "    hg_a = max(a - 1, 0)\n",
    "    hg_b = min(b, len(half_gaps))\n",
    "    n_w = min(b - a - 1, hg_b - hg_a)\n",
    "    if n_w > 0:\n",
    "        loc_w = float(np.mean(np.abs(r_w[1:n_w+1]) < half_gaps[hg_a:hg_a+n_w]))\n",
    "    else:\n",
    "        loc_w = 0.0\n",
    "\n",
    "    T_lo = gamma_n[a] if a < len(gamma_n) else 0\n",
    "    T_hi = gamma_n[min(b-1, len(gamma_n)-1)]\n",
    "    label = f\"[{a//1000}k, {b//1000}k)\"\n",
    "\n",
    "    print(f\"{label:>20} | [{T_lo:>10.1f}, {T_hi:>10.1f}] | {alpha_w:>+8.4f} | {R2_w:>8.4f} | {loc_w*100:>7.2f}%\")\n",
    "    window_results.append({\n",
    "        'window': label, 'T_lo': float(T_lo), 'T_hi': float(T_hi),\n",
    "        'alpha': alpha_w, 'R2': R2_w, 'localization': loc_w\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 8: Train/test protocol (hard out-of-sample)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def find_theta_star(delta_train, gamma0_train, tp_train, primes, k_max, w_func):\n",
    "    \"\"\"Find theta* by bisection such that alpha(theta) = 1.\"\"\"\n",
    "    def alpha_at_theta(theta):\n",
    "        dp = prime_sum_adaptive_chunked(gamma0_train, tp_train, primes, k_max, theta, w_func)\n",
    "        dot_pp = np.dot(dp, dp)\n",
    "        if dot_pp < 1e-30:\n",
    "            return 2.0\n",
    "        return float(np.dot(delta_train, dp) / dot_pp)\n",
    "\n",
    "    # Bisection: alpha(0.5) > 1, alpha(1.5) < 1\n",
    "    lo, hi = 0.5, 1.5\n",
    "    for _ in range(25):\n",
    "        mid = (lo + hi) / 2\n",
    "        a = alpha_at_theta(mid)\n",
    "        if a > 1.0:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return (lo + hi) / 2\n",
    "\n",
    "# Use first 100K as quick training set (already validated)\n",
    "N_TRAIN = 100_000\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAIN/TEST PROTOCOL\")\n",
    "print(f\"  Train: first {N_TRAIN:,} zeros\")\n",
    "print(f\"  Test:  remaining {N_ZEROS - N_TRAIN:,} zeros\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use a moderate prime set for bisection speed\n",
    "primes_bisect = primes[primes <= 50_000]\n",
    "\n",
    "print(\"Finding theta* on training set...\")\n",
    "t0 = time.time()\n",
    "theta_train = find_theta_star(\n",
    "    delta[:N_TRAIN], gamma0[:N_TRAIN], tp[:N_TRAIN],\n",
    "    primes_bisect, K_MAX, w_cosine)\n",
    "print(f\"  theta*(train) = {theta_train:.4f}  (took {time.time()-t0:.1f}s)\")\n",
    "\n",
    "# Evaluate on TEST set with theta*(train), no recalibration\n",
    "print(\"\\nEvaluating on test set (no recalibration)...\")\n",
    "dp_test = np.zeros(N_ZEROS - N_TRAIN)\n",
    "t0 = time.time()\n",
    "for i in range(0, N_ZEROS - N_TRAIN, CHUNK_SIZE):\n",
    "    j = min(i + CHUNK_SIZE, N_ZEROS - N_TRAIN)\n",
    "    idx_lo = N_TRAIN + i\n",
    "    idx_hi = N_TRAIN + j\n",
    "    dp_test[i:j] = prime_sum_adaptive_chunked(\n",
    "        gamma0[idx_lo:idx_hi], tp[idx_lo:idx_hi],\n",
    "        primes, K_MAX, theta_train, w_cosine)\n",
    "    print(f\"    Test chunk [{i:>8,}:{j:>8,}) done\")\n",
    "\n",
    "d_test = delta[N_TRAIN:]\n",
    "r_test = d_test - dp_test\n",
    "alpha_test = float(np.dot(d_test, dp_test) / np.dot(dp_test, dp_test))\n",
    "R2_test = float(1.0 - np.var(r_test) / np.var(d_test))\n",
    "\n",
    "print(f\"\\nTrain/Test Results:\")\n",
    "print(f\"  theta*(train):    {theta_train:.4f}\")\n",
    "print(f\"  alpha(test):      {alpha_test:+.6f}\")\n",
    "print(f\"  R^2(test):        {R2_test:.4f}\")\n",
    "print(f\"  E_rms(test):      {np.sqrt(np.mean(r_test**2)):.4f}\")\n",
    "print(f\"  Elapsed:          {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 9: Residual diagnostics — ACF and PSD\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESIDUAL DIAGNOSTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ACF of residuals\n",
    "res_centered = residuals - np.mean(residuals)\n",
    "var_res = np.var(res_centered)\n",
    "acf_lags = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n",
    "white_noise_bound = 1.96 / np.sqrt(N_ZEROS)  # 95% CI\n",
    "\n",
    "print(f\"\\nAutocorrelation (95% white-noise bound: +/-{white_noise_bound:.5f}):\")\n",
    "print(f\"  {'Lag':>5} | {'ACF':>10} | {'Significant?':>12}\")\n",
    "print(f\"  \" + \"-\" * 35)\n",
    "for lag in acf_lags:\n",
    "    if lag >= N_ZEROS:\n",
    "        break\n",
    "    acf_val = float(np.mean(res_centered[lag:] * res_centered[:-lag]) / var_res)\n",
    "    sig = \"YES\" if abs(acf_val) > white_noise_bound else \"no\"\n",
    "    print(f\"  {lag:>5} | {acf_val:>+10.6f} | {sig:>12}\")\n",
    "\n",
    "# PSD via FFT\n",
    "print(f\"\\nPower Spectral Density (FFT of residuals):\")\n",
    "if GPU:\n",
    "    psd = cp.asnumpy(cp.abs(cp.fft.rfft(cp.asarray(res_centered)))**2)\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "else:\n",
    "    psd = np.abs(np.fft.rfft(res_centered))**2\n",
    "\n",
    "psd = psd / np.sum(psd)  # normalize to total power = 1\n",
    "freqs = np.arange(len(psd)) / N_ZEROS\n",
    "\n",
    "# Check flatness: ratio of max to mean in frequency bands\n",
    "n_bands = 20\n",
    "band_size = len(psd) // n_bands\n",
    "print(f\"  {'Band':>10} | {'Mean power':>12} | {'Max/Mean':>10}\")\n",
    "print(f\"  \" + \"-\" * 40)\n",
    "for b in range(n_bands):\n",
    "    lo = b * band_size\n",
    "    hi = (b + 1) * band_size\n",
    "    band_mean = np.mean(psd[lo:hi])\n",
    "    band_max = np.max(psd[lo:hi])\n",
    "    ratio = band_max / band_mean if band_mean > 0 else 0\n",
    "    print(f\"  [{lo:>6}:{hi:>6}) | {band_mean:>12.2e} | {ratio:>10.1f}\")\n",
    "\n",
    "print(f\"\\n  Overall PSD flatness: max/mean = {np.max(psd)/np.mean(psd):.1f}\")\n",
    "print(f\"  (White noise: ~3-5x; structured: >20x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 10: Mollifier sensitivity (Selberg, Linear vs Cosine)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "MOLLIFIERS = {\n",
    "    'cosine':  w_cosine,\n",
    "    'selberg': w_selberg,\n",
    "    'linear':  w_linear,\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MOLLIFIER SENSITIVITY (first 100K zeros, fast)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use first 100K for quick comparison\n",
    "N_QUICK = 100_000\n",
    "g0_q = gamma0[:N_QUICK]\n",
    "tp_q = tp[:N_QUICK]\n",
    "d_q  = delta[:N_QUICK]\n",
    "hg_q = half_gaps[:N_QUICK-1]\n",
    "\n",
    "print(f\"\\n{'Mollifier':>10} | {'theta*':>8} | {'alpha':>8} | {'R^2':>8} | {'Loc%':>8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, w_func in MOLLIFIERS.items():\n",
    "    # Find theta* for this mollifier\n",
    "    theta_m = find_theta_star(d_q, g0_q, tp_q, primes_bisect, K_MAX, w_func)\n",
    "    # Evaluate at theta*\n",
    "    dp_m = prime_sum_adaptive_chunked(g0_q, tp_q, primes_bisect, K_MAX, theta_m, w_func)\n",
    "    alpha_m = float(np.dot(d_q, dp_m) / np.dot(dp_m, dp_m))\n",
    "    r_m = d_q - dp_m\n",
    "    R2_m = float(1.0 - np.var(r_m) / np.var(d_q))\n",
    "    n_m = min(len(r_m) - 1, len(hg_q))\n",
    "    loc_m = float(np.mean(np.abs(r_m[1:n_m+1]) < hg_q[:n_m]))\n",
    "\n",
    "    print(f\"{name:>10} | {theta_m:>8.4f} | {alpha_m:>+8.4f} | {R2_m:>8.4f} | {loc_m*100:>7.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════\n# Cell 11: Save results to JSON (local + Drive immediately)\n# ═══════════════════════════════════════════════════════════════\n\nresults = {\n    'metadata': {\n        'date': time.strftime('%Y-%m-%d'),\n        'N_zeros': int(N_ZEROS),\n        'T_max': float(gamma_n[-1]),\n        'theta_star': THETA_STAR,\n        'k_max': K_MAX,\n        'mollifier': 'cosine',\n        'source': 'Odlyzko zeros6 table',\n    },\n    'global': {\n        'alpha_OLS': float(alpha_OLS),\n        'R2': float(R2_global),\n        'E_rms': float(np.sqrt(np.mean(residuals**2))),\n        'E_max': float(np.max(np.abs(residuals))),\n        'localization': float(loc_rate),\n    },\n    'train_test': {\n        'theta_train': float(theta_train),\n        'alpha_test': float(alpha_test),\n        'R2_test': float(R2_test),\n    },\n    'windows': window_results,\n}\n\nout_file = 'prime_spectral_2M_results.json'\nwith open(out_file, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f\"Results saved to {out_file}\")\n\n# Immediate Drive backup\nif DRIVE_DIR:\n    import shutil\n    shutil.copy2(out_file, os.path.join(DRIVE_DIR, out_file))\n    print(f\"  -> Backed up to Drive: {DRIVE_DIR}/{out_file}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DONE — paste the output of this notebook into the PR.\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "source": "# ═══════════════════════════════════════════════════════════════\n# Cell 12: Auto-save to Google Drive + browser download\n#          (insurance against Colab idle timeout)\n# ═══════════════════════════════════════════════════════════════\nimport shutil, glob\n\n# ── 1. Mount Google Drive (run this BEFORE the long cells if possible) ──\nDRIVE_DIR = '/content/drive/MyDrive/GIFT_results'\ndrive_mounted = False\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    os.makedirs(DRIVE_DIR, exist_ok=True)\n    drive_mounted = True\n    print(f\"Google Drive mounted -> {DRIVE_DIR}\")\nexcept Exception as e:\n    print(f\"Drive mount failed ({e}) — will use browser download only\")\n\n# ── 2. Collect all output files ──\noutput_files = ['prime_spectral_2M_results.json']\n# Add any .npy caches that were created\nfor f in ['riemann_zeros_100k_genuine.npy', 'riemann_zeros_2M_genuine.npy']:\n    if os.path.exists(f):\n        output_files.append(f)\n# Add any PNG plots if you generated them\noutput_files.extend(glob.glob('*.png'))\n\nprint(f\"\\nFiles to save: {output_files}\")\n\n# ── 3. Copy to Google Drive ──\nif drive_mounted:\n    print(f\"\\nSaving to Google Drive ({DRIVE_DIR}):\")\n    for f in output_files:\n        if os.path.exists(f):\n            dst = os.path.join(DRIVE_DIR, f)\n            shutil.copy2(f, dst)\n            size_mb = os.path.getsize(f) / 1e6\n            print(f\"  {f} -> Drive ({size_mb:.1f} MB)\")\n    print(\"Drive save complete.\")\nelse:\n    print(\"\\nDrive not available — skipping Drive save.\")\n\n# ── 4. Trigger browser download (works even if Drive fails) ──\ntry:\n    from google.colab import files\n    print(\"\\nTriggering browser downloads:\")\n    for f in output_files:\n        if os.path.exists(f) and os.path.getsize(f) < 200e6:  # skip huge .npy\n            files.download(f)\n            print(f\"  {f} -> browser download\")\n        elif os.path.exists(f):\n            print(f\"  {f} -> too large for browser download, Drive only\")\nexcept ImportError:\n    print(\"\\nNot running in Colab — files saved locally.\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ALL RESULTS SAVED. Safe to close Colab.\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}