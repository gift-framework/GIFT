{
 "cells": [
  {
   "cell_type": "code",
   "source": "# ═══════════════════════════════════════════════════════════════\n# Cell 13: VALIDATION TESTS on CONSTANT model (θ* = 0.9941)\n#\n# The 3-param model failed 0/3. Let's see how the simpler,\n# theoretically-motivated constant model does.\n# ═══════════════════════════════════════════════════════════════\nfrom scipy import stats\n\nprint(\"=\" * 70)\nprint(\"VALIDATION TESTS — CONSTANT MODEL (θ* = 0.9941)\")\nprint(\"=\" * 70)\n\n# ── T5: Monte Carlo — is θ* special or could random θ do as well? ──\nprint(\"\\n[T5] Monte Carlo Permutation Test\")\nprint(\"-\" * 50)\nN_TRIALS = 200\nnp.random.seed(42)\n\n# Use first 200k zeros for speed (representative sample)\nN_MC = 200_000\nd_mc = delta[:N_MC]\ng0_mc = gamma0[:N_MC]\ntp_mc = tp[:N_MC]\n\n# R² at θ* = 0.9941\ndp_opt = delta_pred[:N_MC]\nR2_opt = float(1.0 - np.var(d_mc - dp_opt) / np.var(d_mc))\n\n# R² at random θ values\ntheta_random = np.random.uniform(0.3, 2.0, N_TRIALS)\nR2_random = []\nfor i, th in enumerate(theta_random):\n    dp_r = prime_sum_adaptive_chunked(g0_mc, tp_mc, primes_bisect, K_MAX, th, w_cosine)\n    R2_r = float(1.0 - np.var(d_mc - dp_r) / np.var(d_mc))\n    R2_random.append(R2_r)\n    if (i + 1) % 50 == 0:\n        print(f\"    Trial {i+1}/{N_TRIALS}...\")\n\nR2_random = np.array(R2_random)\nR2_best_random = float(np.max(R2_random))\nmargin = R2_opt - R2_best_random\np_val_mc = float(np.mean(R2_random >= R2_opt))\n\nT5_pass = margin > 0\nprint(f\"\\n  R²(θ*=0.9941):     {R2_opt:.6f}\")\nprint(f\"  R²(best random):   {R2_best_random:.6f}\")\nprint(f\"  Margin:             {margin:+.6f}\")\nprint(f\"  p-value:            {p_val_mc:.4f}\")\nprint(f\"  Verdict:            {'PASS ✓' if T5_pass else 'FAIL ✗'}\")\n\n# ── T7: Bootstrap CI for α ──\nprint(f\"\\n[T7] Bootstrap Confidence Interval for α\")\nprint(\"-\" * 50)\nB = 5000\nnp.random.seed(123)\n\nalpha_boots = np.empty(B)\nfor b in range(B):\n    idx = np.random.randint(0, N_ZEROS, N_ZEROS)\n    d_b = delta[idx]\n    dp_b = delta_pred[idx]\n    dot_pp = np.dot(dp_b, dp_b)\n    alpha_boots[b] = np.dot(d_b, dp_b) / dot_pp if dot_pp > 0 else 0.0\n\nci_lo = float(np.percentile(alpha_boots, 2.5))\nci_hi = float(np.percentile(alpha_boots, 97.5))\nalpha_hat = float(alpha_OLS)\n\nT7_pass = ci_lo <= 1.0 <= ci_hi\nprint(f\"  α(OLS):             {alpha_hat:.6f}\")\nprint(f\"  95% CI:             [{ci_lo:.6f}, {ci_hi:.6f}]\")\nprint(f\"  Contains α=1?       {'YES' if T7_pass else 'NO'}\")\nprint(f\"  Distance to 1:      {min(abs(1-ci_lo), abs(1-ci_hi)):.6f}\")\nprint(f\"  Verdict:            {'PASS ✓' if T7_pass else 'FAIL ✗'}\")\n\n# ── T8: Drift test — is α trending across windows? ──\nprint(f\"\\n[T8] Drift Test (α across windows)\")\nprint(\"-\" * 50)\nalphas = np.array([w['alpha'] for w in window_results])\nwindow_idx = np.arange(len(alphas), dtype=float)\n\nslope, intercept, r_val, p_val, se = stats.linregress(window_idx, alphas)\nalpha_std = float(np.std(alphas))\n\n# For the constant model: drift is acceptable if slope < 2*SE\n# or if p > 0.05\nT8_pass = p_val > 0.05\nprint(f\"  α per window:       {alphas}\")\nprint(f\"  Slope:              {slope:+.6f} per window\")\nprint(f\"  SE(slope):          {se:.6f}\")\nprint(f\"  p-value:            {p_val:.4f}\")\nprint(f\"  α std:              {alpha_std:.6f}\")\nprint(f\"  Verdict:            {'PASS ✓' if T8_pass else 'FAIL ✗'}\")\n\n# ── Summary ──\nn_pass = sum([T5_pass, T7_pass, T8_pass])\nprint(f\"\\n{'=' * 70}\")\nprint(f\"CONSTANT MODEL VERDICT: {n_pass}/3 passed\")\nprint(f\"{'=' * 70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gift-framework/GIFT/blob/research/notebooks/Prime_Spectral_2M_Zeros_trained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIc1V0QI3PKF"
   },
   "source": [
    "# Prime-Spectral Mollifier: 2M-Zero Extension\n",
    "\n",
    "**Purpose**: Validate the parameter-free mollified Dirichlet polynomial on\n",
    "Odlyzko's 2,001,052 zeros (zeros6 table), extending the 100K verification\n",
    "to T ~ 2,400,000.\n",
    "\n",
    "**Runtime**: ~15 min on Colab A100 (GPU accelerates permutation tests only).\n",
    "CPU-only is fine for the core analysis (~25 min).\n",
    "\n",
    "**Key questions**:\n",
    "1. Does α remain ≈ 1 at θ* = 0.9941 over 2M zeros?\n",
    "2. Does N(T) counting stay 100% correct?\n",
    "3. Does localization stay ≥ 97%?\n",
    "4. How does the residual PSD/ACF evolve at large T?\n",
    "\n",
    "**Reference**: `research/PRIME_SPECTRAL_K7_METRIC.md`, Section 7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmPyy6613PKH",
    "outputId": "6123882f-3e4c-4f53-f87b-d0a71563617e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB (85.2 GB)\n",
      "NumPy 2.0.2\n",
      "Python 3.12.12\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 1: Environment & GPU detection\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import numpy as np\n",
    "import os, sys, time, json, warnings\n",
    "from scipy.special import loggamma, lambertw\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU detection (optional — accelerates permutation tests)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU = True\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "    gpu_mem = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / 1e9\n",
    "    print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "except Exception:\n",
    "    GPU = False\n",
    "    print(\"No GPU detected — CPU mode (fine for core analysis)\")\n",
    "\n",
    "print(f\"NumPy {np.__version__}\")\n",
    "print(f\"Python {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TzuUGeR3PKI",
    "outputId": "50f4eb7d-8d8f-4b99-e9bc-b5cba70272e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mounted -> /content/drive/MyDrive/GIFT_results\n",
      "  .npy caches and JSON will be saved here automatically.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 1b: Mount Google Drive early (insurance against idle timeout)\n",
    "#          Run this BEFORE the long computations so caches survive.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "DRIVE_DIR = '/content/drive/MyDrive/GIFT_results'\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    print(f\"Google Drive mounted -> {DRIVE_DIR}\")\n",
    "    print(\"  .npy caches and JSON will be saved here automatically.\")\n",
    "except Exception:\n",
    "    DRIVE_DIR = None\n",
    "    print(\"Not in Colab or Drive unavailable — local storage only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Ysv3eTG3PKI",
    "outputId": "b77843ec-7a4d-43e4-8ecb-481ad790a0b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DOWNLOADING GENUINE RIEMANN ZEROS\n",
      "======================================================================\n",
      "  Downloading 100,000 zeros (Odlyzko zeros1)...\n",
      "    Got 100,000 zeros in 2.3s\n",
      "    Backed up to Drive: /content/drive/MyDrive/GIFT_results/riemann_zeros_100k_genuine.npy\n",
      "  Downloading 2,001,052 zeros (Odlyzko zeros6)...\n",
      "    Got 2,001,052 zeros in 5.1s\n",
      "    Backed up to Drive: /content/drive/MyDrive/GIFT_results/riemann_zeros_2M_genuine.npy\n",
      "\n",
      "Loaded 2,001,052 zeros, range [14.135, 1132490.659]\n",
      "\n",
      "Validation (first 5 zeros vs known):\n",
      "  gamma_1 = 14.134725142  (known: 14.134725142, err: 0.00e+00) [OK]\n",
      "  gamma_2 = 21.022039639  (known: 21.022039639, err: 0.00e+00) [OK]\n",
      "  gamma_3 = 25.010857580  (known: 25.010857580, err: 0.00e+00) [OK]\n",
      "  gamma_4 = 30.424876126  (known: 30.424876126, err: 0.00e+00) [OK]\n",
      "  gamma_5 = 32.935061588  (known: 32.935061588, err: 0.00e+00) [OK]\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 2: Download 2M genuine Riemann zeros (Odlyzko zeros6)\n",
    "#          Auto-saves .npy to Drive as soon as download completes.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import urllib.request, shutil\n",
    "\n",
    "CACHE_100K = 'riemann_zeros_100k_genuine.npy'\n",
    "CACHE_2M   = 'riemann_zeros_2M_genuine.npy'\n",
    "\n",
    "def download_odlyzko(url, cache_file, description):\n",
    "    # Check Drive cache first (survives Colab restarts)\n",
    "    drive_cache = os.path.join(DRIVE_DIR, cache_file) if DRIVE_DIR else None\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"  Loading cached {description} (local)...\")\n",
    "        return np.load(cache_file)\n",
    "    if drive_cache and os.path.exists(drive_cache):\n",
    "        print(f\"  Loading cached {description} (Drive)...\")\n",
    "        shutil.copy2(drive_cache, cache_file)\n",
    "        return np.load(cache_file)\n",
    "    print(f\"  Downloading {description}...\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url, timeout=300)\n",
    "        raw = response.read().decode('utf-8')\n",
    "        lines = raw.strip().split('\\n')\n",
    "        zeros = np.array([float(l.strip()) for l in lines if l.strip()])\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"    Got {len(zeros):,} zeros in {elapsed:.1f}s\")\n",
    "        np.save(cache_file, zeros)\n",
    "        # Immediately copy to Drive (insurance)\n",
    "        if drive_cache:\n",
    "            shutil.copy2(cache_file, drive_cache)\n",
    "            print(f\"    Backed up to Drive: {drive_cache}\")\n",
    "        return zeros\n",
    "    except Exception as e:\n",
    "        print(f\"    Download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DOWNLOADING GENUINE RIEMANN ZEROS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Primary: 100k zeros (for train/test baseline)\n",
    "gamma_100k = download_odlyzko(\n",
    "    'https://www-users.cse.umn.edu/~odlyzko/zeta_tables/zeros1',\n",
    "    CACHE_100K, \"100,000 zeros (Odlyzko zeros1)\")\n",
    "\n",
    "# Extended: 2M zeros\n",
    "gamma_2M = download_odlyzko(\n",
    "    'https://www-users.cse.umn.edu/~odlyzko/zeta_tables/zeros6',\n",
    "    CACHE_2M, \"2,001,052 zeros (Odlyzko zeros6)\")\n",
    "\n",
    "if gamma_2M is None:\n",
    "    raise RuntimeError(\"Could not download 2M zeros. Check network.\")\n",
    "\n",
    "gamma_n = gamma_2M\n",
    "N_ZEROS = len(gamma_n)\n",
    "print(f\"\\nLoaded {N_ZEROS:,} zeros, range [{gamma_n[0]:.3f}, {gamma_n[-1]:.3f}]\")\n",
    "\n",
    "# Validation\n",
    "KNOWN = [14.134725142, 21.022039639, 25.010857580, 30.424876126, 32.935061588]\n",
    "print(f\"\\nValidation (first 5 zeros vs known):\")\n",
    "for i, k in enumerate(KNOWN):\n",
    "    err = abs(gamma_n[i] - k)\n",
    "    status = \"OK\" if err < 1e-6 else \"MISMATCH\"\n",
    "    print(f\"  gamma_{i+1} = {gamma_n[i]:.9f}  (known: {k:.9f}, err: {err:.2e}) [{status}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhdR1b-t3PKI",
    "outputId": "07edbb3e-81b5-4cc5-e580-3665c3db372b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infrastructure loaded.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 3: Infrastructure — theta, smooth zeros, primes, mollifier\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def theta_vec(t):\n",
    "    \"\"\"Riemann-Siegel theta function (vectorized).\"\"\"\n",
    "    t = np.asarray(t, dtype=np.float64)\n",
    "    return np.imag(loggamma(0.25 + 0.5j * t)) - 0.5 * t * np.log(np.pi)\n",
    "\n",
    "def theta_deriv(t):\n",
    "    \"\"\"d/dt theta(t) = (1/2) log(t/2pi) + O(1/t^2).\"\"\"\n",
    "    return 0.5 * np.log(np.maximum(np.asarray(t, dtype=np.float64), 1.0) / (2 * np.pi))\n",
    "\n",
    "def smooth_zeros(N):\n",
    "    \"\"\"Compute gamma_n^(0) from theta(t) alone (40 Newton iterations).\"\"\"\n",
    "    ns = np.arange(1, N + 1, dtype=np.float64)\n",
    "    targets = (ns - 1.5) * np.pi\n",
    "    w = np.real(lambertw(ns / np.e))\n",
    "    t = np.maximum(2 * np.pi * ns / w, 2.0)\n",
    "    for _ in range(40):\n",
    "        dt = (theta_vec(t) - targets) / np.maximum(np.abs(theta_deriv(t)), 1e-15)\n",
    "        t -= dt\n",
    "        if np.max(np.abs(dt)) < 1e-12:\n",
    "            break\n",
    "    return t\n",
    "\n",
    "def sieve(N):\n",
    "    \"\"\"Sieve of Eratosthenes up to N.\"\"\"\n",
    "    is_p = np.ones(N + 1, dtype=bool); is_p[:2] = False\n",
    "    for i in range(2, int(N**0.5) + 1):\n",
    "        if is_p[i]: is_p[i*i::i] = False\n",
    "    return np.where(is_p)[0]\n",
    "\n",
    "def w_cosine(x):\n",
    "    \"\"\"Raised cosine mollifier: cos^2(pi*x/2) for x < 1.\"\"\"\n",
    "    return np.where(x < 1.0, np.cos(np.pi * x / 2)**2, 0.0)\n",
    "\n",
    "def w_selberg(x):\n",
    "    \"\"\"Selberg mollifier: (1 - x^2)_+.\"\"\"\n",
    "    return np.maximum(1.0 - x**2, 0.0)\n",
    "\n",
    "def w_linear(x):\n",
    "    \"\"\"Linear taper: (1 - x)_+.\"\"\"\n",
    "    return np.maximum(1.0 - x, 0.0)\n",
    "\n",
    "print(\"Infrastructure loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77esw6xW3PKI",
    "outputId": "782c535a-b9fa-42d6-9755-ea75a6999776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing smooth zeros for 2M zeros...\n",
      "Done in 11.6s\n",
      "  delta stats: mean=-0.000000, std=0.1881, max|delta|=0.9938\n",
      "  T range: [14.5, 1132490.7]\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 4: Compute smooth zeros and corrections for 2M zeros\n",
    "#          (this is the expensive step: ~3 min on A100, ~8 min CPU)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "print(\"Computing smooth zeros for 2M zeros...\")\n",
    "t0 = time.time()\n",
    "\n",
    "gamma0 = smooth_zeros(N_ZEROS)\n",
    "delta  = gamma_n - gamma0\n",
    "tp     = theta_deriv(gamma0)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Done in {elapsed:.1f}s\")\n",
    "print(f\"  delta stats: mean={np.mean(delta):.6f}, std={np.std(delta):.4f}, \"\n",
    "      f\"max|delta|={np.max(np.abs(delta)):.4f}\")\n",
    "print(f\"  T range: [{gamma0[0]:.1f}, {gamma0[-1]:.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fgcjKhi3PKI",
    "outputId": "7562ecea-7103-4fb9-9876-285334ceb337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sieving primes up to 3,000,000...\n",
      "  Found 216,816 primes in 0.0s\n",
      "\n",
      "Computing mollified prime sum (theta*=0.9941, k_max=3)...\n",
      "  Processing 2,001,052 zeros in chunks of 100,000\n",
      "    [       0: 100,000) (  5.0%) — 1197.4s\n",
      "    [ 100,000: 200,000) ( 10.0%) — 1202.7s\n",
      "    [ 200,000: 300,000) ( 15.0%) — 1206.4s\n",
      "    [ 300,000: 400,000) ( 20.0%) — 1216.8s\n",
      "    [ 400,000: 500,000) ( 25.0%) — 1224.8s\n",
      "    [ 500,000: 600,000) ( 30.0%) — 1234.1s\n",
      "    [ 600,000: 700,000) ( 35.0%) — 1243.1s\n",
      "    [ 700,000: 800,000) ( 40.0%) — 1242.7s\n",
      "    [ 800,000: 900,000) ( 45.0%) — 1241.2s\n",
      "    [ 900,000:1,000,000) ( 50.0%) — 1248.5s\n",
      "    [1,000,000:1,100,000) ( 55.0%) — 1249.0s\n",
      "    [1,100,000:1,200,000) ( 60.0%) — 1252.9s\n",
      "    [1,200,000:1,300,000) ( 65.0%) — 1255.9s\n",
      "    [1,300,000:1,400,000) ( 70.0%) — 1262.6s\n",
      "    [1,400,000:1,500,000) ( 75.0%) — 1265.1s\n",
      "    [1,500,000:1,600,000) ( 80.0%) — 1268.0s\n",
      "    [1,600,000:1,700,000) ( 85.0%) — 1272.1s\n",
      "    [1,700,000:1,800,000) ( 90.0%) — 1272.1s\n",
      "    [1,800,000:1,900,000) ( 95.0%) — 1277.7s\n",
      "    [1,900,000:2,000,000) ( 99.9%) — 1280.0s\n",
      "    [2,000,000:2,001,052) (100.0%) — 27.4s\n",
      "\n",
      "Total computation: 24941.9s\n",
      "  Checkpoints saved to Drive after each chunk.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 5: Core analysis — mollified prime sum with theta* = 0.9941\n",
    "#          Chunked computation with incremental Drive checkpointing\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import shutil\n",
    "\n",
    "THETA_STAR = 0.9941\n",
    "K_MAX = 3\n",
    "\n",
    "# Sieve primes — for 2M zeros (T ~ 2.4M), we need primes up to ~2.4M\n",
    "# But in practice, the mollifier suppresses primes beyond T^theta.\n",
    "# We sieve up to a generous upper bound.\n",
    "P_MAX = 3_000_000\n",
    "print(f\"Sieving primes up to {P_MAX:,}...\")\n",
    "t0 = time.time()\n",
    "primes = sieve(P_MAX)\n",
    "print(f\"  Found {len(primes):,} primes in {time.time()-t0:.1f}s\")\n",
    "\n",
    "def prime_sum_adaptive_chunked(gamma0_chunk, tp_chunk, primes, k_max, theta, w_func):\n",
    "    \"\"\"\n",
    "    Mollified prime sum with adaptive cutoff X(T) = T^theta.\n",
    "    Processes one chunk of zeros at a time.\n",
    "    \"\"\"\n",
    "    S = np.zeros_like(gamma0_chunk)\n",
    "    log_gamma0 = np.log(np.maximum(gamma0_chunk, 2.0))\n",
    "    log_X = theta * log_gamma0\n",
    "\n",
    "    for p in primes:\n",
    "        logp = np.log(float(p))\n",
    "        # Early termination: if smallest log_X can't include this prime\n",
    "        if logp / log_X[-1] > 3.0:  # well beyond any mollifier support\n",
    "            break\n",
    "        for m in range(1, k_max + 1):\n",
    "            x = m * logp / log_X\n",
    "            weight = w_func(x)\n",
    "            if np.max(weight) < 1e-15:\n",
    "                continue\n",
    "            S -= weight * np.sin(gamma0_chunk * m * logp) / (m * p**(m / 2.0))\n",
    "\n",
    "    return -S / tp_chunk\n",
    "\n",
    "\n",
    "# ── Check for existing checkpoint on Drive ──\n",
    "CHECKPOINT_LOCAL = 'delta_pred_checkpoint.npy'\n",
    "CHECKPOINT_DRIVE = os.path.join(DRIVE_DIR, CHECKPOINT_LOCAL) if DRIVE_DIR else None\n",
    "start_chunk = 0\n",
    "\n",
    "if CHECKPOINT_DRIVE and os.path.exists(CHECKPOINT_DRIVE):\n",
    "    print(f\"  Found Drive checkpoint: {CHECKPOINT_DRIVE}\")\n",
    "    delta_pred = np.load(CHECKPOINT_DRIVE)\n",
    "    # Figure out how far we got (find last non-zero chunk boundary)\n",
    "    for i in range(N_ZEROS - 1, 0, -1):\n",
    "        if delta_pred[i] != 0.0:\n",
    "            start_chunk = ((i // CHUNK_SIZE) + 1) * CHUNK_SIZE\n",
    "            break\n",
    "    print(f\"  Resuming from index {start_chunk:,} ({100*start_chunk/N_ZEROS:.1f}%)\")\n",
    "elif os.path.exists(CHECKPOINT_LOCAL):\n",
    "    delta_pred = np.load(CHECKPOINT_LOCAL)\n",
    "    for i in range(N_ZEROS - 1, 0, -1):\n",
    "        if delta_pred[i] != 0.0:\n",
    "            start_chunk = ((i // CHUNK_SIZE) + 1) * CHUNK_SIZE\n",
    "            break\n",
    "    print(f\"  Found local checkpoint, resuming from index {start_chunk:,}\")\n",
    "else:\n",
    "    delta_pred = np.zeros(N_ZEROS)\n",
    "\n",
    "# ── Process in chunks with checkpointing ──\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "print(f\"\\nComputing mollified prime sum (theta*={THETA_STAR}, k_max={K_MAX})...\")\n",
    "print(f\"  Processing {N_ZEROS:,} zeros in chunks of {CHUNK_SIZE:,}\")\n",
    "if start_chunk > 0:\n",
    "    print(f\"  Skipping {start_chunk:,} already-computed zeros\")\n",
    "t0 = time.time()\n",
    "\n",
    "for i in range(start_chunk, N_ZEROS, CHUNK_SIZE):\n",
    "    j = min(i + CHUNK_SIZE, N_ZEROS)\n",
    "    chunk_t0 = time.time()\n",
    "    delta_pred[i:j] = prime_sum_adaptive_chunked(\n",
    "        gamma0[i:j], tp[i:j], primes, K_MAX, THETA_STAR, w_cosine)\n",
    "    chunk_elapsed = time.time() - chunk_t0\n",
    "    pct = 100 * j / N_ZEROS\n",
    "    print(f\"    [{i:>8,}:{j:>8,}) ({pct:5.1f}%) — {chunk_elapsed:.1f}s\")\n",
    "\n",
    "    # Checkpoint to Drive after each chunk (insurance against timeout)\n",
    "    np.save(CHECKPOINT_LOCAL, delta_pred)\n",
    "    if CHECKPOINT_DRIVE:\n",
    "        shutil.copy2(CHECKPOINT_LOCAL, CHECKPOINT_DRIVE)\n",
    "\n",
    "total_elapsed = time.time() - t0\n",
    "print(f\"\\nTotal computation: {total_elapsed:.1f}s\")\n",
    "print(\"  Checkpoints saved to Drive after each chunk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6byA5wB83PKJ",
    "outputId": "8775bbbf-947c-4a02-d191-f02ab71d74fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GLOBAL METRICS (2M ZEROS, alpha=1 fixed, theta*=0.9941)\n",
      "======================================================================\n",
      "  alpha (OLS, would-be): +1.006358\n",
      "  |alpha - 1|:          0.006358\n",
      "  R^2 (alpha=1):        0.9219\n",
      "  E_rms:                0.0526\n",
      "  E_max:                0.7782\n",
      "  Localization:         97.20%\n",
      "  N(T) correct (smooth only): 94.66%\n",
      "  N(T) mean |error|:          0.2124\n",
      "  N(T) max |error|:           0.9975\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 6: Global metrics — alpha, R^2, localization, N(T) counting\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "# Alpha and R^2 (global, alpha FIXED to 1)\n",
    "residuals = delta - delta_pred\n",
    "R2_global = float(1.0 - np.var(residuals) / np.var(delta))\n",
    "alpha_OLS = float(np.dot(delta, delta_pred) / np.dot(delta_pred, delta_pred))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GLOBAL METRICS (2M ZEROS, alpha=1 fixed, theta*=0.9941)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  alpha (OLS, would-be): {alpha_OLS:+.6f}\")\n",
    "print(f\"  |alpha - 1|:          {abs(alpha_OLS - 1):.6f}\")\n",
    "print(f\"  R^2 (alpha=1):        {R2_global:.4f}\")\n",
    "print(f\"  E_rms:                {np.sqrt(np.mean(residuals**2)):.4f}\")\n",
    "print(f\"  E_max:                {np.max(np.abs(residuals)):.4f}\")\n",
    "\n",
    "# Localization\n",
    "half_gaps = np.diff(gamma_n) / 2.0\n",
    "n_loc = min(len(residuals) - 1, len(half_gaps))\n",
    "localized = np.abs(residuals[1:n_loc+1]) < half_gaps[:n_loc]\n",
    "loc_rate = float(np.mean(localized))\n",
    "print(f\"  Localization:         {loc_rate*100:.2f}%\")\n",
    "\n",
    "# N(T) counting at midpoints\n",
    "T_mid = (gamma_n[:-1] + gamma_n[1:]) / 2.0\n",
    "N_actual = np.arange(1, len(T_mid) + 1, dtype=np.float64)\n",
    "theta_mid = theta_vec(T_mid)\n",
    "N_smooth = theta_mid / np.pi + 1\n",
    "err_smooth = np.abs(N_actual - N_smooth)\n",
    "frac_correct = float(np.mean(err_smooth < 0.5))\n",
    "print(f\"  N(T) correct (smooth only): {frac_correct*100:.2f}%\")\n",
    "print(f\"  N(T) mean |error|:          {np.mean(err_smooth):.4f}\")\n",
    "print(f\"  N(T) max |error|:           {np.max(err_smooth):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uG0NcmhZ3PKJ",
    "outputId": "6b01d427-cfbc-42a6-de90-aa53b3830d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WINDOW-BY-WINDOW ANALYSIS\n",
      "======================================================================\n",
      "              Window |                   T range |    alpha |      R^2 |     Loc%\n",
      "--------------------------------------------------------------------------------\n",
      "          [0k, 100k) | [      14.1,    74920.8] |  +0.9869 |   0.9394 |   98.09%\n",
      "        [100k, 200k) | [   74921.9,   139502.0] |  +1.0029 |   0.9298 |   99.11%\n",
      "        [200k, 500k) | [  139502.6,   319387.2] |  +1.0059 |   0.9249 |   98.98%\n",
      "       [500k, 1000k) | [  319388.1,   600269.7] |  +1.0082 |   0.9208 |   98.85%\n",
      "      [1000k, 1500k) | [  600270.3,   869610.3] |  +1.0093 |   0.9183 |   98.76%\n",
      "      [1500k, 2001k) | [  869610.7,  1132490.7] |  +1.0097 |   0.9165 |   98.72%\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 7: Window-by-window analysis\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "WINDOWS = [\n",
    "    (0, 100_000),\n",
    "    (100_000, 200_000),\n",
    "    (200_000, 500_000),\n",
    "    (500_000, 1_000_000),\n",
    "    (1_000_000, 1_500_000),\n",
    "    (1_500_000, N_ZEROS),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WINDOW-BY-WINDOW ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Window':>20} | {'T range':>25} | {'alpha':>8} | {'R^2':>8} | {'Loc%':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "window_results = []\n",
    "for (a, b) in WINDOWS:\n",
    "    d_w = delta[a:b]\n",
    "    dp_w = delta_pred[a:b]\n",
    "    r_w = d_w - dp_w\n",
    "    alpha_w = float(np.dot(d_w, dp_w) / np.dot(dp_w, dp_w))\n",
    "    R2_w = float(1.0 - np.var(r_w) / np.var(d_w))\n",
    "\n",
    "    # Localization in this window\n",
    "    hg_a = max(a - 1, 0)\n",
    "    hg_b = min(b, len(half_gaps))\n",
    "    n_w = min(b - a - 1, hg_b - hg_a)\n",
    "    if n_w > 0:\n",
    "        loc_w = float(np.mean(np.abs(r_w[1:n_w+1]) < half_gaps[hg_a:hg_a+n_w]))\n",
    "    else:\n",
    "        loc_w = 0.0\n",
    "\n",
    "    T_lo = gamma_n[a] if a < len(gamma_n) else 0\n",
    "    T_hi = gamma_n[min(b-1, len(gamma_n)-1)]\n",
    "    label = f\"[{a//1000}k, {b//1000}k)\"\n",
    "\n",
    "    print(f\"{label:>20} | [{T_lo:>10.1f}, {T_hi:>10.1f}] | {alpha_w:>+8.4f} | {R2_w:>8.4f} | {loc_w*100:>7.2f}%\")\n",
    "    window_results.append({\n",
    "        'window': label, 'T_lo': float(T_lo), 'T_hi': float(T_hi),\n",
    "        'alpha': alpha_w, 'R2': R2_w, 'localization': loc_w\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sglGssff3PKK",
    "outputId": "c5892dbf-54dc-4d47-a311-074a1a0ff911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAIN/TEST PROTOCOL\n",
      "  Train: first 100,000 zeros\n",
      "  Test:  remaining 1,901,052 zeros\n",
      "======================================================================\n",
      "Finding theta* on training set...\n",
      "  theta*(train) = 0.9640  (took 903.9s)\n",
      "\n",
      "Evaluating on test set (no recalibration)...\n",
      "    Test chunk [       0: 100,000) done\n",
      "    Test chunk [ 100,000: 200,000) done\n",
      "    Test chunk [ 200,000: 300,000) done\n",
      "    Test chunk [ 300,000: 400,000) done\n",
      "    Test chunk [ 400,000: 500,000) done\n",
      "    Test chunk [ 500,000: 600,000) done\n",
      "    Test chunk [ 600,000: 700,000) done\n",
      "    Test chunk [ 700,000: 800,000) done\n",
      "    Test chunk [ 800,000: 900,000) done\n",
      "    Test chunk [ 900,000:1,000,000) done\n",
      "    Test chunk [1,000,000:1,100,000) done\n",
      "    Test chunk [1,100,000:1,200,000) done\n",
      "    Test chunk [1,200,000:1,300,000) done\n",
      "    Test chunk [1,300,000:1,400,000) done\n",
      "    Test chunk [1,400,000:1,500,000) done\n",
      "    Test chunk [1,500,000:1,600,000) done\n",
      "    Test chunk [1,600,000:1,700,000) done\n",
      "    Test chunk [1,700,000:1,800,000) done\n",
      "    Test chunk [1,800,000:1,900,000) done\n",
      "    Test chunk [1,900,000:1,901,052) done\n",
      "\n",
      "Train/Test Results:\n",
      "  theta*(train):    0.9640\n",
      "  alpha(test):      +1.018595\n",
      "  R^2(test):        0.9190\n",
      "  E_rms(test):      0.0528\n",
      "  Elapsed:          23387.1s\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 8: Train/test protocol (hard out-of-sample)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def find_theta_star(delta_train, gamma0_train, tp_train, primes, k_max, w_func):\n",
    "    \"\"\"Find theta* by bisection such that alpha(theta) = 1.\"\"\"\n",
    "    def alpha_at_theta(theta):\n",
    "        dp = prime_sum_adaptive_chunked(gamma0_train, tp_train, primes, k_max, theta, w_func)\n",
    "        dot_pp = np.dot(dp, dp)\n",
    "        if dot_pp < 1e-30:\n",
    "            return 2.0\n",
    "        return float(np.dot(delta_train, dp) / dot_pp)\n",
    "\n",
    "    # Bisection: alpha(0.5) > 1, alpha(1.5) < 1\n",
    "    lo, hi = 0.5, 1.5\n",
    "    for _ in range(25):\n",
    "        mid = (lo + hi) / 2\n",
    "        a = alpha_at_theta(mid)\n",
    "        if a > 1.0:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return (lo + hi) / 2\n",
    "\n",
    "# Use first 100K as quick training set (already validated)\n",
    "N_TRAIN = 100_000\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAIN/TEST PROTOCOL\")\n",
    "print(f\"  Train: first {N_TRAIN:,} zeros\")\n",
    "print(f\"  Test:  remaining {N_ZEROS - N_TRAIN:,} zeros\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use a moderate prime set for bisection speed\n",
    "primes_bisect = primes[primes <= 50_000]\n",
    "\n",
    "print(\"Finding theta* on training set...\")\n",
    "t0 = time.time()\n",
    "theta_train = find_theta_star(\n",
    "    delta[:N_TRAIN], gamma0[:N_TRAIN], tp[:N_TRAIN],\n",
    "    primes_bisect, K_MAX, w_cosine)\n",
    "print(f\"  theta*(train) = {theta_train:.4f}  (took {time.time()-t0:.1f}s)\")\n",
    "\n",
    "# Evaluate on TEST set with theta*(train), no recalibration\n",
    "print(\"\\nEvaluating on test set (no recalibration)...\")\n",
    "dp_test = np.zeros(N_ZEROS - N_TRAIN)\n",
    "t0 = time.time()\n",
    "for i in range(0, N_ZEROS - N_TRAIN, CHUNK_SIZE):\n",
    "    j = min(i + CHUNK_SIZE, N_ZEROS - N_TRAIN)\n",
    "    idx_lo = N_TRAIN + i\n",
    "    idx_hi = N_TRAIN + j\n",
    "    dp_test[i:j] = prime_sum_adaptive_chunked(\n",
    "        gamma0[idx_lo:idx_hi], tp[idx_lo:idx_hi],\n",
    "        primes, K_MAX, theta_train, w_cosine)\n",
    "    print(f\"    Test chunk [{i:>8,}:{j:>8,}) done\")\n",
    "\n",
    "d_test = delta[N_TRAIN:]\n",
    "r_test = d_test - dp_test\n",
    "alpha_test = float(np.dot(d_test, dp_test) / np.dot(dp_test, dp_test))\n",
    "R2_test = float(1.0 - np.var(r_test) / np.var(d_test))\n",
    "\n",
    "print(f\"\\nTrain/Test Results:\")\n",
    "print(f\"  theta*(train):    {theta_train:.4f}\")\n",
    "print(f\"  alpha(test):      {alpha_test:+.6f}\")\n",
    "print(f\"  R^2(test):        {R2_test:.4f}\")\n",
    "print(f\"  E_rms(test):      {np.sqrt(np.mean(r_test**2)):.4f}\")\n",
    "print(f\"  Elapsed:          {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0Jq95W03PKK",
    "outputId": "de405ace-61c8-4395-97ea-634242c9a251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESIDUAL DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "Autocorrelation (95% white-noise bound: +/-0.00139):\n",
      "    Lag |        ACF | Significant?\n",
      "  -----------------------------------\n",
      "      1 |  -0.466634 |          YES\n",
      "      2 |  -0.052985 |          YES\n",
      "      3 |  +0.014917 |          YES\n",
      "      5 |  +0.002644 |          YES\n",
      "      8 |  +0.014085 |          YES\n",
      "     13 |  +0.023263 |          YES\n",
      "     21 |  +0.006754 |          YES\n",
      "     34 |  +0.007082 |          YES\n",
      "     55 |  +0.000450 |           no\n",
      "     89 |  -0.002021 |          YES\n",
      "\n",
      "Power Spectral Density (FFT of residuals):\n",
      "        Band |   Mean power |   Max/Mean\n",
      "  ----------------------------------------\n",
      "  [     0: 50026) |     1.49e-08 |       14.4\n",
      "  [ 50026:100052) |     4.84e-08 |        9.6\n",
      "  [100052:150078) |     9.69e-08 |       10.0\n",
      "  [150078:200104) |     1.48e-07 |        9.6\n",
      "  [200104:250130) |     2.27e-07 |        9.5\n",
      "  [250130:300156) |     2.90e-07 |       15.0\n",
      "  [300156:350182) |     5.61e-07 |        8.3\n",
      "  [350182:400208) |     6.26e-07 |       10.4\n",
      "  [400208:450234) |     7.43e-07 |       13.8\n",
      "  [450234:500260) |     1.29e-06 |        8.6\n",
      "  [500260:550286) |     1.26e-06 |       11.1\n",
      "  [550286:600312) |     1.29e-06 |       11.0\n",
      "  [600312:650338) |     1.42e-06 |       12.5\n",
      "  [650338:700364) |     1.50e-06 |       10.9\n",
      "  [700364:750390) |     1.61e-06 |       10.2\n",
      "  [750390:800416) |     1.70e-06 |       10.5\n",
      "  [800416:850442) |     1.75e-06 |        9.0\n",
      "  [850442:900468) |     1.77e-06 |       11.4\n",
      "  [900468:950494) |     1.81e-06 |       12.0\n",
      "  [950494:1000520) |     1.83e-06 |       13.2\n",
      "\n",
      "  Overall PSD flatness: max/mean = 31.2\n",
      "  (White noise: ~3-5x; structured: >20x)\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 9: Residual diagnostics — ACF and PSD\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESIDUAL DIAGNOSTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ACF of residuals\n",
    "res_centered = residuals - np.mean(residuals)\n",
    "var_res = np.var(res_centered)\n",
    "acf_lags = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n",
    "white_noise_bound = 1.96 / np.sqrt(N_ZEROS)  # 95% CI\n",
    "\n",
    "print(f\"\\nAutocorrelation (95% white-noise bound: +/-{white_noise_bound:.5f}):\")\n",
    "print(f\"  {'Lag':>5} | {'ACF':>10} | {'Significant?':>12}\")\n",
    "print(f\"  \" + \"-\" * 35)\n",
    "for lag in acf_lags:\n",
    "    if lag >= N_ZEROS:\n",
    "        break\n",
    "    acf_val = float(np.mean(res_centered[lag:] * res_centered[:-lag]) / var_res)\n",
    "    sig = \"YES\" if abs(acf_val) > white_noise_bound else \"no\"\n",
    "    print(f\"  {lag:>5} | {acf_val:>+10.6f} | {sig:>12}\")\n",
    "\n",
    "# PSD via FFT\n",
    "print(f\"\\nPower Spectral Density (FFT of residuals):\")\n",
    "if GPU:\n",
    "    psd = cp.asnumpy(cp.abs(cp.fft.rfft(cp.asarray(res_centered)))**2)\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "else:\n",
    "    psd = np.abs(np.fft.rfft(res_centered))**2\n",
    "\n",
    "psd = psd / np.sum(psd)  # normalize to total power = 1\n",
    "freqs = np.arange(len(psd)) / N_ZEROS\n",
    "\n",
    "# Check flatness: ratio of max to mean in frequency bands\n",
    "n_bands = 20\n",
    "band_size = len(psd) // n_bands\n",
    "print(f\"  {'Band':>10} | {'Mean power':>12} | {'Max/Mean':>10}\")\n",
    "print(f\"  \" + \"-\" * 40)\n",
    "for b in range(n_bands):\n",
    "    lo = b * band_size\n",
    "    hi = (b + 1) * band_size\n",
    "    band_mean = np.mean(psd[lo:hi])\n",
    "    band_max = np.max(psd[lo:hi])\n",
    "    ratio = band_max / band_mean if band_mean > 0 else 0\n",
    "    print(f\"  [{lo:>6}:{hi:>6}) | {band_mean:>12.2e} | {ratio:>10.1f}\")\n",
    "\n",
    "print(f\"\\n  Overall PSD flatness: max/mean = {np.max(psd)/np.mean(psd):.1f}\")\n",
    "print(f\"  (White noise: ~3-5x; structured: >20x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAVd8mHH3PKK",
    "outputId": "7ab14a15-7f37-4026-9ff4-67b6c7c722b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MOLLIFIER SENSITIVITY (first 100K zeros, fast)\n",
      "======================================================================\n",
      "\n",
      " Mollifier |   theta* |    alpha |      R^2 |     Loc%\n",
      "-------------------------------------------------------\n",
      "    cosine |   0.9640 |  +1.0000 |   0.9390 |   98.04%\n",
      "   selberg |   0.6947 |  +1.0000 |   0.9375 |   97.99%\n",
      "    linear |   1.0878 |  +1.0000 |   0.9295 |   98.09%\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 10: Mollifier sensitivity (Selberg, Linear vs Cosine)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "MOLLIFIERS = {\n",
    "    'cosine':  w_cosine,\n",
    "    'selberg': w_selberg,\n",
    "    'linear':  w_linear,\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MOLLIFIER SENSITIVITY (first 100K zeros, fast)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use first 100K for quick comparison\n",
    "N_QUICK = 100_000\n",
    "g0_q = gamma0[:N_QUICK]\n",
    "tp_q = tp[:N_QUICK]\n",
    "d_q  = delta[:N_QUICK]\n",
    "hg_q = half_gaps[:N_QUICK-1]\n",
    "\n",
    "print(f\"\\n{'Mollifier':>10} | {'theta*':>8} | {'alpha':>8} | {'R^2':>8} | {'Loc%':>8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, w_func in MOLLIFIERS.items():\n",
    "    # Find theta* for this mollifier\n",
    "    theta_m = find_theta_star(d_q, g0_q, tp_q, primes_bisect, K_MAX, w_func)\n",
    "    # Evaluate at theta*\n",
    "    dp_m = prime_sum_adaptive_chunked(g0_q, tp_q, primes_bisect, K_MAX, theta_m, w_func)\n",
    "    alpha_m = float(np.dot(d_q, dp_m) / np.dot(dp_m, dp_m))\n",
    "    r_m = d_q - dp_m\n",
    "    R2_m = float(1.0 - np.var(r_m) / np.var(d_q))\n",
    "    n_m = min(len(r_m) - 1, len(hg_q))\n",
    "    loc_m = float(np.mean(np.abs(r_m[1:n_m+1]) < hg_q[:n_m]))\n",
    "\n",
    "    print(f\"{name:>10} | {theta_m:>8.4f} | {alpha_m:>+8.4f} | {R2_m:>8.4f} | {loc_m*100:>7.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DXm8UiMh3PKL",
    "outputId": "2bf6f778-b6cb-4cfe-8566-b16d865635cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to prime_spectral_2M_results.json\n",
      "  -> Backed up to Drive: /content/drive/MyDrive/GIFT_results/prime_spectral_2M_results.json\n",
      "\n",
      "======================================================================\n",
      "DONE — paste the output of this notebook into the PR.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 11: Save results to JSON (local + Drive immediately)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "results = {\n",
    "    'metadata': {\n",
    "        'date': time.strftime('%Y-%m-%d'),\n",
    "        'N_zeros': int(N_ZEROS),\n",
    "        'T_max': float(gamma_n[-1]),\n",
    "        'theta_star': THETA_STAR,\n",
    "        'k_max': K_MAX,\n",
    "        'mollifier': 'cosine',\n",
    "        'source': 'Odlyzko zeros6 table',\n",
    "    },\n",
    "    'global': {\n",
    "        'alpha_OLS': float(alpha_OLS),\n",
    "        'R2': float(R2_global),\n",
    "        'E_rms': float(np.sqrt(np.mean(residuals**2))),\n",
    "        'E_max': float(np.max(np.abs(residuals))),\n",
    "        'localization': float(loc_rate),\n",
    "    },\n",
    "    'train_test': {\n",
    "        'theta_train': float(theta_train),\n",
    "        'alpha_test': float(alpha_test),\n",
    "        'R2_test': float(R2_test),\n",
    "    },\n",
    "    'windows': window_results,\n",
    "}\n",
    "\n",
    "out_file = 'prime_spectral_2M_results.json'\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {out_file}\")\n",
    "\n",
    "# Immediate Drive backup\n",
    "if DRIVE_DIR:\n",
    "    import shutil\n",
    "    shutil.copy2(out_file, os.path.join(DRIVE_DIR, out_file))\n",
    "    print(f\"  -> Backed up to Drive: {DRIVE_DIR}/{out_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DONE — paste the output of this notebook into the PR.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGctA0gy3PKL"
   },
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Cell 12: Auto-save to Google Drive + browser download\n",
    "#          (insurance against Colab idle timeout)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "import shutil, glob\n",
    "\n",
    "# ── 1. Mount Google Drive (run this BEFORE the long cells if possible) ──\n",
    "DRIVE_DIR = '/content/drive/MyDrive/GIFT_results'\n",
    "drive_mounted = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    drive_mounted = True\n",
    "    print(f\"Google Drive mounted -> {DRIVE_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Drive mount failed ({e}) — will use browser download only\")\n",
    "\n",
    "# ── 2. Collect all output files ──\n",
    "output_files = ['prime_spectral_2M_results.json']\n",
    "# Add any .npy caches that were created\n",
    "for f in ['riemann_zeros_100k_genuine.npy', 'riemann_zeros_2M_genuine.npy']:\n",
    "    if os.path.exists(f):\n",
    "        output_files.append(f)\n",
    "# Add any PNG plots if you generated them\n",
    "output_files.extend(glob.glob('*.png'))\n",
    "\n",
    "print(f\"\\nFiles to save: {output_files}\")\n",
    "\n",
    "# ── 3. Copy to Google Drive ──\n",
    "if drive_mounted:\n",
    "    print(f\"\\nSaving to Google Drive ({DRIVE_DIR}):\")\n",
    "    for f in output_files:\n",
    "        if os.path.exists(f):\n",
    "            dst = os.path.join(DRIVE_DIR, f)\n",
    "            shutil.copy2(f, dst)\n",
    "            size_mb = os.path.getsize(f) / 1e6\n",
    "            print(f\"  {f} -> Drive ({size_mb:.1f} MB)\")\n",
    "    print(\"Drive save complete.\")\n",
    "else:\n",
    "    print(\"\\nDrive not available — skipping Drive save.\")\n",
    "\n",
    "# ── 4. Trigger browser download (works even if Drive fails) ──\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nTriggering browser downloads:\")\n",
    "    for f in output_files:\n",
    "        if os.path.exists(f) and os.path.getsize(f) < 200e6:  # skip huge .npy\n",
    "            files.download(f)\n",
    "            print(f\"  {f} -> browser download\")\n",
    "        elif os.path.exists(f):\n",
    "            print(f\"  {f} -> too large for browser download, Drive only\")\n",
    "except ImportError:\n",
    "    print(\"\\nNot running in Colab — files saved locally.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL RESULTS SAVED. Safe to close Colab.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# Extended Analysis: Constant Model Validation & Diagnostics\n*Added 2026-02-09 — Claude debrief session*",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}