{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Full G₂ Holonomy Metric on K₇ via PINN\n",
    "## Comprehensive Reconstruction from Prime-Spectral Data\n",
    "\n",
    "**GIFT Framework** — Geometric Information Field Theory \n",
    "**Hardware**: NVIDIA A100-SXM4-80GB (Colab) \n",
    "**Date**: 2026-02\n",
    "\n",
    "---\n",
    "\n",
    "This notebook reconstructs the **explicit 7×7 metric tensor** $g_{ij}(x^1,\\ldots,x^7)$ on the compact G₂-holonomy manifold K₇, using a Physics-Informed Neural Network constrained by:\n",
    "\n",
    "1. **Determinant**: $\\det(g) = 65/32$ (topologically determined)\n",
    "2. **Torsion**: $\\|\\nabla\\varphi\\| \\to 0$ (G₂ holonomy)\n",
    "3. **Spectral gap**: $\\lambda_1 = 14/99$ (Hodge Laplacian)\n",
    "4. **Period integrals**: $\\int_{C_k} \\varphi = \\Pi_k$ for 77 3-cycles\n",
    "5. **Positive definiteness**: all eigenvalues of $g > 0$\n",
    "\n",
    "The 77 moduli $\\Pi_k(T)$ come from the prime-spectral formula (Steps 1–4).\n",
    "\n",
    "### Pipeline Summary\n",
    "| Step | Content | Status |\n",
    "|------|---------|--------|\n",
    "| 1–2 | Mollified Dirichlet polynomial, α=1, 100% counting | ✅ |\n",
    "| 3 | 77 periods → moduli space (35 local + 42 global) | ✅ |\n",
    "| 4 | G₂ decomposition, E₈/K3 lattice, metric Jacobian | ✅ |\n",
    "| **5** | **PINN reconstruction of g_ij(x)** | **This notebook** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Header](#)\n",
    "1. [Setup & Dependencies](#sec1)\n",
    "2. [GIFT Constants & G₂ Structure](#sec2)\n",
    "3. [Prime-Spectral Periods](#sec3)\n",
    "4. [TCS Neck Sampling](#sec4)\n",
    "5. [PINN Model Architecture](#sec5)\n",
    "6. [Loss Function (6 Terms, 3 Tiers)](#sec6)\n",
    "7. [Training Infrastructure](#sec7)\n",
    "8. [Training Execution (3 Phases)](#sec8)\n",
    "9. [Evaluation & Metric Extraction](#sec9)\n",
    "10. [Visualization (8 Figures)](#sec10)\n",
    "11. [Export (JSON + npy + pt)](#sec11)\n",
    "12. [Summary & Conclusions](#sec12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies <a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Install dependencies\n",
    "!pip install -q torch numpy scipy matplotlib tqdm\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(f'CuPy already installed: {cp.__version__}')\n",
    "except ImportError:\n",
    "    !pip install -q cupy-cuda12x\n",
    "    import cupy as cp\n",
    "    print(f'CuPy installed: {cp.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.2: Imports and GPU detection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json, os, time, gc, warnings\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU detection\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU_AVAILABLE = True\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "    gpu_mem = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / 1e9\n",
    "    print(f'GPU: {gpu_name} ({gpu_mem:.0f} GB)')\n",
    "except Exception as e:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(f'No GPU/CuPy: {e}. Using CPU fallback.')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DTYPE = torch.float64\n",
    "torch.set_default_dtype(DTYPE)\n",
    "print(f'PyTorch device: {DEVICE}')\n",
    "print(f'Default dtype: {DTYPE}')\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Free all GPU memory pools.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU_AVAILABLE:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1.3: Download data from repository\nimport urllib.request\n\nBASE_URL = 'https://raw.githubusercontent.com/gift-framework/GIFT/research/notebooks'\nRIEMANN_URL = f'{BASE_URL}/riemann'\nDATA_DIR = 'data'\nos.makedirs(DATA_DIR, exist_ok=True)\n\ndef download(url, path):\n    if not os.path.exists(path):\n        print(f'  Downloading {os.path.basename(path)}...')\n        try:\n            urllib.request.urlretrieve(url, path)\n        except Exception as e:\n            print(f'  Failed: {e}')\n            return False\n    return True\n\n# Download pre-computed results (Step 3-4)\njson_files = [\n    'moduli_reconstruction_results.json',\n    'harmonic_forms_results.json',\n    'heat_kernel_results.json',\n]\nstep_data = {}\nfor fname in json_files:\n    path = os.path.join(DATA_DIR, fname)\n    if download(f'{RIEMANN_URL}/{fname}', path):\n        try:\n            with open(path) as f:\n                step_data[fname.replace('_results.json', '')] = json.load(f)\n            print(f'  Loaded {fname}')\n        except Exception as e:\n            print(f'  Parse error: {e}')\n\n# Riemann zeros: LFS-tracked, not downloadable via raw.githubusercontent.com\n# Not needed for Step 5 (periods are computed analytically from prime formula)\nGAMMA = None\nzeros_path = os.path.join(DATA_DIR, 'riemann_zeros_100k_genuine.npy')\nif os.path.exists(zeros_path):\n    try:\n        GAMMA = np.load(zeros_path)\n        print(f'  Loaded {len(GAMMA):,} Riemann zeros (local cache)')\n    except:\n        print('  Could not load cached zeros (not needed for Step 5)')\nelse:\n    print('  Riemann zeros: skipped (LFS file, not needed for Step 5 training)')\n\nprint(f'\\nData loaded: {len(step_data)} JSON files')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. GIFT Constants & G₂ Structure <a id='sec2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1: Topological constants\n",
    "DIM = 7\n",
    "DIM_G2 = 14\n",
    "B2, B3 = 21, 77\n",
    "H_STAR = 99\n",
    "KAPPA_T = 1.0 / 61\n",
    "DET_G = 65.0 / 32.0\n",
    "LAMBDA1 = 14.0 / 99.0\n",
    "N_GEN = 3\n",
    "\n",
    "# TCS building blocks\n",
    "B2_M1, B3_M1 = 11, 40   # quintic in CP4\n",
    "B2_M2, B3_M2 = 10, 37   # CI(2,2,2) in CP6\n",
    "TCS_R1, TCS_R2 = 33, 28  # TCS ratio: H*/(6*dim_G2) ≈ 33/28\n",
    "\n",
    "# Fano plane triples (octonion multiplication)\n",
    "FANO_TRIPLES = [(0,1,2), (0,3,4), (0,5,6), (1,3,5), (1,4,6), (2,3,6), (2,4,5)]\n",
    "FANO_SIGNS   = [+1,      +1,      +1,      +1,      -1,      -1,      -1     ]\n",
    "\n",
    "# All C(7,3) = 35 triples\n",
    "ALL_TRIPLES = list(combinations(range(DIM), 3))\n",
    "TRIPLE_INDEX = {t: i for i, t in enumerate(ALL_TRIPLES)}\n",
    "\n",
    "print(f'K7 topology: dim={DIM}, b2={B2}, b3={B3}, H*={H_STAR}')\n",
    "print(f'Target: det(g)={DET_G}, lambda1={LAMBDA1:.6f}, kappa_T={KAPPA_T:.6f}')\n",
    "print(f'TCS: M1(b2={B2_M1},b3={B3_M1}) + M2(b2={B2_M2},b3={B3_M2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.2: Build standard associative 3-form phi0\n",
    "\n",
    "def build_phi0_components():\n",
    "    \"\"\"35-component vector of the standard associative 3-form.\"\"\"\n",
    "    phi = np.zeros(35, dtype=np.float64)\n",
    "    for (i,j,k), s in zip(FANO_TRIPLES, FANO_SIGNS):\n",
    "        phi[TRIPLE_INDEX[(i,j,k)]] = s\n",
    "    return phi\n",
    "\n",
    "def build_phi0_tensor():\n",
    "    \"\"\"Full 7x7x7 antisymmetric tensor.\"\"\"\n",
    "    phi = build_phi0_components()\n",
    "    T = np.zeros((DIM, DIM, DIM), dtype=np.float64)\n",
    "    for idx, (i,j,k) in enumerate(ALL_TRIPLES):\n",
    "        val = phi[idx]\n",
    "        T[i,j,k] = val;  T[j,k,i] = val;  T[k,i,j] = val\n",
    "        T[i,k,j] = -val; T[k,j,i] = -val; T[j,i,k] = -val\n",
    "    return T\n",
    "\n",
    "PHI0_COMPS = build_phi0_components()\n",
    "PHI0_TENSOR = build_phi0_tensor()\n",
    "\n",
    "# Scale factor for det(g) = 65/32\n",
    "C_REF = DET_G ** (1.0 / 14.0)  # phi_ref = C_REF * phi0\n",
    "\n",
    "# Verify: metric from phi0 = I_7\n",
    "g0 = np.einsum('ikl,jkl->ij', PHI0_TENSOR, PHI0_TENSOR) / 6.0\n",
    "assert np.allclose(g0, np.eye(DIM)), f'g0 from phi0 is not I_7!\\n{g0}'\n",
    "print(f'phi0: {np.count_nonzero(PHI0_COMPS)} nonzero / 35, ||phi0|| = {np.linalg.norm(PHI0_COMPS):.6f}')\n",
    "print(f'c_ref = (65/32)^(1/14) = {C_REF:.6f}')\n",
    "print(f'g_ref = {C_REF**2:.6f} * I_7, det(g_ref) = {C_REF**14:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.3: G2 generators and Lie derivatives\n",
    "\n",
    "def g2_generators():\n",
    "    \"\"\"14 generators of G2 in so(7).\"\"\"\n",
    "    gens = np.zeros((14, DIM, DIM), dtype=np.float64)\n",
    "    # First 7: rotations in Fano planes\n",
    "    for idx, (i,j,k) in enumerate(FANO_TRIPLES):\n",
    "        gens[idx, i, j] = 1; gens[idx, j, i] = -1\n",
    "    # Remaining 7: mixed rotations\n",
    "    for idx in range(7):\n",
    "        i, j, k = idx, (idx+1) % 7, (idx+3) % 7\n",
    "        gens[7+idx, i, k] = 1;   gens[7+idx, k, i] = -1\n",
    "        gens[7+idx, j, k] = 0.5; gens[7+idx, k, j] = -0.5\n",
    "    # Normalize\n",
    "    for idx in range(14):\n",
    "        norm = np.linalg.norm(gens[idx])\n",
    "        if norm > 1e-10:\n",
    "            gens[idx] /= norm\n",
    "    return gens\n",
    "\n",
    "def precompute_lie_derivatives():\n",
    "    \"\"\"Lie derivatives of phi0 along G2 generators: 14 x 35 matrix.\"\"\"\n",
    "    gens = g2_generators()\n",
    "    L = np.zeros((14, 35), dtype=np.float64)\n",
    "    for a in range(14):\n",
    "        X = gens[a]\n",
    "        idx = 0\n",
    "        for i in range(DIM):\n",
    "            for j in range(i+1, DIM):\n",
    "                for k in range(j+1, DIM):\n",
    "                    val = sum(X[i,l]*PHI0_TENSOR[l,j,k] + X[j,l]*PHI0_TENSOR[i,l,k]\n",
    "                              + X[k,l]*PHI0_TENSOR[i,j,l] for l in range(DIM))\n",
    "                    L[a, idx] = val\n",
    "                    idx += 1\n",
    "    return L\n",
    "\n",
    "G2_GENS = g2_generators()\n",
    "LIE_DERIVS = precompute_lie_derivatives()\n",
    "print(f'G2 generators: {G2_GENS.shape} (14 elements of so(7))')\n",
    "print(f'Lie derivatives: {LIE_DERIVS.shape} (14 x 35 matrix)')\n",
    "print(f'Rank of Lie derivative matrix: {np.linalg.matrix_rank(LIE_DERIVS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.4: G2 decomposition verification\n",
    "\n",
    "# Verify Lambda^3 = 1 + 7 + 27\n",
    "phi0_norm = np.linalg.norm(PHI0_COMPS)\n",
    "e1 = PHI0_COMPS / phi0_norm  # unit vector along Lambda^3_1\n",
    "\n",
    "# Lambda^3_7 from coassociative 4-form contraction\n",
    "# (already verified in Step 4)\n",
    "\n",
    "# Projection of Fano-aligned forms\n",
    "fano_idx = [TRIPLE_INDEX[t] for t in FANO_TRIPLES]\n",
    "non_fano_idx = [i for i in range(35) if i not in fano_idx]\n",
    "\n",
    "proj_1 = np.array([e1[k]**2 for k in range(35)])\n",
    "print(f'G2 decomposition: Lambda^3 = 1 + 7 + 27 = 35')\n",
    "print(f'  Fano projection onto Lambda^3_1: {proj_1[fano_idx[0]]:.4f} each (=1/7)')\n",
    "print(f'  Non-Fano projection: {proj_1[non_fano_idx[0]]:.4f} (=0)')\n",
    "print(f'\\nModuli: 77 = 1 (volume) + 0 (gauge, b1=0) + 76 (shape)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prime-Spectral Periods <a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Prime sieve and period map\n",
    "\n",
    "def sieve(N):\n",
    "    is_p = np.ones(N+1, dtype=bool); is_p[:2] = False\n",
    "    for i in range(2, int(N**0.5)+1):\n",
    "        if is_p[i]: is_p[i*i::i] = False\n",
    "    return list(np.where(is_p)[0])\n",
    "\n",
    "PRIMES_77 = sieve(400)[:B3]  # first 77 primes: 2, 3, ..., 389\n",
    "\n",
    "THETA_0, THETA_1 = 1.4091, -3.9537  # Adaptive cutoff parameters\n",
    "\n",
    "def period_map(T, primes=PRIMES_77):\n",
    "    \"\"\"Compute 77 prime periods Pi_k(T).\"\"\"\n",
    "    log_X = THETA_0 * np.log(T) + THETA_1\n",
    "    if log_X < 0.5:\n",
    "        log_X = 0.5\n",
    "    Pi = np.zeros(len(primes))\n",
    "    for k, p in enumerate(primes):\n",
    "        logp = np.log(float(p))\n",
    "        x = logp / log_X\n",
    "        w = np.cos(np.pi * x / 2.0)**2 if x < 1 else 0.0\n",
    "        Pi[k] = KAPPA_T * w / np.sqrt(float(p))\n",
    "    return Pi\n",
    "\n",
    "print(f'77 primes: {PRIMES_77[0]}, {PRIMES_77[1]}, ..., {PRIMES_77[-1]}')\n",
    "print(f'Adaptive cutoff: theta(T) = {THETA_0} + {THETA_1}/log(T)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Compute periods at multiple scales\n",
    "\n",
    "T_SCALES = [100, 1000, 10000, 40000, 75000]\n",
    "T_REF = 40434.2\n",
    "\n",
    "PERIODS = {}\n",
    "for T in T_SCALES + [T_REF]:\n",
    "    Pi = period_map(T)\n",
    "    PERIODS[T] = Pi\n",
    "\n",
    "Pi_ref = PERIODS[T_REF]\n",
    "print(f'Reference scale T = {T_REF:.0f}:')\n",
    "print(f'  ||Pi||_2 = {np.linalg.norm(Pi_ref):.6f}')\n",
    "print(f'  Local (35):  ||Pi_L|| = {np.linalg.norm(Pi_ref[:35]):.6f}')\n",
    "print(f'  Global M1:   ||Pi_G1|| = {np.linalg.norm(Pi_ref[35:56]):.6f}')\n",
    "print(f'  Global M2:   ||Pi_G2|| = {np.linalg.norm(Pi_ref[56:]):.6f}')\n",
    "print(f'\\nPeriods at {len(T_SCALES)} scales computed.')\n",
    "\n",
    "# Convert to torch tensors\n",
    "PI_TARGETS = {T: torch.tensor(Pi, dtype=DTYPE, device=DEVICE) for T, Pi in PERIODS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.3: Period evolution table\n",
    "\n",
    "print(f'{\"T\":>8s} | {\"||Pi||\":>8s} | {\"||local||\":>10s} | {\"||global||\":>10s} | {\"#active\":>7s}')\n",
    "print(f'{\"-\"*8}-+-{\"-\"*8}-+-{\"-\"*10}-+-{\"-\"*10}-+-{\"-\"*7}')\n",
    "for T in sorted(PERIODS.keys()):\n",
    "    Pi = PERIODS[T]\n",
    "    n_active = np.sum(np.abs(Pi) > 1e-10)\n",
    "    print(f'{T:8.0f} | {np.linalg.norm(Pi):8.5f} | {np.linalg.norm(Pi[:35]):10.6f} | '\n",
    "          f'{np.linalg.norm(Pi[35:]):10.6f} | {n_active:7d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TCS Neck Sampling <a id='sec4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Sampling functions\n",
    "\n",
    "def sample_S3(n, device=DEVICE):\n",
    "    \"\"\"Uniform sampling on S3 via Gaussian normalization.\"\"\"\n",
    "    x = torch.randn(n, 4, device=device, dtype=DTYPE)\n",
    "    return x / x.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def sample_tcs_neck(n, r1=TCS_R1, r2=TCS_R2, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Sample from TCS neck S1 x S3 x S3.\n",
    "    Returns (n, 7) coordinates + full quaternions for distance computation.\n",
    "    \"\"\"\n",
    "    theta = 2 * np.pi * torch.rand(n, 1, device=device, dtype=DTYPE)\n",
    "    q1 = sample_S3(n, device)\n",
    "    q2 = sample_S3(n, device)\n",
    "    # Stereographic projection to get 3D from each S3\n",
    "    q1_3d = q1[:, 1:4] / (1 + q1[:, 0:1].abs() + 1e-8)\n",
    "    q2_3d = q2[:, 1:4] / (1 + q2[:, 0:1].abs() + 1e-8)\n",
    "    coords = torch.cat([theta, q1_3d, q2_3d], dim=-1)  # (n, 7)\n",
    "    return coords, q1, q2\n",
    "\n",
    "def geodesic_dist_S3(q1, q2):\n",
    "    \"\"\"Geodesic distance on S3 (quaternionic).\"\"\"\n",
    "    dot = torch.abs(torch.sum(q1 * q2, dim=-1))\n",
    "    dot = torch.clamp(dot, 0, 1)\n",
    "    return torch.arccos(dot)\n",
    "\n",
    "# Test sampling\n",
    "x_test, q1_test, q2_test = sample_tcs_neck(1000)\n",
    "print(f'TCS neck sampling: {x_test.shape}')\n",
    "print(f'  Coordinate ranges:')\n",
    "for i, name in enumerate(['theta', 'q1_x', 'q1_y', 'q1_z', 'q2_x', 'q2_y', 'q2_z']):\n",
    "    print(f'    {name}: [{x_test[:,i].min():.3f}, {x_test[:,i].max():.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Verify sampling distribution\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].hist(x_test[:,0].cpu().numpy(), bins=30, alpha=0.7)\n",
    "axes[0].set_title('theta (S1)')\n",
    "axes[1].hist(torch.norm(x_test[:,1:4], dim=-1).cpu().numpy(), bins=30, alpha=0.7)\n",
    "axes[1].set_title('||q1_3d|| (stereographic)')\n",
    "axes[2].hist(torch.norm(x_test[:,4:7], dim=-1).cpu().numpy(), bins=30, alpha=0.7)\n",
    "axes[2].set_title('||q2_3d|| (stereographic)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sampling_check.png', dpi=100)\n",
    "plt.show()\n",
    "print('Sampling OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PINN Model Architecture <a id='sec5'></a>\n",
    "\n",
    "Architecture: `G2MetricPINN` (~120K parameters)\n",
    "\n",
    "```\n",
    "Input: (x1,...,x7, log T) in R8\n",
    "  |-> FourierFeatures(48 freq) -> R96\n",
    "  |-> MLP: 96 -> 256 -> 256 -> 256 -> 128\n",
    "  |-> Local head: 128 -> 14 (G2 adjoint) -> Lie derivs -> 35\n",
    "  |-> Global head: 128 -> 42 (TCS modes)\n",
    "  |-> phi = c*phi0 + 0.1 * [delta_local || delta_global]\n",
    "  |-> g_ij = (1/6) sum_{kl} phi_ikl * phi_jkl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: FourierFeatures\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Random Fourier feature encoding for periodic structure.\"\"\"\n",
    "    def __init__(self, input_dim=8, num_frequencies=48, scale=2.0):\n",
    "        super().__init__()\n",
    "        self.output_dim = 2 * num_frequencies\n",
    "        B = torch.randn(num_frequencies, input_dim, dtype=DTYPE) * scale\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = 2 * np.pi * torch.matmul(x, self.B.T)\n",
    "        return torch.cat([torch.cos(projected), torch.sin(projected)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: G2MetricPINN model\n",
    "\n",
    "class G2MetricPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for G2 holonomy metric on K7.\n",
    "    \n",
    "    Key features:\n",
    "    - Scale input (log T as 8th dimension)\n",
    "    - G2-adjoint parameterization (14 DOF -> 35 via Lie derivatives)\n",
    "    - Separate global head for 42 TCS modes\n",
    "    - Perturbation scale 0.1 for non-trivial anisotropy\n",
    "    \"\"\"\n",
    "    def __init__(self, n_freq=48, hidden=[256, 256, 256, 128],\n",
    "                 perturbation_scale=0.1, include_scale=True):\n",
    "        super().__init__()\n",
    "        self.perturbation_scale = perturbation_scale\n",
    "        self.include_scale = include_scale\n",
    "        input_dim = 8 if include_scale else 7\n",
    "        \n",
    "        # Buffers\n",
    "        self.register_buffer('phi0_comps', torch.tensor(C_REF * PHI0_COMPS, dtype=DTYPE))\n",
    "        self.register_buffer('phi0_tensor', torch.tensor(C_REF * PHI0_TENSOR, dtype=DTYPE))\n",
    "        self.register_buffer('lie_derivs', torch.tensor(LIE_DERIVS, dtype=DTYPE))\n",
    "        \n",
    "        # Fourier features\n",
    "        self.fourier = FourierFeatures(input_dim=input_dim, num_frequencies=n_freq)\n",
    "        \n",
    "        # Shared backbone\n",
    "        layers = []\n",
    "        in_dim = self.fourier.output_dim\n",
    "        for h_dim in hidden:\n",
    "            layers.extend([nn.Linear(in_dim, h_dim), nn.SiLU()])\n",
    "            in_dim = h_dim\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        \n",
    "        # Local head: backbone -> 14 G2 adjoint params -> Lie derivs -> 35\n",
    "        self.local_head = nn.Linear(hidden[-1], 14)\n",
    "        \n",
    "        # Global head: backbone -> 42 TCS product modes\n",
    "        self.global_head = nn.Linear(hidden[-1], 42)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def _make_input(self, x, log_T=None):\n",
    "        \"\"\"Append log(T) as 8th dimension if needed.\"\"\"\n",
    "        if self.include_scale and log_T is not None:\n",
    "            T_col = torch.full((x.shape[0], 1), log_T, device=x.device, dtype=DTYPE)\n",
    "            return torch.cat([x, T_col], dim=-1)\n",
    "        elif self.include_scale:\n",
    "            T_col = torch.full((x.shape[0], 1), np.log(T_REF), device=x.device, dtype=DTYPE)\n",
    "            return torch.cat([x, T_col], dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, log_T=None):\n",
    "        \"\"\"Return 77 moduli components.\"\"\"\n",
    "        inp = self._make_input(x, log_T)\n",
    "        h = self.backbone(self.fourier(inp))\n",
    "        \n",
    "        # Local: G2 adjoint -> Lie derivatives -> 35 components\n",
    "        adjoint = self.local_head(h)  # (N, 14)\n",
    "        delta_local = torch.matmul(adjoint, self.lie_derivs)  # (N, 35)\n",
    "        \n",
    "        # Global: direct 42 components\n",
    "        delta_global = self.global_head(h)  # (N, 42)\n",
    "        \n",
    "        # Full 3-form: phi_ref + perturbation\n",
    "        local_comps = self.phi0_comps.unsqueeze(0) + self.perturbation_scale * delta_local\n",
    "        global_comps = self.perturbation_scale * delta_global\n",
    "        \n",
    "        return local_comps, global_comps, adjoint\n",
    "    \n",
    "    def phi_tensor(self, x, log_T=None):\n",
    "        \"\"\"Full (N,7,7,7) antisymmetric tensor from local 35 components.\"\"\"\n",
    "        local_comps, _, _ = self.forward(x, log_T)\n",
    "        N = local_comps.shape[0]\n",
    "        phi = torch.zeros(N, DIM, DIM, DIM, device=x.device, dtype=DTYPE)\n",
    "        idx = 0\n",
    "        for i in range(DIM):\n",
    "            for j in range(i+1, DIM):\n",
    "                for k in range(j+1, DIM):\n",
    "                    val = local_comps[:, idx]\n",
    "                    phi[:,i,j,k] = val; phi[:,j,k,i] = val; phi[:,k,i,j] = val\n",
    "                    phi[:,j,i,k] = -val; phi[:,i,k,j] = -val; phi[:,k,j,i] = -val\n",
    "                    idx += 1\n",
    "        return phi\n",
    "    \n",
    "    def metric(self, x, log_T=None):\n",
    "        \"\"\"Compute metric g_ij = (1/6) sum_{kl} phi_ikl phi_jkl.\"\"\"\n",
    "        phi = self.phi_tensor(x, log_T)\n",
    "        return torch.einsum('nikl,njkl->nij', phi, phi) / 6.0\n",
    "    \n",
    "    def det_g(self, x, log_T=None):\n",
    "        return torch.linalg.det(self.metric(x, log_T))\n",
    "    \n",
    "    def moduli_77(self, x, log_T=None):\n",
    "        \"\"\"Return all 77 moduli components as a single vector.\"\"\"\n",
    "        local, glob, _ = self.forward(x, log_T)\n",
    "        # Extract perturbation: (phi - phi_ref) for local\n",
    "        delta_local = local - self.phi0_comps.unsqueeze(0)\n",
    "        return torch.cat([delta_local, glob], dim=-1)  # (N, 77)\n",
    "\n",
    "# Instantiate and report\n",
    "model = G2MetricPINN().to(DEVICE)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'G2MetricPINN: {n_params:,} parameters')\n",
    "print(f'  Backbone: FourierFeatures(48) -> MLP(256,256,256,128)')\n",
    "print(f'  Local head: 128 -> 14 (G2 adjoint) -> 35 (Lie derivatives)')\n",
    "print(f'  Global head: 128 -> 42 (TCS modes)')\n",
    "print(f'  Perturbation scale: {model.perturbation_scale}')\n",
    "\n",
    "# Quick test\n",
    "with torch.no_grad():\n",
    "    x_test = sample_tcs_neck(100)[0]\n",
    "    g_test = model.metric(x_test)\n",
    "    det_test = torch.linalg.det(g_test)\n",
    "    print(f'\\nQuick test (100 points):')\n",
    "    print(f'  det(g) mean: {det_test.mean():.6f} (target: {DET_G:.6f})')\n",
    "    print(f'  det(g) std:  {det_test.std():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Loss Function (v2) <a id='sec6'></a>\n\nTwo tiers:\n- **Tier 1** (every batch): determinant, positive-definiteness, torsion, sparsity\n- **Tier 2** (every batch): multi-scale period matching (5 scales) + anisotropy (Step 4 target metric)\n\n**v2 change**: Spectral gap loss **removed**. $\\lambda_1 = 14/99$ is a **global** property\nof compact K₇ — the Rayleigh quotient on the local TCS neck patch gives\n$\\lambda_1^{\\text{local}} \\approx 1/c^2 \\approx 0.90$, which is the correct local value\nand mathematically cannot reach the global target 0.141. Replaced by anisotropy loss\nusing the explicit metric from Step 4."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Tier 1 — Pointwise losses\n",
    "\n",
    "def loss_determinant(model, x, log_T=None):\n",
    "    \"\"\"L_det = (det(g) - 65/32)^2.\"\"\"\n",
    "    det_g = model.det_g(x, log_T)\n",
    "    return torch.mean((det_g - DET_G) ** 2)\n",
    "\n",
    "def loss_positive_definite(model, x, log_T=None):\n",
    "    \"\"\"L_pd = ReLU(-eigenvalues)^2.\"\"\"\n",
    "    g = model.metric(x, log_T)\n",
    "    eigvals = torch.linalg.eigvalsh(g)\n",
    "    return torch.mean(F.relu(-eigvals) ** 2)\n",
    "\n",
    "def loss_torsion(model, x, log_T=None):\n",
    "    \"\"\"Torsion proxy: penalize spatial variation of phi.\"\"\"\n",
    "    eps = 0.01\n",
    "    local_0, _, _ = model.forward(x, log_T)\n",
    "    torsion = 0.0\n",
    "    for dim in range(DIM):\n",
    "        x_plus = x.clone()\n",
    "        x_plus[:, dim] += eps\n",
    "        local_plus, _, _ = model.forward(x_plus, log_T)\n",
    "        dphi = (local_plus - local_0) / eps\n",
    "        torsion += (dphi ** 2).mean()\n",
    "    return torsion / DIM\n",
    "\n",
    "def loss_sparse(model, x, log_T=None):\n",
    "    \"\"\"L_sparse = ||adjoint||^2 (regularization).\"\"\"\n",
    "    _, _, adjoint = model.forward(x, log_T)\n",
    "    return torch.mean(adjoint ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6.2: Tier 2a — Period loss (multi-scale)\n\ndef loss_periods(model, x, T_val=T_REF, log_T=None):\n    \"\"\"Period loss at a single scale.\"\"\"\n    if log_T is None:\n        log_T = np.log(T_val)\n    moduli = model.moduli_77(x, log_T)  # (N, 77)\n    Pi_target = PI_TARGETS[T_val]       # (77,)\n    moduli_mean = moduli.mean(dim=0)    # (77,)\n    return torch.mean((moduli_mean - Pi_target) ** 2)\n\ndef loss_periods_multiscale(model, x):\n    \"\"\"Period loss averaged over all 5 energy scales simultaneously.\n    \n    Same batch of TCS points, different log(T) → different moduli targets.\n    This trains the scale dimension of the PINN.\n    \"\"\"\n    total = torch.tensor(0.0, device=DEVICE, dtype=DTYPE)\n    for T in T_SCALES:\n        moduli = model.moduli_77(x, np.log(T)).mean(dim=0)\n        target = PI_TARGETS[T]\n        total = total + torch.mean((moduli - target) ** 2)\n    return total / len(T_SCALES)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6.3: Tier 2b — Anisotropy loss (Step 4 target metric)\n\n# Load and rescale Step 4 target metric to have det = 65/32\nif 'harmonic_forms' in step_data and 'full_metric' in step_data['harmonic_forms']:\n    G_TARGET_RAW = np.array(step_data['harmonic_forms']['full_metric']['g_full_matrix'])\n    det_raw = np.linalg.det(G_TARGET_RAW)\n    # Rescale: det(s*A) = s^7 * det(A), so s = (DET_G/det_raw)^(1/7)\n    rescale = (DET_G / det_raw) ** (1.0 / DIM)\n    G_TARGET_NP = rescale * G_TARGET_RAW\n    HAS_TARGET = True\n    eigs_tgt = np.linalg.eigvalsh(G_TARGET_NP)\n    print(f'Step 4 target metric loaded and rescaled:')\n    print(f'  Original det = {det_raw:.6f}, rescaled det = {np.linalg.det(G_TARGET_NP):.6f}')\n    print(f'  Rescale factor: {rescale:.6f}')\n    print(f'  Condition number: {eigs_tgt.max()/eigs_tgt.min():.6f}')\n    print(f'  Diagonal: [{\", \".join(f\"{v:.5f}\" for v in np.diag(G_TARGET_NP))}]')\n    offdiag_max = np.max(np.abs(G_TARGET_NP - np.diag(np.diag(G_TARGET_NP))))\n    print(f'  Off-diagonal max: {offdiag_max:.6f}')\nelse:\n    # Fallback: isotropic metric (anisotropy loss will be zero — no signal)\n    G_TARGET_NP = C_REF**2 * np.eye(DIM)\n    HAS_TARGET = False\n    print('WARNING: Step 4 data not available. Using isotropic fallback.')\n\nG_TARGET = torch.tensor(G_TARGET_NP, dtype=DTYPE, device=DEVICE)\n\n\ndef loss_anisotropy(model, x, log_T=None):\n    \"\"\"Match spatial average of metric to Step 4 target structure.\n    \n    The target encodes the Fano-plane anisotropy from Step 4:\n    - 7 Fano-aligned modes: Tr(dg/dPi) = ±2.104 (volume-changing)\n    - 28 non-Fano modes: traceless (pure shape)\n    - Off-diagonal elements ~0.003 encode cross-coupling\n    \"\"\"\n    if log_T is None:\n        log_T = np.log(T_REF)\n    g = model.metric(x, log_T)  # (N, 7, 7) — with gradients\n    g_mean = g.mean(dim=0)      # (7, 7)\n    return torch.sum((g_mean - G_TARGET) ** 2)\n\n\n# Post-hoc spectral evaluation (diagnostic only, not for training)\ndef evaluate_spectral_posthoc(model, n_samples=5000):\n    \"\"\"\n    Rayleigh quotient on TCS neck — diagnostic purposes only.\n    \n    NOTE: This measures the LOCAL spectral gap on the coordinate patch,\n    not the global lambda1 of compact K7. The global value (14/99)\n    depends on the full manifold topology and cannot be computed locally.\n    \"\"\"\n    x, _, _ = sample_tcs_neck(n_samples)\n    with torch.no_grad():\n        g = model.metric(x)\n        g_inv = torch.linalg.inv(g)\n        det_g = torch.linalg.det(g)\n        sqrt_det = torch.sqrt(torch.abs(det_g) + 1e-10)\n    \n    rayleigh_values = []\n    for dim_idx in range(DIM):\n        for freq in [1, 2, 3]:\n            f = torch.cos(freq * x[:, dim_idx])\n            f = f - f.mean()\n            if f.var() < 1e-10:\n                continue\n            grad_f = torch.zeros(n_samples, DIM, device=x.device, dtype=DTYPE)\n            grad_f[:, dim_idx] = -freq * torch.sin(freq * x[:, dim_idx])\n            grad_f_norm_sq = torch.einsum('ni,nij,nj->n', grad_f, g_inv, grad_f)\n            R = float((grad_f_norm_sq * sqrt_det).mean() / ((f**2 * sqrt_det).mean() + 1e-10))\n            rayleigh_values.append(R)\n    \n    for i in range(DIM):\n        for j in range(i+1, min(i+3, DIM)):\n            f = torch.cos(x[:, i]) * torch.cos(x[:, j])\n            f = f - f.mean()\n            if f.var() < 1e-10:\n                continue\n            grad_f = torch.zeros(n_samples, DIM, device=x.device, dtype=DTYPE)\n            grad_f[:, i] = -torch.sin(x[:, i]) * torch.cos(x[:, j])\n            grad_f[:, j] = -torch.cos(x[:, i]) * torch.sin(x[:, j])\n            grad_f_norm_sq = torch.einsum('ni,nij,nj->n', grad_f, g_inv, grad_f)\n            R = float((grad_f_norm_sq * sqrt_det).mean() / ((f**2 * sqrt_det).mean() + 1e-10))\n            rayleigh_values.append(R)\n    \n    if rayleigh_values:\n        vals = sorted(rayleigh_values)\n        return vals[0], vals[:5]\n    return 0.0, []\n\n\n# Quick tests\nwith torch.no_grad():\n    x_test = sample_tcs_neck(500)[0]\n    L_a = loss_anisotropy(model, x_test)\n    print(f'\\nInitial anisotropy loss: {float(L_a):.6f}')\n    print(f'  => Weighted (×500): {float(L_a) * 500:.4f}')\n    \n    lam1, _ = evaluate_spectral_posthoc(model, n_samples=500)\n    print(f'\\nLocal Rayleigh quotient (diagnostic only):')\n    print(f'  lambda1_local = {lam1:.4f}, lambda1_local * H* = {lam1*99:.1f}')\n    print(f'  Global prediction: lambda1 = 14/99 = {LAMBDA1:.4f} (topological, not trained)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6.4: Loss weights (v2 — anisotropy replaces spectral)\n\nLOSS_WEIGHTS = {\n    'det': 100.0,       # Determinant = 65/32\n    'pd': 50.0,         # Positive definiteness\n    'torsion': 1.0,     # Torsion proxy (spatial gradient penalty)\n    'sparse': 0.01,     # Regularization (reduced to allow non-trivial structure)\n    'period': 1000.0,   # Period integrals at 5 scales (100x boost from v1)\n    'aniso': 500.0,     # Step 4 target metric structure (NEW, replaces spectral)\n}\n\nprint('Loss weights (v2):')\nfor k, v in LOSS_WEIGHTS.items():\n    print(f'  {k}: {v}')\nprint(f'\\nv2 changes:')\nprint(f'  REMOVED: spectral (lambda1 is global, Rayleigh quotient is local)')\nprint(f'  ADDED:   aniso = 500 (Step 4 metric target, Frobenius norm)')\nprint(f'  BOOSTED: period 10 -> 1000 (main geometric signal, 5 scales)')\nprint(f'  REDUCED: sparse 0.1 -> 0.01 (allow non-trivial parameters)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Infrastructure <a id='sec7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7.1: Checkpointing\n\nCKPT_DIR = 'checkpoints'\nos.makedirs(CKPT_DIR, exist_ok=True)\n\n# Try mounting Google Drive for persistent storage\nDRIVE_DIR = None\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_DIR = '/content/drive/MyDrive/GIFT_PINN'\n    os.makedirs(DRIVE_DIR, exist_ok=True)\n    print(f'Google Drive mounted: {DRIVE_DIR}')\nexcept:\n    print('No Google Drive (running locally). Checkpoints in ./checkpoints/')\n\ndef save_checkpoint(model, optimizer, epoch, history, label='latest'):\n    state = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'history': history,\n    }\n    path = os.path.join(CKPT_DIR, f'step5_{label}.pt')\n    torch.save(state, path)\n    if DRIVE_DIR:\n        torch.save(state, os.path.join(DRIVE_DIR, f'step5_{label}.pt'))\n    return path\n\ndef load_checkpoint(model, optimizer, label='latest'):\n    for d in [DRIVE_DIR, CKPT_DIR]:\n        if d is None: continue\n        path = os.path.join(d, f'step5_{label}.pt')\n        if os.path.exists(path):\n            state = torch.load(path, map_location=DEVICE, weights_only=False)\n            model.load_state_dict(state['model_state_dict'])\n            optimizer.load_state_dict(state['optimizer_state_dict'])\n            print(f'Loaded checkpoint from epoch {state[\"epoch\"]}')\n            return state['epoch'], state['history']\n    print('No checkpoint found, starting fresh.')\n    return 0, {'loss': [], 'det': [], 'torsion': [], 'period': [], 'aniso': []}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7.2: Optimizer and scheduler (v2 — simplified, no spectral phases)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n# 2-phase schedule:\n# Phase 1 (main):      epochs 0-3500,    lr=1e-3 cosine → 1e-5, all losses\n# Phase 2 (fine-tune): epochs 3500-5000, lr=1e-5, all losses\n\nPHASE_1_END = 3500\nPHASE_2_END = 5000\nPHASE_3_END = 5000   # alias for compatibility\nBATCH_SIZE = 2048\n\ndef get_lr(epoch):\n    if epoch < PHASE_1_END:\n        progress = epoch / PHASE_1_END\n        return 1e-3 * (1 + np.cos(np.pi * progress)) / 2 + 1e-5\n    else:\n        return 1e-5\n\ndef get_phase(epoch):\n    if epoch < PHASE_1_END: return 1\n    return 2\n\nprint(f'Training schedule (v2):')\nprint(f'  Phase 1 (main):      epochs 0-{PHASE_1_END}, lr=1e-3 cosine→1e-5, all losses')\nprint(f'  Phase 2 (fine-tune): epochs {PHASE_1_END}-{PHASE_3_END}, lr=1e-5, all losses')\nprint(f'  Batch size: {BATCH_SIZE}')\nprint(f'  All losses active from epoch 0 (no warmup needed)')\nprint(f'  NO spectral loss — replaced by anisotropy + boosted periods')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7.3: Training step (v2 — period + anisotropy, no spectral)\n\ndef train_step(model, optimizer, epoch, step, history):\n    \"\"\"Single training step: Tier 1 + multi-scale periods + anisotropy.\"\"\"\n    lr = get_lr(epoch)\n    for pg in optimizer.param_groups:\n        pg['lr'] = lr\n    \n    # Sample batch\n    x, _, _ = sample_tcs_neck(BATCH_SIZE)\n    \n    optimizer.zero_grad()\n    \n    # Tier 1: pointwise constraints\n    L_det = loss_determinant(model, x)\n    L_pd = loss_positive_definite(model, x)\n    L_torsion = loss_torsion(model, x)\n    L_sparse = loss_sparse(model, x)\n    \n    total = (LOSS_WEIGHTS['det'] * L_det + LOSS_WEIGHTS['pd'] * L_pd +\n             LOSS_WEIGHTS['torsion'] * L_torsion + LOSS_WEIGHTS['sparse'] * L_sparse)\n    \n    # Tier 2a: multi-scale period matching (every step)\n    L_period = loss_periods_multiscale(model, x)\n    total = total + LOSS_WEIGHTS['period'] * L_period\n    \n    # Tier 2b: anisotropy — match Step 4 metric structure (every step)\n    L_aniso = loss_anisotropy(model, x)\n    total = total + LOSS_WEIGHTS['aniso'] * L_aniso\n    \n    total.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    \n    # Record\n    with torch.no_grad():\n        history['loss'].append(float(total))\n        history['det'].append(float(L_det))\n        history['torsion'].append(float(L_torsion))\n        history['period'].append(float(L_period))\n        history['aniso'].append(float(L_aniso))\n    \n    return float(total)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Training Execution <a id='sec8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.1: Resume from checkpoint if available\n\nstart_epoch, history = load_checkpoint(model, optimizer)\n\n# Ensure all v2 history keys exist (handles v1 checkpoints)\nfor key in ['loss', 'det', 'torsion', 'period', 'aniso']:\n    if key not in history:\n        history[key] = []\n\nbest_loss = float('inf')\nt0_train = time.time()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.2: Phase 1 — Main training (all losses from start)\n\nprint('=' * 70)\nprint('PHASE 1: MAIN TRAINING (period + anisotropy + Tier 1, cosine LR)')\nprint('=' * 70)\n\nstep = 0\nfor epoch in tqdm(range(max(start_epoch, 0), PHASE_1_END), desc='Phase 1'):\n    loss_val = train_step(model, optimizer, epoch, step, history)\n    step += 1\n    \n    if loss_val < best_loss:\n        best_loss = loss_val\n        save_checkpoint(model, optimizer, epoch, history, 'best')\n    \n    if (epoch + 1) % 100 == 0:\n        with torch.no_grad():\n            x_eval = sample_tcs_neck(1000)[0]\n            det_eval = model.det_g(x_eval)\n            L_p = float(loss_periods_multiscale(model, x_eval))\n            L_a = float(loss_anisotropy(model, x_eval))\n        det_dev = 100 * abs(float(det_eval.mean()) - DET_G) / DET_G\n        print(f'  Epoch {epoch+1:4d} | loss={loss_val:.2e} | '\n              f'det={det_dev:.3f}% | period={L_p:.2e} | aniso={L_a:.2e} | '\n              f'lr={get_lr(epoch):.1e}')\n    \n    if (epoch + 1) % 500 == 0:\n        save_checkpoint(model, optimizer, epoch, history, f'epoch{epoch+1}')\n        clear_gpu()\n\nsave_checkpoint(model, optimizer, PHASE_1_END, history, 'phase1')\nprint(f'Phase 1 complete. Time: {time.time()-t0_train:.0f}s')\nclear_gpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.3: Phase 2 — Fine-tuning (reduced learning rate)\n\nprint('\\n' + '=' * 70)\nprint('PHASE 2: FINE-TUNING (all losses, lr=1e-5)')\nprint('=' * 70)\n\nt1 = time.time()\nfor epoch in tqdm(range(max(start_epoch, PHASE_1_END), PHASE_3_END), desc='Phase 2'):\n    loss_val = train_step(model, optimizer, epoch, step, history)\n    step += 1\n    \n    if loss_val < best_loss:\n        best_loss = loss_val\n        save_checkpoint(model, optimizer, epoch, history, 'best')\n    \n    if (epoch + 1) % 100 == 0:\n        with torch.no_grad():\n            x_eval = sample_tcs_neck(1000)[0]\n            det_eval = model.det_g(x_eval)\n            L_p = float(loss_periods_multiscale(model, x_eval))\n            L_a = float(loss_anisotropy(model, x_eval))\n        det_dev = 100 * abs(float(det_eval.mean()) - DET_G) / DET_G\n        print(f'  Epoch {epoch+1:4d} | loss={loss_val:.2e} | '\n              f'det={det_dev:.3f}% | period={L_p:.2e} | aniso={L_a:.2e} | '\n              f'lr={get_lr(epoch):.1e}')\n\nsave_checkpoint(model, optimizer, PHASE_3_END, history, 'final')\ntotal_time = time.time() - t0_train\nprint(f'\\nTraining complete! Total time: {total_time/60:.1f} minutes')\nclear_gpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.4: Post-training spectral diagnostic\n\nprint('=' * 70)\nprint('POST-TRAINING SPECTRAL DIAGNOSTIC')\nprint('=' * 70)\nprint()\nprint('Note: lambda1 = 14/99 is a GLOBAL property of compact K7.')\nprint('The Rayleigh quotient on the TCS neck measures LOCAL eigenvalues.')\nprint('For nearly-isotropic metric with det = 65/32:')\nprint('  lambda1_local ~ 1/c^2 ~ 0.90 (expected for any valid K7 metric)')\nprint('  lambda1_global = 14/99 = 0.141 (from topology, not computable locally)')\nprint()\n\nwith torch.no_grad():\n    lam1_local, top5 = evaluate_spectral_posthoc(model, n_samples=5000)\n    print(f'Local Rayleigh quotient: lambda1_local = {lam1_local:.6f}')\n    print(f'  lambda1_local * H* = {lam1_local * H_STAR:.2f}')\n    print(f'  Top 5 local values: {[f\"{v:.4f}\" for v in top5]}')\n    print()\n    print(f'Global prediction (analytical): lambda1 = 14/99 = {LAMBDA1:.6f}')\n    print(f'  lambda1 * H* = 14.0 (topological)')\n\n# Store for later use in JSON export\nlambda1_final = lam1_local"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Evaluation & Metric Extraction <a id='sec9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.1: Load best model and generate high-res evaluation\n",
    "\n",
    "# Load best model\n",
    "load_checkpoint(model, optimizer, 'best')\n",
    "model.eval()\n",
    "\n",
    "N_EVAL = 50000\n",
    "print(f'Evaluating on {N_EVAL:,} points...')\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_eval, q1_eval, q2_eval = sample_tcs_neck(N_EVAL)\n",
    "    \n",
    "    # Evaluate in batches to avoid memory issues\n",
    "    batch = 5000\n",
    "    g_all = []\n",
    "    phi_all = []\n",
    "    for i in range(0, N_EVAL, batch):\n",
    "        x_batch = x_eval[i:i+batch]\n",
    "        g_batch = model.metric(x_batch)\n",
    "        local, glob, _ = model.forward(x_batch)\n",
    "        g_all.append(g_batch.cpu().numpy())\n",
    "        phi_all.append(local.cpu().numpy())\n",
    "        if (i // batch) % 5 == 0:\n",
    "            print(f'  Batch {i//batch + 1}/{(N_EVAL+batch-1)//batch}')\n",
    "    \n",
    "    g_all = np.concatenate(g_all, axis=0)\n",
    "    phi_all = np.concatenate(phi_all, axis=0)\n",
    "\n",
    "print(f'\\nMetric tensor shape: {g_all.shape}')\n",
    "print(f'3-form shape: {phi_all.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.2: Metric statistics\n",
    "\n",
    "det_all = np.linalg.det(g_all)\n",
    "eigvals_all = np.linalg.eigvalsh(g_all)\n",
    "\n",
    "print('METRIC STATISTICS')\n",
    "print('=' * 50)\n",
    "print(f'  det(g): mean={np.mean(det_all):.6f}, std={np.std(det_all):.6f}')\n",
    "print(f'          target={DET_G:.6f}, deviation={100*abs(np.mean(det_all)-DET_G)/DET_G:.3f}%')\n",
    "print(f'  Eigenvalues:')\n",
    "print(f'    min:  {eigvals_all.min():.6f}')\n",
    "print(f'    max:  {eigvals_all.max():.6f}')\n",
    "print(f'    mean: {eigvals_all.mean():.6f}')\n",
    "print(f'  Positive definite: {np.all(eigvals_all > 0)}')\n",
    "print(f'  Mean condition number: {(eigvals_all.max(axis=1)/eigvals_all.min(axis=1)).mean():.4f}')\n",
    "print(f'  Off-diagonal max: {np.max(np.abs(g_all - np.einsum(\"nii->ni\", g_all)[:,:,None] * np.eye(DIM)[None,:,:])):.6f}')\n",
    "\n",
    "# Mean metric tensor\n",
    "g_mean = np.mean(g_all, axis=0)\n",
    "print(f'\\nMean metric tensor g_mean:')\n",
    "for i in range(DIM):\n",
    "    row = ' '.join(f'{g_mean[i,j]:+.5f}' for j in range(DIM))\n",
    "    print(f'  [{row}]')\n",
    "print(f'  det(g_mean) = {np.linalg.det(g_mean):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9.3: Spectral analysis — local diagnostic + global analytical\n\nprint('\\nSPECTRAL ANALYSIS')\nprint('=' * 50)\n\nprint('LOCAL (Rayleigh quotient on TCS neck):')\nprint(f'  lambda1_local = {lambda1_final:.6f}')\nprint(f'  lambda1_local * H* = {lambda1_final * H_STAR:.2f}')\nprint(f'  This is the correct local value for g ~ {C_REF**2:.3f} * I_7')\n\nprint(f'\\nGLOBAL (topological prediction, not computed):')\nprint(f'  lambda1 = 14/99 = {LAMBDA1:.6f}')\nprint(f'  lambda1 * H* = 14.0')\nprint(f'  Derived from: b2={B2}, b3={B3}, H*={H_STAR}')\n\nprint(f'\\nANISOTROPY vs STEP 4 TARGET:')\nprint(f'  Target off-diag max: {np.max(np.abs(G_TARGET_NP - np.diag(np.diag(G_TARGET_NP)))):.6f}')\nprint(f'  Achieved off-diag max: {np.max(np.abs(g_mean - np.diag(np.diag(g_mean)))):.6f}')\nprint(f'  ||g_mean - G_target||_F = {np.linalg.norm(g_mean - G_TARGET_NP):.6f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.4: Period integral verification\n",
    "\n",
    "print('\\nPERIOD INTEGRAL VERIFICATION')\n",
    "print('=' * 50)\n",
    "\n",
    "period_results = {}\n",
    "for T in T_SCALES:\n",
    "    with torch.no_grad():\n",
    "        x_per = sample_tcs_neck(10000)[0]\n",
    "        moduli = model.moduli_77(x_per, np.log(T))\n",
    "        moduli_mean = moduli.mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    Pi_target = PERIODS[T]\n",
    "    rms_error = np.sqrt(np.mean((moduli_mean - Pi_target)**2))\n",
    "    period_results[T] = {'achieved': moduli_mean.tolist(), 'target': Pi_target.tolist(),\n",
    "                         'rms_error': float(rms_error)}\n",
    "    print(f'  T={T:6d}: RMS period error = {rms_error:.6f}, '\n",
    "          f'||Pi_target||={np.linalg.norm(Pi_target):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.5: Multi-scale metric evaluation\n",
    "\n",
    "print('\\nMULTI-SCALE METRIC EVOLUTION')\n",
    "print('=' * 50)\n",
    "\n",
    "scale_metrics = []\n",
    "for T in T_SCALES:\n",
    "    with torch.no_grad():\n",
    "        x_s = sample_tcs_neck(10000)[0]\n",
    "        g_s = model.metric(x_s, np.log(T))\n",
    "        det_s = torch.linalg.det(g_s).cpu().numpy()\n",
    "        eig_s = torch.linalg.eigvalsh(g_s).cpu().numpy()\n",
    "    \n",
    "    scale_metrics.append({\n",
    "        'T': T,\n",
    "        'det_mean': float(np.mean(det_s)),\n",
    "        'det_std': float(np.std(det_s)),\n",
    "        'det_dev_pct': float(100*abs(np.mean(det_s)-DET_G)/DET_G),\n",
    "        'kappa_mean': float((eig_s.max(axis=1)/eig_s.min(axis=1)).mean()),\n",
    "        'pd': bool(np.all(eig_s > 0)),\n",
    "    })\n",
    "    print(f'  T={T:6d}: det(g)={np.mean(det_s):.5f} +/- {np.std(det_s):.5f}, '\n",
    "          f'dev={100*abs(np.mean(det_s)-DET_G)/DET_G:.2f}%, PD={np.all(eig_s > 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Visualization <a id='sec10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10.1: Training curves (4-panel, v2)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Total loss\naxes[0,0].semilogy(history['loss'], alpha=0.5, linewidth=0.5)\naxes[0,0].set_title('Total Loss')\naxes[0,0].set_xlabel('Step')\naxes[0,0].axvline(x=PHASE_1_END, color='r', linestyle='--', alpha=0.5, label='Phase 2')\naxes[0,0].legend()\n\n# Determinant error\naxes[0,1].semilogy(history['det'], alpha=0.5, linewidth=0.5)\naxes[0,1].set_title('Determinant Error (det(g) - 65/32)²')\naxes[0,1].set_xlabel('Step')\n\n# Period loss\nif history.get('period'):\n    axes[1,0].semilogy(history['period'], alpha=0.5, linewidth=0.5)\n    axes[1,0].set_title('Period Loss (5-scale)')\n    axes[1,0].set_xlabel('Step')\nelse:\n    axes[1,0].semilogy(history['torsion'], alpha=0.5, linewidth=0.5)\n    axes[1,0].set_title('Torsion')\n    axes[1,0].set_xlabel('Step')\n\n# Anisotropy loss\nif history.get('aniso'):\n    axes[1,1].semilogy(history['aniso'], alpha=0.5, linewidth=0.5)\n    axes[1,1].set_title('Anisotropy Loss (Step 4 target)')\n    axes[1,1].set_xlabel('Step')\nelse:\n    axes[1,1].text(0.5, 0.5, 'No anisotropy data', ha='center', va='center',\n                   transform=axes[1,1].transAxes)\n\nplt.tight_layout()\nplt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Saved: training_curves.png')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10.2: Period + anisotropy convergence (replaces spectral convergence)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Period loss convergence\nif history.get('period'):\n    axes[0].semilogy(history['period'], alpha=0.5, linewidth=0.5)\n    axes[0].set_xlabel('Step', fontsize=12)\n    axes[0].set_ylabel('Period Loss (MSE)', fontsize=12)\n    axes[0].set_title('Period Matching: 5 Scales', fontsize=14)\n    axes[0].axvline(x=PHASE_1_END, color='r', linestyle='--', alpha=0.5, label='Phase 2')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n# Anisotropy loss convergence\nif history.get('aniso'):\n    axes[1].semilogy(history['aniso'], alpha=0.5, linewidth=0.5)\n    axes[1].set_xlabel('Step', fontsize=12)\n    axes[1].set_ylabel('||g_mean - G_target||²_F', fontsize=12)\n    axes[1].set_title('Metric Anisotropy: Step 4 Target', fontsize=14)\n    axes[1].axvline(x=PHASE_1_END, color='r', linestyle='--', alpha=0.5, label='Phase 2')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('convergence_v2.png', dpi=150)\nplt.show()\nprint('Saved: convergence_v2.png')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.3: Metric field visualization (2D slices)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Diagonal components\n",
    "for i in range(DIM):\n",
    "    ax = axes[i//4, i%4]\n",
    "    ax.hist(g_all[:, i, i], bins=50, alpha=0.7, color=f'C{i}')\n",
    "    ax.axvline(x=C_REF**2, color='r', linestyle='--', label=f'ref={C_REF**2:.4f}')\n",
    "    ax.set_title(f'g_{{{i}{i}}}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Off-diagonal distribution\n",
    "offdiag = []\n",
    "for i in range(DIM):\n",
    "    for j in range(i+1, DIM):\n",
    "        offdiag.extend(g_all[:, i, j].tolist())\n",
    "axes[1, 3].hist(offdiag, bins=50, alpha=0.7, color='gray')\n",
    "axes[1, 3].set_title('All off-diagonal g_ij')\n",
    "axes[1, 3].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "plt.suptitle('Metric Field Components (50K points)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_field.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: metric_field.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.4: Determinant stability across manifold and scales\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Determinant histogram\n",
    "axes[0].hist(det_all, bins=50, alpha=0.7, density=True)\n",
    "axes[0].axvline(x=DET_G, color='r', linewidth=2, linestyle='--', label=f'Target: {DET_G}')\n",
    "axes[0].set_xlabel('det(g)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Determinant Distribution (50K points)', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Determinant vs scale\n",
    "T_vals = [sm['T'] for sm in scale_metrics]\n",
    "det_vals = [sm['det_mean'] for sm in scale_metrics]\n",
    "det_errs = [sm['det_std'] for sm in scale_metrics]\n",
    "axes[1].errorbar(T_vals, det_vals, yerr=det_errs, fmt='bo-', capsize=5)\n",
    "axes[1].axhline(y=DET_G, color='r', linewidth=2, linestyle='--', label=f'Target: {DET_G}')\n",
    "axes[1].set_xlabel('T (scale)', fontsize=12)\n",
    "axes[1].set_ylabel('det(g)', fontsize=12)\n",
    "axes[1].set_title('Determinant vs Scale', fontsize=13)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('det_stability.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: det_stability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.5: Torsion map\n",
    "\n",
    "# Compute torsion at evaluation points\n",
    "eps = 0.01\n",
    "torsion_vals = np.zeros(min(10000, N_EVAL))\n",
    "with torch.no_grad():\n",
    "    x_tor = x_eval[:10000]\n",
    "    phi_0 = model.forward(x_tor)[0]\n",
    "    for dim in range(DIM):\n",
    "        x_p = x_tor.clone()\n",
    "        x_p[:, dim] += eps\n",
    "        phi_p = model.forward(x_p)[0]\n",
    "        dphi = ((phi_p - phi_0) / eps).cpu().numpy()\n",
    "        torsion_vals += np.mean(dphi**2, axis=1)\n",
    "    torsion_vals = np.sqrt(torsion_vals / DIM)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(torsion_vals, bins=50, alpha=0.7)\n",
    "axes[0].axvline(x=0.1, color='r', linestyle='--', label='Joyce bound')\n",
    "axes[0].set_xlabel('||torsion||', fontsize=12)\n",
    "axes[0].set_title('Torsion Distribution', fontsize=13)\n",
    "axes[0].legend()\n",
    "\n",
    "coords_np = x_eval[:10000].cpu().numpy()\n",
    "axes[1].scatter(coords_np[:, 0], coords_np[:, 1], c=torsion_vals, s=1, cmap='hot')\n",
    "axes[1].set_xlabel('theta (S1)', fontsize=12)\n",
    "axes[1].set_ylabel('q1_x', fontsize=12)\n",
    "axes[1].set_title('Torsion Map (2D slice)', fontsize=13)\n",
    "plt.colorbar(axes[1].collections[0], ax=axes[1], label='||torsion||')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('torsion_map.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'Torsion: mean={np.mean(torsion_vals):.6f}, max={np.max(torsion_vals):.6f}')\n",
    "print('Saved: torsion_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.6: Period verification bar chart\n",
    "\n",
    "T_show = T_SCALES[3]  # T = 40000\n",
    "if T_show in period_results:\n",
    "    Pi_t = np.array(period_results[T_show]['target'])\n",
    "    Pi_a = np.array(period_results[T_show]['achieved'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Local periods (first 35)\n",
    "    x_idx = np.arange(35)\n",
    "    axes[0].bar(x_idx - 0.2, Pi_t[:35], width=0.4, alpha=0.7, label='Target')\n",
    "    axes[0].bar(x_idx + 0.2, Pi_a[:35], width=0.4, alpha=0.7, label='PINN')\n",
    "    axes[0].set_xlabel('Modulus index k')\n",
    "    axes[0].set_ylabel('Pi_k')\n",
    "    axes[0].set_title(f'Local Periods (35) at T={T_show}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Global periods (last 42)\n",
    "    x_idx = np.arange(42)\n",
    "    axes[1].bar(x_idx - 0.2, Pi_t[35:], width=0.4, alpha=0.7, label='Target')\n",
    "    axes[1].bar(x_idx + 0.2, Pi_a[35:], width=0.4, alpha=0.7, label='PINN')\n",
    "    axes[1].set_xlabel('Modulus index k - 35')\n",
    "    axes[1].set_ylabel('Pi_k')\n",
    "    axes[1].set_title(f'Global Periods (42) at T={T_show}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('period_verification.png', dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved: period_verification.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Export <a id='sec11'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11.1: Export all results to JSON (v2)\n\nresults = {\n    'metadata': {\n        'notebook': 'K7_PINN_Step5_Reconstruction',\n        'version': 2,\n        'date': time.strftime('%Y-%m-%d'),\n        'gpu': gpu_name if GPU_AVAILABLE else 'CPU',\n        'training_time_min': float(total_time / 60),\n        'n_params': int(n_params),\n        'n_epochs': int(PHASE_3_END),\n    },\n    'model': {\n        'architecture': 'G2MetricPINN',\n        'fourier_freq': 48,\n        'hidden_dims': [256, 256, 256, 128],\n        'perturbation_scale': 0.1,\n        'n_params': int(n_params),\n    },\n    'metric': {\n        'det_mean': float(np.mean(det_all)),\n        'det_std': float(np.std(det_all)),\n        'det_target': float(DET_G),\n        'det_deviation_pct': float(100 * abs(np.mean(det_all) - DET_G) / DET_G),\n        'eigenvalue_min': float(eigvals_all.min()),\n        'eigenvalue_max': float(eigvals_all.max()),\n        'eigenvalue_mean': float(eigvals_all.mean()),\n        'positive_definite': bool(np.all(eigvals_all > 0)),\n        'condition_number_mean': float((eigvals_all.max(axis=1)/eigvals_all.min(axis=1)).mean()),\n        'g_mean_diagonal': np.diag(g_mean).tolist(),\n        'g_mean_matrix': g_mean.tolist(),\n    },\n    'anisotropy': {\n        'target_diagonal': np.diag(G_TARGET_NP).tolist(),\n        'achieved_diagonal': np.diag(g_mean).tolist(),\n        'target_offdiag_max': float(np.max(np.abs(G_TARGET_NP - np.diag(np.diag(G_TARGET_NP))))),\n        'achieved_offdiag_max': float(np.max(np.abs(g_mean - np.diag(np.diag(g_mean))))),\n        'frobenius_error': float(np.linalg.norm(g_mean - G_TARGET_NP)),\n        'target_condition': float(np.max(np.linalg.eigvalsh(G_TARGET_NP)) / np.min(np.linalg.eigvalsh(G_TARGET_NP))),\n        'achieved_condition': float((eigvals_all.max(axis=1)/eigvals_all.min(axis=1)).mean()),\n    },\n    'spectral_local': {\n        'note': 'Local Rayleigh quotient on TCS neck — NOT global lambda1 of K7',\n        'lambda1_local': float(lambda1_final),\n        'lambda1_local_times_Hstar': float(lambda1_final * H_STAR),\n        'expected_local': float(1.0 / C_REF**2),\n        'global_lambda1': float(LAMBDA1),\n        'global_lambda1_times_Hstar': 14.0,\n    },\n    'torsion': {\n        'mean': float(np.mean(torsion_vals)),\n        'max': float(np.max(torsion_vals)),\n        'joyce_bound': 0.1,\n        'passed': bool(np.max(torsion_vals) < 0.1),\n    },\n    'periods': {str(int(T)): pr for T, pr in period_results.items()},\n    'scale_evolution': scale_metrics,\n    'convergence': {\n        'final_loss': float(history['loss'][-1]) if history['loss'] else None,\n        'best_loss': float(best_loss),\n        'final_period_loss': float(history['period'][-1]) if history.get('period') else None,\n        'final_aniso_loss': float(history['aniso'][-1]) if history.get('aniso') else None,\n    },\n    'validation': {\n        'det_passed': bool(abs(np.mean(det_all) - DET_G) / DET_G < 0.01),\n        'pd_passed': bool(np.all(eigvals_all > 0)),\n        'torsion_passed': bool(np.max(torsion_vals) < 0.1),\n        'period_rms_best': float(min(pr.get('rms_error', 1.0) for pr in period_results.values())),\n        'aniso_frobenius': float(np.linalg.norm(g_mean - G_TARGET_NP)),\n    },\n}\n\nwith open('k7_pinn_step5_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\nprint('Saved: k7_pinn_step5_results.json')\n\n# Training history\nwith open('k7_pinn_step5_history.json', 'w') as f:\n    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f)\nprint('Saved: k7_pinn_step5_history.json')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.2: Export NumPy arrays and model checkpoints\n",
    "\n",
    "np.save('k7_pinn_step5_metric.npy', g_all)\n",
    "np.save('k7_pinn_step5_phi.npy', phi_all)\n",
    "np.save('k7_pinn_step5_coords.npy', x_eval.cpu().numpy())\n",
    "print(f'Saved: k7_pinn_step5_metric.npy ({g_all.shape})')\n",
    "print(f'Saved: k7_pinn_step5_phi.npy ({phi_all.shape})')\n",
    "print(f'Saved: k7_pinn_step5_coords.npy')\n",
    "\n",
    "# Multi-scale metric\n",
    "g_multiscale = []\n",
    "for T in T_SCALES:\n",
    "    with torch.no_grad():\n",
    "        g_T = model.metric(x_eval[:10000], np.log(T)).cpu().numpy()\n",
    "        g_multiscale.append(g_T)\n",
    "g_multiscale = np.stack(g_multiscale, axis=0)\n",
    "np.save('k7_pinn_step5_metric_multiscale.npy', g_multiscale)\n",
    "print(f'Saved: k7_pinn_step5_metric_multiscale.npy ({g_multiscale.shape})')\n",
    "\n",
    "# Copy to Drive if available\n",
    "if DRIVE_DIR:\n",
    "    import shutil\n",
    "    for f in ['k7_pinn_step5_results.json', 'k7_pinn_step5_history.json',\n",
    "              'k7_pinn_step5_metric.npy', 'k7_pinn_step5_phi.npy']:\n",
    "        shutil.copy(f, os.path.join(DRIVE_DIR, f))\n",
    "    print(f'Copied to Google Drive: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.3: Final model save\n",
    "\n",
    "torch.save(model.state_dict(), 'k7_pinn_step5_final.pt')\n",
    "print(f'Saved: k7_pinn_step5_final.pt ({n_params:,} parameters)')\n",
    "\n",
    "if DRIVE_DIR:\n",
    "    import shutil\n",
    "    shutil.copy('k7_pinn_step5_final.pt', os.path.join(DRIVE_DIR, 'k7_pinn_step5_final.pt'))\n",
    "    for fig_name in ['training_curves.png', 'spectral_convergence.png', 'metric_field.png',\n",
    "                     'det_stability.png', 'torsion_map.png', 'period_verification.png']:\n",
    "        if os.path.exists(fig_name):\n",
    "            shutil.copy(fig_name, os.path.join(DRIVE_DIR, fig_name))\n",
    "    print('All figures and model copied to Google Drive.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary & Conclusions <a id='sec12'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12.1: Final summary (v2)\n\nprint('=' * 70)\nprint('  STEP 5 RESULTS (v2): PINN RECONSTRUCTION OF K7 METRIC')\nprint('=' * 70)\n\nV = results['validation']\nM = results['metric']\nA = results['anisotropy']\nT_r = results['torsion']\n\nprint(f'''\n  MODEL\n    Architecture: G2MetricPINN ({n_params:,} params)\n    Training: {PHASE_3_END} epochs, {total_time/60:.1f} min on {gpu_name if GPU_AVAILABLE else \"CPU\"}\n    Loss: period (x1000, 5 scales) + anisotropy (x500, Step 4 target) + Tier 1\n    NO spectral loss (lambda1 = 14/99 is global topological, not local)\n\n  METRIC\n    det(g): {M[\"det_mean\"]:.6f} +/- {M[\"det_std\"]:.2e}  (target: {DET_G})\n    Deviation: {M[\"det_deviation_pct\"]:.4f}%\n    Positive definite: {M[\"positive_definite\"]}\n    Condition number: {M[\"condition_number_mean\"]:.6f}\n    det PASSED: {\"YES\" if V[\"det_passed\"] else \"NO\"} (< 1%)\n\n  ANISOTROPY (Step 4 target)\n    Target diagonal:   [{\", \".join(f\"{v:.5f}\" for v in A[\"target_diagonal\"])}]\n    Achieved diagonal: [{\", \".join(f\"{v:.5f}\" for v in A[\"achieved_diagonal\"])}]\n    Off-diag target:  {A[\"target_offdiag_max\"]:.6f}\n    Off-diag achieved: {A[\"achieved_offdiag_max\"]:.6f}\n    Frobenius error:   {A[\"frobenius_error\"]:.6f}\n\n  PERIODS\n    Best RMS error: {V[\"period_rms_best\"]:.6f}\n    Period errors by scale:''')\n\nfor T_key, pr in sorted(period_results.items(), key=lambda x: x[0]):\n    print(f'      T={T_key:6}: RMS = {pr[\"rms_error\"]:.6f}')\n\nprint(f'''\n  TORSION\n    Mean: {T_r[\"mean\"]:.2e}\n    Max:  {T_r[\"max\"]:.2e}  (Joyce bound: 0.1)\n    PASSED: {\"YES\" if V[\"torsion_passed\"] else \"NO\"}\n\n  SPECTRAL GAP\n    Global: lambda1 = 14/99 = {LAMBDA1:.6f} (topological, analytical)\n    Local Rayleigh: {lambda1_final:.4f} (TCS neck, diagnostic only)\n    Note: Local != Global is expected and correct.\n\n  OUTPUTS:\n    k7_pinn_step5_results.json    — all metrics (v2)\n    k7_pinn_step5_history.json    — training curves (period + aniso)\n    k7_pinn_step5_metric.npy      — 50K metric tensors (50000,7,7)\n    k7_pinn_step5_phi.npy         — 50K 3-forms (50000,35)\n    k7_pinn_step5_final.pt        — trained model\n    training_curves.png           — 4-panel diagnostics\n    convergence_v2.png            — period + anisotropy convergence\n    metric_field.png              — component distributions\n    det_stability.png             — determinant stability\n    torsion_map.png               — torsion visualization\n    period_verification.png       — period integrals\n''')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Notebook Produces\n",
    "\n",
    "The PINN learns a position-dependent G₂ structure $\\varphi(x)$ on the TCS neck of K₇,\n",
    "yielding the metric $g_{ij}(x) = \\frac{1}{6}\\sum_{k,l}\\varphi_{ikl}\\varphi_{jkl}$.\n",
    "\n",
    "**Data for the exhaustive document:**\n",
    "- Full 7×7 metric tensor at 50,000 points\n",
    "- Metric at 5 energy scales (T = 100, 1000, 10K, 40K, 75K)\n",
    "- Spectral gap convergence curve (Rayleigh quotient estimates)\n",
    "- 77 period integrals verified against prime-spectral targets\n",
    "- Torsion field mapped across the manifold\n",
    "- Determinant stability (spatial and scale-dependent)\n",
    "- Complete training diagnostics\n",
    "\n",
    "**Connection to Steps 1–4:**\n",
    "- The 77 moduli $\\Pi_k(T)$ from the mollified Dirichlet polynomial (Steps 1–2)\n",
    "  constrain the PINN's period integrals\n",
    "- The G₂ decomposition (Step 4) ensures only 14 local DOF (not 35)\n",
    "- The metric Jacobian $\\partial g/\\partial\\Pi_k$ from Step 4 guides the warm-start\n",
    "- The E₈/K3 lattice infrastructure (Step 4) determines the 42 global modes\n",
    "\n",
    "---\n",
    "*GIFT Framework — Step 5: PINN Reconstruction*"
   ]
  }
 ]
}