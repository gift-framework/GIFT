{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Full G₂ Holonomy Metric on K₇ via PINN\n",
    "## Comprehensive Reconstruction from Prime-Spectral Data\n",
    "\n",
    "**GIFT Framework** — Geometric Information Field Theory \n",
    "**Hardware**: NVIDIA A100-SXM4-80GB (Colab) \n",
    "**Date**: 2026-02\n",
    "\n",
    "---\n",
    "\n",
    "This notebook reconstructs the **explicit 7×7 metric tensor** $g_{ij}(x^1,\\ldots,x^7)$ on the compact G₂-holonomy manifold K₇, using a Physics-Informed Neural Network constrained by:\n",
    "\n",
    "1. **Determinant**: $\\det(g) = 65/32$ (topologically determined)\n",
    "2. **Torsion**: $\\|\\nabla\\varphi\\| \\to 0$ (G₂ holonomy)\n",
    "3. **Spectral gap**: $\\lambda_1 = 14/99$ (Hodge Laplacian)\n",
    "4. **Period integrals**: $\\int_{C_k} \\varphi = \\Pi_k$ for 77 3-cycles\n",
    "5. **Positive definiteness**: all eigenvalues of $g > 0$\n",
    "\n",
    "The 77 moduli $\\Pi_k(T)$ come from the prime-spectral formula (Steps 1–4).\n",
    "\n",
    "### Pipeline Summary\n",
    "| Step | Content | Status |\n",
    "|------|---------|--------|\n",
    "| 1–2 | Mollified Dirichlet polynomial, α=1, 100% counting | ✅ |\n",
    "| 3 | 77 periods → moduli space (35 local + 42 global) | ✅ |\n",
    "| 4 | G₂ decomposition, E₈/K3 lattice, metric Jacobian | ✅ |\n",
    "| **5** | **PINN reconstruction of g_ij(x)** | **This notebook** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Header](#)\n",
    "1. [Setup & Dependencies](#sec1)\n",
    "2. [GIFT Constants & G₂ Structure](#sec2)\n",
    "3. [Prime-Spectral Periods](#sec3)\n",
    "4. [TCS Neck Sampling](#sec4)\n",
    "5. [PINN Model Architecture](#sec5)\n",
    "6. [Loss Function (6 Terms, 3 Tiers)](#sec6)\n",
    "7. [Training Infrastructure](#sec7)\n",
    "8. [Training Execution (3 Phases)](#sec8)\n",
    "9. [Evaluation & Metric Extraction](#sec9)\n",
    "10. [Visualization (8 Figures)](#sec10)\n",
    "11. [Export (JSON + npy + pt)](#sec11)\n",
    "12. [Summary & Conclusions](#sec12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies <a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Install dependencies\n",
    "!pip install -q torch numpy scipy matplotlib tqdm\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(f'CuPy already installed: {cp.__version__}')\n",
    "except ImportError:\n",
    "    !pip install -q cupy-cuda12x\n",
    "    import cupy as cp\n",
    "    print(f'CuPy installed: {cp.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.2: Imports and GPU detection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json, os, time, gc, warnings\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU detection\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
    "    from cupyx.scipy.sparse.linalg import eigsh as cp_eigsh\n",
    "    GPU_AVAILABLE = True\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "    gpu_mem = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / 1e9\n",
    "    print(f'GPU: {gpu_name} ({gpu_mem:.0f} GB)')\n",
    "except Exception as e:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(f'No GPU/CuPy: {e}. Using CPU fallback.')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DTYPE = torch.float64\n",
    "torch.set_default_dtype(DTYPE)\n",
    "print(f'PyTorch device: {DEVICE}')\n",
    "print(f'Default dtype: {DTYPE}')\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Free all GPU memory pools.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU_AVAILABLE:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1.3: Download data from repository\nimport urllib.request\n\nBASE_URL = 'https://raw.githubusercontent.com/gift-framework/GIFT/research/notebooks'\nRIEMANN_URL = f'{BASE_URL}/riemann'\nDATA_DIR = 'data'\nos.makedirs(DATA_DIR, exist_ok=True)\n\ndef download(url, path):\n    if not os.path.exists(path):\n        print(f'  Downloading {os.path.basename(path)}...')\n        try:\n            urllib.request.urlretrieve(url, path)\n        except Exception as e:\n            print(f'  Failed: {e}')\n            return False\n    return True\n\n# Download pre-computed results (Step 3-4)\njson_files = [\n    'moduli_reconstruction_results.json',\n    'harmonic_forms_results.json',\n    'heat_kernel_results.json',\n]\nstep_data = {}\nfor fname in json_files:\n    path = os.path.join(DATA_DIR, fname)\n    if download(f'{RIEMANN_URL}/{fname}', path):\n        try:\n            with open(path) as f:\n                step_data[fname.replace('_results.json', '')] = json.load(f)\n            print(f'  Loaded {fname}')\n        except Exception as e:\n            print(f'  Parse error: {e}')\n\n# Riemann zeros: LFS-tracked, not downloadable via raw.githubusercontent.com\n# Not needed for Step 5 (periods are computed analytically from prime formula)\nGAMMA = None\nzeros_path = os.path.join(DATA_DIR, 'riemann_zeros_100k_genuine.npy')\nif os.path.exists(zeros_path):\n    try:\n        GAMMA = np.load(zeros_path)\n        print(f'  Loaded {len(GAMMA):,} Riemann zeros (local cache)')\n    except:\n        print('  Could not load cached zeros (not needed for Step 5)')\nelse:\n    print('  Riemann zeros: skipped (LFS file, not needed for Step 5 training)')\n\nprint(f'\\nData loaded: {len(step_data)} JSON files')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. GIFT Constants & G₂ Structure <a id='sec2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1: Topological constants\n",
    "DIM = 7\n",
    "DIM_G2 = 14\n",
    "B2, B3 = 21, 77\n",
    "H_STAR = 99\n",
    "KAPPA_T = 1.0 / 61\n",
    "DET_G = 65.0 / 32.0\n",
    "LAMBDA1 = 14.0 / 99.0\n",
    "N_GEN = 3\n",
    "\n",
    "# TCS building blocks\n",
    "B2_M1, B3_M1 = 11, 40   # quintic in CP4\n",
    "B2_M2, B3_M2 = 10, 37   # CI(2,2,2) in CP6\n",
    "TCS_R1, TCS_R2 = 33, 28  # TCS ratio: H*/(6*dim_G2) ≈ 33/28\n",
    "\n",
    "# Fano plane triples (octonion multiplication)\n",
    "FANO_TRIPLES = [(0,1,2), (0,3,4), (0,5,6), (1,3,5), (1,4,6), (2,3,6), (2,4,5)]\n",
    "FANO_SIGNS   = [+1,      +1,      +1,      +1,      -1,      -1,      -1     ]\n",
    "\n",
    "# All C(7,3) = 35 triples\n",
    "ALL_TRIPLES = list(combinations(range(DIM), 3))\n",
    "TRIPLE_INDEX = {t: i for i, t in enumerate(ALL_TRIPLES)}\n",
    "\n",
    "print(f'K7 topology: dim={DIM}, b2={B2}, b3={B3}, H*={H_STAR}')\n",
    "print(f'Target: det(g)={DET_G}, lambda1={LAMBDA1:.6f}, kappa_T={KAPPA_T:.6f}')\n",
    "print(f'TCS: M1(b2={B2_M1},b3={B3_M1}) + M2(b2={B2_M2},b3={B3_M2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.2: Build standard associative 3-form phi0\n",
    "\n",
    "def build_phi0_components():\n",
    "    \"\"\"35-component vector of the standard associative 3-form.\"\"\"\n",
    "    phi = np.zeros(35, dtype=np.float64)\n",
    "    for (i,j,k), s in zip(FANO_TRIPLES, FANO_SIGNS):\n",
    "        phi[TRIPLE_INDEX[(i,j,k)]] = s\n",
    "    return phi\n",
    "\n",
    "def build_phi0_tensor():\n",
    "    \"\"\"Full 7x7x7 antisymmetric tensor.\"\"\"\n",
    "    phi = build_phi0_components()\n",
    "    T = np.zeros((DIM, DIM, DIM), dtype=np.float64)\n",
    "    for idx, (i,j,k) in enumerate(ALL_TRIPLES):\n",
    "        val = phi[idx]\n",
    "        T[i,j,k] = val;  T[j,k,i] = val;  T[k,i,j] = val\n",
    "        T[i,k,j] = -val; T[k,j,i] = -val; T[j,i,k] = -val\n",
    "    return T\n",
    "\n",
    "PHI0_COMPS = build_phi0_components()\n",
    "PHI0_TENSOR = build_phi0_tensor()\n",
    "\n",
    "# Scale factor for det(g) = 65/32\n",
    "C_REF = DET_G ** (1.0 / 14.0)  # phi_ref = C_REF * phi0\n",
    "\n",
    "# Verify: metric from phi0 = I_7\n",
    "g0 = np.einsum('ikl,jkl->ij', PHI0_TENSOR, PHI0_TENSOR) / 6.0\n",
    "assert np.allclose(g0, np.eye(DIM)), f'g0 from phi0 is not I_7!\\n{g0}'\n",
    "print(f'phi0: {np.count_nonzero(PHI0_COMPS)} nonzero / 35, ||phi0|| = {np.linalg.norm(PHI0_COMPS):.6f}')\n",
    "print(f'c_ref = (65/32)^(1/14) = {C_REF:.6f}')\n",
    "print(f'g_ref = {C_REF**2:.6f} * I_7, det(g_ref) = {C_REF**14:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.3: G2 generators and Lie derivatives\n",
    "\n",
    "def g2_generators():\n",
    "    \"\"\"14 generators of G2 in so(7).\"\"\"\n",
    "    gens = np.zeros((14, DIM, DIM), dtype=np.float64)\n",
    "    # First 7: rotations in Fano planes\n",
    "    for idx, (i,j,k) in enumerate(FANO_TRIPLES):\n",
    "        gens[idx, i, j] = 1; gens[idx, j, i] = -1\n",
    "    # Remaining 7: mixed rotations\n",
    "    for idx in range(7):\n",
    "        i, j, k = idx, (idx+1) % 7, (idx+3) % 7\n",
    "        gens[7+idx, i, k] = 1;   gens[7+idx, k, i] = -1\n",
    "        gens[7+idx, j, k] = 0.5; gens[7+idx, k, j] = -0.5\n",
    "    # Normalize\n",
    "    for idx in range(14):\n",
    "        norm = np.linalg.norm(gens[idx])\n",
    "        if norm > 1e-10:\n",
    "            gens[idx] /= norm\n",
    "    return gens\n",
    "\n",
    "def precompute_lie_derivatives():\n",
    "    \"\"\"Lie derivatives of phi0 along G2 generators: 14 x 35 matrix.\"\"\"\n",
    "    gens = g2_generators()\n",
    "    L = np.zeros((14, 35), dtype=np.float64)\n",
    "    for a in range(14):\n",
    "        X = gens[a]\n",
    "        idx = 0\n",
    "        for i in range(DIM):\n",
    "            for j in range(i+1, DIM):\n",
    "                for k in range(j+1, DIM):\n",
    "                    val = sum(X[i,l]*PHI0_TENSOR[l,j,k] + X[j,l]*PHI0_TENSOR[i,l,k]\n",
    "                              + X[k,l]*PHI0_TENSOR[i,j,l] for l in range(DIM))\n",
    "                    L[a, idx] = val\n",
    "                    idx += 1\n",
    "    return L\n",
    "\n",
    "G2_GENS = g2_generators()\n",
    "LIE_DERIVS = precompute_lie_derivatives()\n",
    "print(f'G2 generators: {G2_GENS.shape} (14 elements of so(7))')\n",
    "print(f'Lie derivatives: {LIE_DERIVS.shape} (14 x 35 matrix)')\n",
    "print(f'Rank of Lie derivative matrix: {np.linalg.matrix_rank(LIE_DERIVS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.4: G2 decomposition verification\n",
    "\n",
    "# Verify Lambda^3 = 1 + 7 + 27\n",
    "phi0_norm = np.linalg.norm(PHI0_COMPS)\n",
    "e1 = PHI0_COMPS / phi0_norm  # unit vector along Lambda^3_1\n",
    "\n",
    "# Lambda^3_7 from coassociative 4-form contraction\n",
    "# (already verified in Step 4)\n",
    "\n",
    "# Projection of Fano-aligned forms\n",
    "fano_idx = [TRIPLE_INDEX[t] for t in FANO_TRIPLES]\n",
    "non_fano_idx = [i for i in range(35) if i not in fano_idx]\n",
    "\n",
    "proj_1 = np.array([e1[k]**2 for k in range(35)])\n",
    "print(f'G2 decomposition: Lambda^3 = 1 + 7 + 27 = 35')\n",
    "print(f'  Fano projection onto Lambda^3_1: {proj_1[fano_idx[0]]:.4f} each (=1/7)')\n",
    "print(f'  Non-Fano projection: {proj_1[non_fano_idx[0]]:.4f} (=0)')\n",
    "print(f'\\nModuli: 77 = 1 (volume) + 0 (gauge, b1=0) + 76 (shape)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prime-Spectral Periods <a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Prime sieve and period map\n",
    "\n",
    "def sieve(N):\n",
    "    is_p = np.ones(N+1, dtype=bool); is_p[:2] = False\n",
    "    for i in range(2, int(N**0.5)+1):\n",
    "        if is_p[i]: is_p[i*i::i] = False\n",
    "    return list(np.where(is_p)[0])\n",
    "\n",
    "PRIMES_77 = sieve(400)[:B3]  # first 77 primes: 2, 3, ..., 389\n",
    "\n",
    "THETA_0, THETA_1 = 1.4091, -3.9537  # Adaptive cutoff parameters\n",
    "\n",
    "def period_map(T, primes=PRIMES_77):\n",
    "    \"\"\"Compute 77 prime periods Pi_k(T).\"\"\"\n",
    "    log_X = THETA_0 * np.log(T) + THETA_1\n",
    "    if log_X < 0.5:\n",
    "        log_X = 0.5\n",
    "    Pi = np.zeros(len(primes))\n",
    "    for k, p in enumerate(primes):\n",
    "        logp = np.log(float(p))\n",
    "        x = logp / log_X\n",
    "        w = np.cos(np.pi * x / 2.0)**2 if x < 1 else 0.0\n",
    "        Pi[k] = KAPPA_T * w / np.sqrt(float(p))\n",
    "    return Pi\n",
    "\n",
    "print(f'77 primes: {PRIMES_77[0]}, {PRIMES_77[1]}, ..., {PRIMES_77[-1]}')\n",
    "print(f'Adaptive cutoff: theta(T) = {THETA_0} + {THETA_1}/log(T)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Compute periods at multiple scales\n",
    "\n",
    "T_SCALES = [100, 1000, 10000, 40000, 75000]\n",
    "T_REF = 40434.2\n",
    "\n",
    "PERIODS = {}\n",
    "for T in T_SCALES + [T_REF]:\n",
    "    Pi = period_map(T)\n",
    "    PERIODS[T] = Pi\n",
    "\n",
    "Pi_ref = PERIODS[T_REF]\n",
    "print(f'Reference scale T = {T_REF:.0f}:')\n",
    "print(f'  ||Pi||_2 = {np.linalg.norm(Pi_ref):.6f}')\n",
    "print(f'  Local (35):  ||Pi_L|| = {np.linalg.norm(Pi_ref[:35]):.6f}')\n",
    "print(f'  Global M1:   ||Pi_G1|| = {np.linalg.norm(Pi_ref[35:56]):.6f}')\n",
    "print(f'  Global M2:   ||Pi_G2|| = {np.linalg.norm(Pi_ref[56:]):.6f}')\n",
    "print(f'\\nPeriods at {len(T_SCALES)} scales computed.')\n",
    "\n",
    "# Convert to torch tensors\n",
    "PI_TARGETS = {T: torch.tensor(Pi, dtype=DTYPE, device=DEVICE) for T, Pi in PERIODS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.3: Period evolution table\n",
    "\n",
    "print(f'{\"T\":>8s} | {\"||Pi||\":>8s} | {\"||local||\":>10s} | {\"||global||\":>10s} | {\"#active\":>7s}')\n",
    "print(f'{\"-\"*8}-+-{\"-\"*8}-+-{\"-\"*10}-+-{\"-\"*10}-+-{\"-\"*7}')\n",
    "for T in sorted(PERIODS.keys()):\n",
    "    Pi = PERIODS[T]\n",
    "    n_active = np.sum(np.abs(Pi) > 1e-10)\n",
    "    print(f'{T:8.0f} | {np.linalg.norm(Pi):8.5f} | {np.linalg.norm(Pi[:35]):10.6f} | '\n",
    "          f'{np.linalg.norm(Pi[35:]):10.6f} | {n_active:7d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TCS Neck Sampling <a id='sec4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Sampling functions\n",
    "\n",
    "def sample_S3(n, device=DEVICE):\n",
    "    \"\"\"Uniform sampling on S3 via Gaussian normalization.\"\"\"\n",
    "    x = torch.randn(n, 4, device=device, dtype=DTYPE)\n",
    "    return x / x.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def sample_tcs_neck(n, r1=TCS_R1, r2=TCS_R2, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Sample from TCS neck S1 x S3 x S3.\n",
    "    Returns (n, 7) coordinates + full quaternions for distance computation.\n",
    "    \"\"\"\n",
    "    theta = 2 * np.pi * torch.rand(n, 1, device=device, dtype=DTYPE)\n",
    "    q1 = sample_S3(n, device)\n",
    "    q2 = sample_S3(n, device)\n",
    "    # Stereographic projection to get 3D from each S3\n",
    "    q1_3d = q1[:, 1:4] / (1 + q1[:, 0:1].abs() + 1e-8)\n",
    "    q2_3d = q2[:, 1:4] / (1 + q2[:, 0:1].abs() + 1e-8)\n",
    "    coords = torch.cat([theta, q1_3d, q2_3d], dim=-1)  # (n, 7)\n",
    "    return coords, q1, q2\n",
    "\n",
    "def geodesic_dist_S3(q1, q2):\n",
    "    \"\"\"Geodesic distance on S3 (quaternionic).\"\"\"\n",
    "    dot = torch.abs(torch.sum(q1 * q2, dim=-1))\n",
    "    dot = torch.clamp(dot, 0, 1)\n",
    "    return torch.arccos(dot)\n",
    "\n",
    "# Test sampling\n",
    "x_test, q1_test, q2_test = sample_tcs_neck(1000)\n",
    "print(f'TCS neck sampling: {x_test.shape}')\n",
    "print(f'  Coordinate ranges:')\n",
    "for i, name in enumerate(['theta', 'q1_x', 'q1_y', 'q1_z', 'q2_x', 'q2_y', 'q2_z']):\n",
    "    print(f'    {name}: [{x_test[:,i].min():.3f}, {x_test[:,i].max():.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Verify sampling distribution\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].hist(x_test[:,0].cpu().numpy(), bins=30, alpha=0.7)\n",
    "axes[0].set_title('theta (S1)')\n",
    "axes[1].hist(torch.norm(x_test[:,1:4], dim=-1).cpu().numpy(), bins=30, alpha=0.7)\n",
    "axes[1].set_title('||q1_3d|| (stereographic)')\n",
    "axes[2].hist(torch.norm(x_test[:,4:7], dim=-1).cpu().numpy(), bins=30, alpha=0.7)\n",
    "axes[2].set_title('||q2_3d|| (stereographic)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sampling_check.png', dpi=100)\n",
    "plt.show()\n",
    "print('Sampling OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PINN Model Architecture <a id='sec5'></a>\n",
    "\n",
    "Architecture: `G2MetricPINN` (~120K parameters)\n",
    "\n",
    "```\n",
    "Input: (x1,...,x7, log T) in R8\n",
    "  |-> FourierFeatures(48 freq) -> R96\n",
    "  |-> MLP: 96 -> 256 -> 256 -> 256 -> 128\n",
    "  |-> Local head: 128 -> 14 (G2 adjoint) -> Lie derivs -> 35\n",
    "  |-> Global head: 128 -> 42 (TCS modes)\n",
    "  |-> phi = c*phi0 + 0.1 * [delta_local || delta_global]\n",
    "  |-> g_ij = (1/6) sum_{kl} phi_ikl * phi_jkl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: FourierFeatures\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Random Fourier feature encoding for periodic structure.\"\"\"\n",
    "    def __init__(self, input_dim=8, num_frequencies=48, scale=2.0):\n",
    "        super().__init__()\n",
    "        self.output_dim = 2 * num_frequencies\n",
    "        B = torch.randn(num_frequencies, input_dim, dtype=DTYPE) * scale\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = 2 * np.pi * torch.matmul(x, self.B.T)\n",
    "        return torch.cat([torch.cos(projected), torch.sin(projected)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: G2MetricPINN model\n",
    "\n",
    "class G2MetricPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for G2 holonomy metric on K7.\n",
    "    \n",
    "    Key features:\n",
    "    - Scale input (log T as 8th dimension)\n",
    "    - G2-adjoint parameterization (14 DOF -> 35 via Lie derivatives)\n",
    "    - Separate global head for 42 TCS modes\n",
    "    - Perturbation scale 0.1 for non-trivial anisotropy\n",
    "    \"\"\"\n",
    "    def __init__(self, n_freq=48, hidden=[256, 256, 256, 128],\n",
    "                 perturbation_scale=0.1, include_scale=True):\n",
    "        super().__init__()\n",
    "        self.perturbation_scale = perturbation_scale\n",
    "        self.include_scale = include_scale\n",
    "        input_dim = 8 if include_scale else 7\n",
    "        \n",
    "        # Buffers\n",
    "        self.register_buffer('phi0_comps', torch.tensor(C_REF * PHI0_COMPS, dtype=DTYPE))\n",
    "        self.register_buffer('phi0_tensor', torch.tensor(C_REF * PHI0_TENSOR, dtype=DTYPE))\n",
    "        self.register_buffer('lie_derivs', torch.tensor(LIE_DERIVS, dtype=DTYPE))\n",
    "        \n",
    "        # Fourier features\n",
    "        self.fourier = FourierFeatures(input_dim=input_dim, num_frequencies=n_freq)\n",
    "        \n",
    "        # Shared backbone\n",
    "        layers = []\n",
    "        in_dim = self.fourier.output_dim\n",
    "        for h_dim in hidden:\n",
    "            layers.extend([nn.Linear(in_dim, h_dim), nn.SiLU()])\n",
    "            in_dim = h_dim\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        \n",
    "        # Local head: backbone -> 14 G2 adjoint params -> Lie derivs -> 35\n",
    "        self.local_head = nn.Linear(hidden[-1], 14)\n",
    "        \n",
    "        # Global head: backbone -> 42 TCS product modes\n",
    "        self.global_head = nn.Linear(hidden[-1], 42)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def _make_input(self, x, log_T=None):\n",
    "        \"\"\"Append log(T) as 8th dimension if needed.\"\"\"\n",
    "        if self.include_scale and log_T is not None:\n",
    "            T_col = torch.full((x.shape[0], 1), log_T, device=x.device, dtype=DTYPE)\n",
    "            return torch.cat([x, T_col], dim=-1)\n",
    "        elif self.include_scale:\n",
    "            T_col = torch.full((x.shape[0], 1), np.log(T_REF), device=x.device, dtype=DTYPE)\n",
    "            return torch.cat([x, T_col], dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, log_T=None):\n",
    "        \"\"\"Return 77 moduli components.\"\"\"\n",
    "        inp = self._make_input(x, log_T)\n",
    "        h = self.backbone(self.fourier(inp))\n",
    "        \n",
    "        # Local: G2 adjoint -> Lie derivatives -> 35 components\n",
    "        adjoint = self.local_head(h)  # (N, 14)\n",
    "        delta_local = torch.matmul(adjoint, self.lie_derivs)  # (N, 35)\n",
    "        \n",
    "        # Global: direct 42 components\n",
    "        delta_global = self.global_head(h)  # (N, 42)\n",
    "        \n",
    "        # Full 3-form: phi_ref + perturbation\n",
    "        local_comps = self.phi0_comps.unsqueeze(0) + self.perturbation_scale * delta_local\n",
    "        global_comps = self.perturbation_scale * delta_global\n",
    "        \n",
    "        return local_comps, global_comps, adjoint\n",
    "    \n",
    "    def phi_tensor(self, x, log_T=None):\n",
    "        \"\"\"Full (N,7,7,7) antisymmetric tensor from local 35 components.\"\"\"\n",
    "        local_comps, _, _ = self.forward(x, log_T)\n",
    "        N = local_comps.shape[0]\n",
    "        phi = torch.zeros(N, DIM, DIM, DIM, device=x.device, dtype=DTYPE)\n",
    "        idx = 0\n",
    "        for i in range(DIM):\n",
    "            for j in range(i+1, DIM):\n",
    "                for k in range(j+1, DIM):\n",
    "                    val = local_comps[:, idx]\n",
    "                    phi[:,i,j,k] = val; phi[:,j,k,i] = val; phi[:,k,i,j] = val\n",
    "                    phi[:,j,i,k] = -val; phi[:,i,k,j] = -val; phi[:,k,j,i] = -val\n",
    "                    idx += 1\n",
    "        return phi\n",
    "    \n",
    "    def metric(self, x, log_T=None):\n",
    "        \"\"\"Compute metric g_ij = (1/6) sum_{kl} phi_ikl phi_jkl.\"\"\"\n",
    "        phi = self.phi_tensor(x, log_T)\n",
    "        return torch.einsum('nikl,njkl->nij', phi, phi) / 6.0\n",
    "    \n",
    "    def det_g(self, x, log_T=None):\n",
    "        return torch.linalg.det(self.metric(x, log_T))\n",
    "    \n",
    "    def moduli_77(self, x, log_T=None):\n",
    "        \"\"\"Return all 77 moduli components as a single vector.\"\"\"\n",
    "        local, glob, _ = self.forward(x, log_T)\n",
    "        # Extract perturbation: (phi - phi_ref) for local\n",
    "        delta_local = local - self.phi0_comps.unsqueeze(0)\n",
    "        return torch.cat([delta_local, glob], dim=-1)  # (N, 77)\n",
    "\n",
    "# Instantiate and report\n",
    "model = G2MetricPINN().to(DEVICE)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'G2MetricPINN: {n_params:,} parameters')\n",
    "print(f'  Backbone: FourierFeatures(48) -> MLP(256,256,256,128)')\n",
    "print(f'  Local head: 128 -> 14 (G2 adjoint) -> 35 (Lie derivatives)')\n",
    "print(f'  Global head: 128 -> 42 (TCS modes)')\n",
    "print(f'  Perturbation scale: {model.perturbation_scale}')\n",
    "\n",
    "# Quick test\n",
    "with torch.no_grad():\n",
    "    x_test = sample_tcs_neck(100)[0]\n",
    "    g_test = model.metric(x_test)\n",
    "    det_test = torch.linalg.det(g_test)\n",
    "    print(f'\\nQuick test (100 points):')\n",
    "    print(f'  det(g) mean: {det_test.mean():.6f} (target: {DET_G:.6f})')\n",
    "    print(f'  det(g) std:  {det_test.std():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Loss Function <a id='sec6'></a>\n",
    "\n",
    "Three tiers:\n",
    "- **Tier 1** (every batch): determinant, positive-definiteness, torsion, sparsity\n",
    "- **Tier 2** (every 10 steps): period integral matching\n",
    "- **Tier 3** (every 50 steps): spectral gap via Rayleigh quotient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Tier 1 — Pointwise losses\n",
    "\n",
    "def loss_determinant(model, x, log_T=None):\n",
    "    \"\"\"L_det = (det(g) - 65/32)^2.\"\"\"\n",
    "    det_g = model.det_g(x, log_T)\n",
    "    return torch.mean((det_g - DET_G) ** 2)\n",
    "\n",
    "def loss_positive_definite(model, x, log_T=None):\n",
    "    \"\"\"L_pd = ReLU(-eigenvalues)^2.\"\"\"\n",
    "    g = model.metric(x, log_T)\n",
    "    eigvals = torch.linalg.eigvalsh(g)\n",
    "    return torch.mean(F.relu(-eigvals) ** 2)\n",
    "\n",
    "def loss_torsion(model, x, log_T=None):\n",
    "    \"\"\"Torsion proxy: penalize spatial variation of phi.\"\"\"\n",
    "    eps = 0.01\n",
    "    local_0, _, _ = model.forward(x, log_T)\n",
    "    torsion = 0.0\n",
    "    for dim in range(DIM):\n",
    "        x_plus = x.clone()\n",
    "        x_plus[:, dim] += eps\n",
    "        local_plus, _, _ = model.forward(x_plus, log_T)\n",
    "        dphi = (local_plus - local_0) / eps\n",
    "        torsion += (dphi ** 2).mean()\n",
    "    return torsion / DIM\n",
    "\n",
    "def loss_sparse(model, x, log_T=None):\n",
    "    \"\"\"L_sparse = ||adjoint||^2 (regularization).\"\"\"\n",
    "    _, _, adjoint = model.forward(x, log_T)\n",
    "    return torch.mean(adjoint ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.2: Tier 2 — Period loss\n",
    "\n",
    "def loss_periods(model, x, T_val=T_REF, log_T=None):\n",
    "    \"\"\"L_period = sum_k (moduli_k - Pi_k)^2.\"\"\"\n",
    "    if log_T is None:\n",
    "        log_T = np.log(T_val)\n",
    "    \n",
    "    moduli = model.moduli_77(x, log_T)  # (N, 77)\n",
    "    Pi_target = PI_TARGETS[T_val]  # (77,)\n",
    "    \n",
    "    # Average moduli over the point cloud (spatial average)\n",
    "    moduli_mean = moduli.mean(dim=0)  # (77,)\n",
    "    \n",
    "    return torch.mean((moduli_mean - Pi_target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6.3: Tier 3 — Spectral gap via Rayleigh quotient\n\ndef compute_rayleigh_quotient(model, n_samples=2000, differentiable=False):\n    \"\"\"\n    Estimate lambda1 via Rayleigh quotient with Fourier test functions.\n    \n    The Rayleigh quotient R[f] = int |grad f|^2_g sqrt(det g) / int f^2 sqrt(det g)\n    gives an upper bound on lambda1. We minimize over a set of test functions.\n    \n    If differentiable=True: returns (loss_tensor, lambda1_float) where loss has\n    gradients through the metric (for training).\n    If differentiable=False: returns (lambda1_float, top5_list) (for evaluation).\n    \"\"\"\n    x, _, _ = sample_tcs_neck(n_samples)\n    \n    if differentiable:\n        # Metric WITH gradients — this is the key for training!\n        g = model.metric(x)\n        g_inv = torch.linalg.inv(g)\n        det_g = torch.linalg.det(g)\n        sqrt_det = torch.sqrt(torch.abs(det_g) + 1e-10)\n    else:\n        with torch.no_grad():\n            g = model.metric(x)\n            g_inv = torch.linalg.inv(g)\n            det_g = torch.linalg.det(g)\n            sqrt_det = torch.sqrt(torch.abs(det_g) + 1e-10)\n    \n    rayleigh_values = []\n    \n    # Single-frequency modes: cos(freq * x_dim)\n    for dim_idx in range(DIM):\n        for freq in [1, 2, 3]:\n            f = torch.cos(freq * x[:, dim_idx].detach())\n            f = f - f.mean()\n            if f.var() < 1e-10:\n                continue\n            \n            grad_f = torch.zeros(n_samples, DIM, device=x.device, dtype=DTYPE)\n            grad_f[:, dim_idx] = -freq * torch.sin(freq * x[:, dim_idx].detach())\n            \n            # R = <grad_f, g^{-1} grad_f> * sqrt(det g) / (f^2 * sqrt(det g))\n            grad_f_norm_sq = torch.einsum('ni,nij,nj->n', grad_f, g_inv, grad_f)\n            numerator = (grad_f_norm_sq * sqrt_det).mean()\n            denominator = (f**2 * sqrt_det).mean() + 1e-10\n            R = numerator / denominator\n            rayleigh_values.append(R)\n    \n    # Mixed modes: cos(x_i) * cos(x_j)\n    for i in range(DIM):\n        for j in range(i+1, min(i+3, DIM)):\n            f = torch.cos(x[:, i].detach()) * torch.cos(x[:, j].detach())\n            f = f - f.mean()\n            if f.var() < 1e-10:\n                continue\n            \n            grad_f = torch.zeros(n_samples, DIM, device=x.device, dtype=DTYPE)\n            grad_f[:, i] = -torch.sin(x[:, i].detach()) * torch.cos(x[:, j].detach())\n            grad_f[:, j] = -torch.cos(x[:, i].detach()) * torch.sin(x[:, j].detach())\n            \n            grad_f_norm_sq = torch.einsum('ni,nij,nj->n', grad_f, g_inv, grad_f)\n            numerator = (grad_f_norm_sq * sqrt_det).mean()\n            denominator = (f**2 * sqrt_det).mean() + 1e-10\n            R = numerator / denominator\n            rayleigh_values.append(R)\n    \n    if differentiable:\n        if rayleigh_values:\n            R_stack = torch.stack(rayleigh_values)\n            lambda1 = R_stack.min()\n        else:\n            lambda1 = torch.tensor(LAMBDA1, device=DEVICE)\n        loss = (lambda1 - LAMBDA1) ** 2\n        return loss, float(lambda1.detach())\n    else:\n        if rayleigh_values:\n            vals = sorted([float(r) for r in rayleigh_values])\n            return vals[0], vals[:5]\n        return 0.0, []\n\n\ndef loss_spectral(model, n_samples=2000):\n    \"\"\"Differentiable spectral loss for training backward pass.\"\"\"\n    return compute_rayleigh_quotient(model, n_samples, differentiable=True)\n\n\n# Quick test\nwith torch.no_grad():\n    lam1, top5 = compute_rayleigh_quotient(model, n_samples=500, differentiable=False)\n    print(f'Initial lambda1 estimate: {lam1:.6f}')\n    print(f'Target: {LAMBDA1:.6f} (= 14/99)')\n    print(f'lambda1 * H*: {lam1 * H_STAR:.2f} (target: 14.0)')\n    print(f'Initial spectral loss would be: {(lam1 - LAMBDA1)**2 * LOSS_WEIGHTS[\"spectral\"]:.2f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.4: Combined loss with weight scheduling\n",
    "\n",
    "LOSS_WEIGHTS = {\n",
    "    'det': 100.0,\n",
    "    'pd': 50.0,\n",
    "    'torsion': 1.0,\n",
    "    'sparse': 0.1,\n",
    "    'period': 10.0,\n",
    "    'spectral': 500.0,\n",
    "}\n",
    "\n",
    "SPECTRAL_INTERVAL = 50   # compute spectral loss every N steps\n",
    "PERIOD_INTERVAL = 10     # compute period loss every N steps\n",
    "\n",
    "print('Loss weights:')\n",
    "for k, v in LOSS_WEIGHTS.items():\n",
    "    print(f'  {k}: {v}')\n",
    "print(f'\\nSpectral evaluation: every {SPECTRAL_INTERVAL} steps')\n",
    "print(f'Period evaluation: every {PERIOD_INTERVAL} steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Infrastructure <a id='sec7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7.1: Checkpointing\n\nCKPT_DIR = 'checkpoints'\nos.makedirs(CKPT_DIR, exist_ok=True)\n\n# Try mounting Google Drive for persistent storage\nDRIVE_DIR = None\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_DIR = '/content/drive/MyDrive/GIFT_PINN'\n    os.makedirs(DRIVE_DIR, exist_ok=True)\n    print(f'Google Drive mounted: {DRIVE_DIR}')\nexcept:\n    print('No Google Drive (running locally). Checkpoints in ./checkpoints/')\n\ndef save_checkpoint(model, optimizer, epoch, history, label='latest'):\n    state = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'history': history,\n    }\n    path = os.path.join(CKPT_DIR, f'step5_{label}.pt')\n    torch.save(state, path)\n    if DRIVE_DIR:\n        torch.save(state, os.path.join(DRIVE_DIR, f'step5_{label}.pt'))\n    return path\n\ndef load_checkpoint(model, optimizer, label='latest'):\n    for d in [DRIVE_DIR, CKPT_DIR]:\n        if d is None: continue\n        path = os.path.join(d, f'step5_{label}.pt')\n        if os.path.exists(path):\n            state = torch.load(path, map_location=DEVICE, weights_only=False)\n            model.load_state_dict(state['model_state_dict'])\n            optimizer.load_state_dict(state['optimizer_state_dict'])\n            print(f'Loaded checkpoint from epoch {state[\"epoch\"]}')\n            return state['epoch'], state['history']\n    print('No checkpoint found, starting fresh.')\n    return 0, {'loss': [], 'det': [], 'torsion': [], 'spectral': [], 'lambda1': []}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Optimizer and scheduler\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# 3-phase schedule:\n",
    "# Phase 1 (warmup, epochs 0-500):   lr=1e-3, Tier 1 only\n",
    "# Phase 2 (spectral, epochs 500-3000): lr=5e-4 cosine, Tier 1+2+3\n",
    "# Phase 3 (fine-tune, epochs 3000-5000): lr=1e-5, all losses\n",
    "\n",
    "PHASE_1_END = 500\n",
    "PHASE_2_END = 3000\n",
    "PHASE_3_END = 5000\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "def get_lr(epoch):\n",
    "    if epoch < PHASE_1_END:\n",
    "        return 1e-3\n",
    "    elif epoch < PHASE_2_END:\n",
    "        progress = (epoch - PHASE_1_END) / (PHASE_2_END - PHASE_1_END)\n",
    "        return 5e-4 * (1 + np.cos(np.pi * progress)) / 2 + 1e-5\n",
    "    else:\n",
    "        return 1e-5\n",
    "\n",
    "def get_phase(epoch):\n",
    "    if epoch < PHASE_1_END: return 1\n",
    "    if epoch < PHASE_2_END: return 2\n",
    "    return 3\n",
    "\n",
    "print(f'Training schedule:')\n",
    "print(f'  Phase 1 (warmup):   epochs 0-{PHASE_1_END}, lr=1e-3, Tier 1 only')\n",
    "print(f'  Phase 2 (spectral): epochs {PHASE_1_END}-{PHASE_2_END}, lr=5e-4 cosine, Tier 1+2+3')\n",
    "print(f'  Phase 3 (fine):     epochs {PHASE_2_END}-{PHASE_3_END}, lr=1e-5, all')\n",
    "print(f'  Batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7.3: Training step function\n\ndef train_step(model, optimizer, epoch, step, history):\n    \"\"\"Single training step with differentiable spectral loss.\"\"\"\n    phase = get_phase(epoch)\n    lr = get_lr(epoch)\n    for pg in optimizer.param_groups:\n        pg['lr'] = lr\n    \n    # Sample batch\n    x, _, _ = sample_tcs_neck(BATCH_SIZE)\n    \n    optimizer.zero_grad()\n    \n    # Tier 1: always (pointwise)\n    L_det = loss_determinant(model, x)\n    L_pd = loss_positive_definite(model, x)\n    L_torsion = loss_torsion(model, x)\n    L_sparse = loss_sparse(model, x)\n    \n    total = (LOSS_WEIGHTS['det'] * L_det + LOSS_WEIGHTS['pd'] * L_pd +\n             LOSS_WEIGHTS['torsion'] * L_torsion + LOSS_WEIGHTS['sparse'] * L_sparse)\n    \n    # Tier 2: period loss — included from Phase 1 for initial gradient signal\n    L_period = loss_periods(model, x)\n    total = total + LOSS_WEIGHTS['period'] * L_period\n    \n    # Tier 3: spectral loss — DIFFERENTIABLE, enters backward pass\n    lambda1_val = None\n    interval = SPECTRAL_INTERVAL if phase <= 2 else SPECTRAL_INTERVAL // 2\n    if phase >= 2 and step % interval == 0:\n        L_spec, lambda1_val = loss_spectral(model, n_samples=2000)\n        total = total + LOSS_WEIGHTS['spectral'] * L_spec\n    \n    total.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    \n    # Record\n    with torch.no_grad():\n        history['loss'].append(float(total))\n        history['det'].append(float(L_det))\n        history['torsion'].append(float(L_torsion))\n        if lambda1_val is not None:\n            history['lambda1'].append(lambda1_val)\n            history['spectral'].append((lambda1_val - LAMBDA1)**2)\n    \n    return float(total), lambda1_val"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Training Execution <a id='sec8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.1: Resume from checkpoint if available\n",
    "\n",
    "start_epoch, history = load_checkpoint(model, optimizer)\n",
    "best_loss = float('inf')\n",
    "t0_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.2: Phase 1 — Warmup (Tier 1 + period loss for initial gradient signal)\n\nprint('=' * 70)\nprint('PHASE 1: WARMUP (Tier 1 + period — establish metric + initial moduli)')\nprint('=' * 70)\n\nstep = 0\nfor epoch in tqdm(range(max(start_epoch, 0), PHASE_1_END), desc='Phase 1'):\n    loss_val, _ = train_step(model, optimizer, epoch, step, history)\n    step += 1\n    \n    if (epoch + 1) % 100 == 0:\n        with torch.no_grad():\n            x_eval = sample_tcs_neck(1000)[0]\n            det_eval = model.det_g(x_eval)\n            moduli_eval = model.moduli_77(x_eval)\n            moduli_norm = float(moduli_eval.mean(dim=0).norm())\n        det_dev = 100 * abs(float(det_eval.mean()) - DET_G) / DET_G\n        print(f'  Epoch {epoch+1:4d} | loss={loss_val:.2e} | '\n              f'det_dev={det_dev:.3f}% | ||moduli||={moduli_norm:.4e} | '\n              f'lr={get_lr(epoch):.1e}')\n\nsave_checkpoint(model, optimizer, PHASE_1_END, history, 'phase1')\nprint(f'Phase 1 complete. Time: {time.time()-t0_train:.0f}s')\nclear_gpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.3: Phase 2 — Spectral (add differentiable spectral gap loss)\n\nprint('\\n' + '=' * 70)\nprint('PHASE 2: SPECTRAL (Tier 1+2+3 — differentiable Rayleigh quotient)')\nprint('=' * 70)\n\nt1 = time.time()\nfor epoch in tqdm(range(max(start_epoch, PHASE_1_END), PHASE_2_END), desc='Phase 2'):\n    loss_val, lambda1_val = train_step(model, optimizer, epoch, step, history)\n    step += 1\n    \n    if loss_val < best_loss:\n        best_loss = loss_val\n        save_checkpoint(model, optimizer, epoch, history, 'best')\n    \n    if (epoch + 1) % 100 == 0:\n        with torch.no_grad():\n            x_eval = sample_tcs_neck(1000)[0]\n            det_eval = model.det_g(x_eval)\n        det_dev = 100 * abs(float(det_eval.mean()) - DET_G) / DET_G\n        if lambda1_val is not None:\n            lam_str = f'lam1*H*={lambda1_val*H_STAR:.2f}'\n        else:\n            lam_str = f'lam1*H*=...'\n        print(f'  Epoch {epoch+1:4d} | loss={loss_val:.2e} | {lam_str} | '\n              f'det_dev={det_dev:.3f}% | lr={get_lr(epoch):.1e}')\n    \n    if (epoch + 1) % 500 == 0:\n        save_checkpoint(model, optimizer, epoch, history, f'epoch{epoch+1}')\n        clear_gpu()\n\nsave_checkpoint(model, optimizer, PHASE_2_END, history, 'phase2')\nprint(f'Phase 2 complete. Time: {time.time()-t1:.0f}s')\nclear_gpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8.4: Phase 3 — Fine-tuning (more frequent spectral evaluation)\n\nprint('\\n' + '=' * 70)\nprint('PHASE 3: FINE-TUNING (all losses, spectral every 25 steps)')\nprint('=' * 70)\n\nt2 = time.time()\nfor epoch in tqdm(range(max(start_epoch, PHASE_2_END), PHASE_3_END), desc='Phase 3'):\n    loss_val, lambda1_val = train_step(model, optimizer, epoch, step, history)\n    step += 1\n    \n    if loss_val < best_loss:\n        best_loss = loss_val\n        save_checkpoint(model, optimizer, epoch, history, 'best')\n    \n    if (epoch + 1) % 100 == 0:\n        with torch.no_grad():\n            x_eval = sample_tcs_neck(1000)[0]\n            det_eval = model.det_g(x_eval)\n        det_dev = 100 * abs(float(det_eval.mean()) - DET_G) / DET_G\n        if lambda1_val is not None:\n            lam_str = f'lam1*H*={lambda1_val*H_STAR:.2f}'\n        else:\n            lam_str = f'lam1*H*=...'\n        print(f'  Epoch {epoch+1:4d} | loss={loss_val:.2e} | {lam_str} | '\n              f'det_dev={det_dev:.3f}% | lr={get_lr(epoch):.1e}')\n\nsave_checkpoint(model, optimizer, PHASE_3_END, history, 'final')\ntotal_time = time.time() - t0_train\nprint(f'\\nTraining complete! Total time: {total_time/60:.1f} minutes')\nclear_gpu()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Evaluation & Metric Extraction <a id='sec9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.1: Load best model and generate high-res evaluation\n",
    "\n",
    "# Load best model\n",
    "load_checkpoint(model, optimizer, 'best')\n",
    "model.eval()\n",
    "\n",
    "N_EVAL = 50000\n",
    "print(f'Evaluating on {N_EVAL:,} points...')\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_eval, q1_eval, q2_eval = sample_tcs_neck(N_EVAL)\n",
    "    \n",
    "    # Evaluate in batches to avoid memory issues\n",
    "    batch = 5000\n",
    "    g_all = []\n",
    "    phi_all = []\n",
    "    for i in range(0, N_EVAL, batch):\n",
    "        x_batch = x_eval[i:i+batch]\n",
    "        g_batch = model.metric(x_batch)\n",
    "        local, glob, _ = model.forward(x_batch)\n",
    "        g_all.append(g_batch.cpu().numpy())\n",
    "        phi_all.append(local.cpu().numpy())\n",
    "        if (i // batch) % 5 == 0:\n",
    "            print(f'  Batch {i//batch + 1}/{(N_EVAL+batch-1)//batch}')\n",
    "    \n",
    "    g_all = np.concatenate(g_all, axis=0)\n",
    "    phi_all = np.concatenate(phi_all, axis=0)\n",
    "\n",
    "print(f'\\nMetric tensor shape: {g_all.shape}')\n",
    "print(f'3-form shape: {phi_all.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.2: Metric statistics\n",
    "\n",
    "det_all = np.linalg.det(g_all)\n",
    "eigvals_all = np.linalg.eigvalsh(g_all)\n",
    "\n",
    "print('METRIC STATISTICS')\n",
    "print('=' * 50)\n",
    "print(f'  det(g): mean={np.mean(det_all):.6f}, std={np.std(det_all):.6f}')\n",
    "print(f'          target={DET_G:.6f}, deviation={100*abs(np.mean(det_all)-DET_G)/DET_G:.3f}%')\n",
    "print(f'  Eigenvalues:')\n",
    "print(f'    min:  {eigvals_all.min():.6f}')\n",
    "print(f'    max:  {eigvals_all.max():.6f}')\n",
    "print(f'    mean: {eigvals_all.mean():.6f}')\n",
    "print(f'  Positive definite: {np.all(eigvals_all > 0)}')\n",
    "print(f'  Mean condition number: {(eigvals_all.max(axis=1)/eigvals_all.min(axis=1)).mean():.4f}')\n",
    "print(f'  Off-diagonal max: {np.max(np.abs(g_all - np.einsum(\"nii->ni\", g_all)[:,:,None] * np.eye(DIM)[None,:,:])):.6f}')\n",
    "\n",
    "# Mean metric tensor\n",
    "g_mean = np.mean(g_all, axis=0)\n",
    "print(f'\\nMean metric tensor g_mean:')\n",
    "for i in range(DIM):\n",
    "    row = ' '.join(f'{g_mean[i,j]:+.5f}' for j in range(DIM))\n",
    "    print(f'  [{row}]')\n",
    "print(f'  det(g_mean) = {np.linalg.det(g_mean):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.3: Spectral gap (high-resolution)\n",
    "\n",
    "print('\\nSPECTRAL GAP ANALYSIS')\n",
    "print('=' * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lambda1_final, top5_final = compute_rayleigh_quotient(model, n_samples=5000)\n",
    "\n",
    "print(f'  lambda1 = {lambda1_final:.6f}')\n",
    "print(f'  Target:   {LAMBDA1:.6f} (= 14/99)')\n",
    "print(f'  lambda1 * H* = {lambda1_final * H_STAR:.4f} (target: 14.0)')\n",
    "print(f'  Deviation: {100*abs(lambda1_final - LAMBDA1)/LAMBDA1:.2f}%')\n",
    "print(f'\\n  Top 5 Rayleigh quotients: {[f\"{v:.4f}\" for v in top5_final]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.4: Period integral verification\n",
    "\n",
    "print('\\nPERIOD INTEGRAL VERIFICATION')\n",
    "print('=' * 50)\n",
    "\n",
    "period_results = {}\n",
    "for T in T_SCALES:\n",
    "    with torch.no_grad():\n",
    "        x_per = sample_tcs_neck(10000)[0]\n",
    "        moduli = model.moduli_77(x_per, np.log(T))\n",
    "        moduli_mean = moduli.mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    Pi_target = PERIODS[T]\n",
    "    rms_error = np.sqrt(np.mean((moduli_mean - Pi_target)**2))\n",
    "    period_results[T] = {'achieved': moduli_mean.tolist(), 'target': Pi_target.tolist(),\n",
    "                         'rms_error': float(rms_error)}\n",
    "    print(f'  T={T:6d}: RMS period error = {rms_error:.6f}, '\n",
    "          f'||Pi_target||={np.linalg.norm(Pi_target):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.5: Multi-scale metric evaluation\n",
    "\n",
    "print('\\nMULTI-SCALE METRIC EVOLUTION')\n",
    "print('=' * 50)\n",
    "\n",
    "scale_metrics = []\n",
    "for T in T_SCALES:\n",
    "    with torch.no_grad():\n",
    "        x_s = sample_tcs_neck(10000)[0]\n",
    "        g_s = model.metric(x_s, np.log(T))\n",
    "        det_s = torch.linalg.det(g_s).cpu().numpy()\n",
    "        eig_s = torch.linalg.eigvalsh(g_s).cpu().numpy()\n",
    "    \n",
    "    scale_metrics.append({\n",
    "        'T': T,\n",
    "        'det_mean': float(np.mean(det_s)),\n",
    "        'det_std': float(np.std(det_s)),\n",
    "        'det_dev_pct': float(100*abs(np.mean(det_s)-DET_G)/DET_G),\n",
    "        'kappa_mean': float((eig_s.max(axis=1)/eig_s.min(axis=1)).mean()),\n",
    "        'pd': bool(np.all(eig_s > 0)),\n",
    "    })\n",
    "    print(f'  T={T:6d}: det(g)={np.mean(det_s):.5f} +/- {np.std(det_s):.5f}, '\n",
    "          f'dev={100*abs(np.mean(det_s)-DET_G)/DET_G:.2f}%, PD={np.all(eig_s > 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Visualization <a id='sec10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.1: Training curves (4-panel)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total loss\n",
    "axes[0,0].semilogy(history['loss'], alpha=0.5, linewidth=0.5)\n",
    "axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].set_xlabel('Step')\n",
    "axes[0,0].axvline(x=PHASE_1_END, color='r', linestyle='--', alpha=0.5, label='Phase 2')\n",
    "axes[0,0].axvline(x=PHASE_2_END, color='g', linestyle='--', alpha=0.5, label='Phase 3')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Determinant error\n",
    "axes[0,1].semilogy(history['det'], alpha=0.5, linewidth=0.5)\n",
    "axes[0,1].set_title('Determinant Error (det(g) - 65/32)^2')\n",
    "axes[0,1].set_xlabel('Step')\n",
    "\n",
    "# Torsion\n",
    "axes[1,0].semilogy(history['torsion'], alpha=0.5, linewidth=0.5)\n",
    "axes[1,0].set_title('Torsion Proxy')\n",
    "axes[1,0].set_xlabel('Step')\n",
    "\n",
    "# Spectral gap\n",
    "if history['lambda1']:\n",
    "    lam_vals = [v * H_STAR for v in history['lambda1']]\n",
    "    axes[1,1].plot(lam_vals, 'b.-', markersize=3)\n",
    "    axes[1,1].axhline(y=14.0, color='r', linestyle='--', label='Target = 14.0')\n",
    "    axes[1,1].set_title('Spectral Gap: lambda1 x H*')\n",
    "    axes[1,1].set_xlabel('Spectral eval #')\n",
    "    axes[1,1].legend()\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No spectral data yet', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.2: Spectral convergence\n",
    "\n",
    "if history['lambda1']:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    lam_vals = [v * H_STAR for v in history['lambda1']]\n",
    "    ax.plot(lam_vals, 'b-o', markersize=4, alpha=0.7)\n",
    "    ax.axhline(y=14.0, color='r', linewidth=2, linestyle='--', label='Target: 14.0')\n",
    "    ax.fill_between(range(len(lam_vals)), 14*0.85, 14*1.15, alpha=0.1, color='r', label='15% band')\n",
    "    ax.set_xlabel('Spectral evaluation #', fontsize=12)\n",
    "    ax.set_ylabel('lambda1 x H*', fontsize=12)\n",
    "    ax.set_title('Spectral Gap Convergence', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('spectral_convergence.png', dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved: spectral_convergence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.3: Metric field visualization (2D slices)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Diagonal components\n",
    "for i in range(DIM):\n",
    "    ax = axes[i//4, i%4]\n",
    "    ax.hist(g_all[:, i, i], bins=50, alpha=0.7, color=f'C{i}')\n",
    "    ax.axvline(x=C_REF**2, color='r', linestyle='--', label=f'ref={C_REF**2:.4f}')\n",
    "    ax.set_title(f'g_{{{i}{i}}}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Off-diagonal distribution\n",
    "offdiag = []\n",
    "for i in range(DIM):\n",
    "    for j in range(i+1, DIM):\n",
    "        offdiag.extend(g_all[:, i, j].tolist())\n",
    "axes[1, 3].hist(offdiag, bins=50, alpha=0.7, color='gray')\n",
    "axes[1, 3].set_title('All off-diagonal g_ij')\n",
    "axes[1, 3].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "plt.suptitle('Metric Field Components (50K points)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_field.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: metric_field.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.4: Determinant stability across manifold and scales\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Determinant histogram\n",
    "axes[0].hist(det_all, bins=50, alpha=0.7, density=True)\n",
    "axes[0].axvline(x=DET_G, color='r', linewidth=2, linestyle='--', label=f'Target: {DET_G}')\n",
    "axes[0].set_xlabel('det(g)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Determinant Distribution (50K points)', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Determinant vs scale\n",
    "T_vals = [sm['T'] for sm in scale_metrics]\n",
    "det_vals = [sm['det_mean'] for sm in scale_metrics]\n",
    "det_errs = [sm['det_std'] for sm in scale_metrics]\n",
    "axes[1].errorbar(T_vals, det_vals, yerr=det_errs, fmt='bo-', capsize=5)\n",
    "axes[1].axhline(y=DET_G, color='r', linewidth=2, linestyle='--', label=f'Target: {DET_G}')\n",
    "axes[1].set_xlabel('T (scale)', fontsize=12)\n",
    "axes[1].set_ylabel('det(g)', fontsize=12)\n",
    "axes[1].set_title('Determinant vs Scale', fontsize=13)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('det_stability.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: det_stability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.5: Torsion map\n",
    "\n",
    "# Compute torsion at evaluation points\n",
    "eps = 0.01\n",
    "torsion_vals = np.zeros(min(10000, N_EVAL))\n",
    "with torch.no_grad():\n",
    "    x_tor = x_eval[:10000]\n",
    "    phi_0 = model.forward(x_tor)[0]\n",
    "    for dim in range(DIM):\n",
    "        x_p = x_tor.clone()\n",
    "        x_p[:, dim] += eps\n",
    "        phi_p = model.forward(x_p)[0]\n",
    "        dphi = ((phi_p - phi_0) / eps).cpu().numpy()\n",
    "        torsion_vals += np.mean(dphi**2, axis=1)\n",
    "    torsion_vals = np.sqrt(torsion_vals / DIM)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(torsion_vals, bins=50, alpha=0.7)\n",
    "axes[0].axvline(x=0.1, color='r', linestyle='--', label='Joyce bound')\n",
    "axes[0].set_xlabel('||torsion||', fontsize=12)\n",
    "axes[0].set_title('Torsion Distribution', fontsize=13)\n",
    "axes[0].legend()\n",
    "\n",
    "coords_np = x_eval[:10000].cpu().numpy()\n",
    "axes[1].scatter(coords_np[:, 0], coords_np[:, 1], c=torsion_vals, s=1, cmap='hot')\n",
    "axes[1].set_xlabel('theta (S1)', fontsize=12)\n",
    "axes[1].set_ylabel('q1_x', fontsize=12)\n",
    "axes[1].set_title('Torsion Map (2D slice)', fontsize=13)\n",
    "plt.colorbar(axes[1].collections[0], ax=axes[1], label='||torsion||')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('torsion_map.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'Torsion: mean={np.mean(torsion_vals):.6f}, max={np.max(torsion_vals):.6f}')\n",
    "print('Saved: torsion_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.6: Period verification bar chart\n",
    "\n",
    "T_show = T_SCALES[3]  # T = 40000\n",
    "if T_show in period_results:\n",
    "    Pi_t = np.array(period_results[T_show]['target'])\n",
    "    Pi_a = np.array(period_results[T_show]['achieved'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Local periods (first 35)\n",
    "    x_idx = np.arange(35)\n",
    "    axes[0].bar(x_idx - 0.2, Pi_t[:35], width=0.4, alpha=0.7, label='Target')\n",
    "    axes[0].bar(x_idx + 0.2, Pi_a[:35], width=0.4, alpha=0.7, label='PINN')\n",
    "    axes[0].set_xlabel('Modulus index k')\n",
    "    axes[0].set_ylabel('Pi_k')\n",
    "    axes[0].set_title(f'Local Periods (35) at T={T_show}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Global periods (last 42)\n",
    "    x_idx = np.arange(42)\n",
    "    axes[1].bar(x_idx - 0.2, Pi_t[35:], width=0.4, alpha=0.7, label='Target')\n",
    "    axes[1].bar(x_idx + 0.2, Pi_a[35:], width=0.4, alpha=0.7, label='PINN')\n",
    "    axes[1].set_xlabel('Modulus index k - 35')\n",
    "    axes[1].set_ylabel('Pi_k')\n",
    "    axes[1].set_title(f'Global Periods (42) at T={T_show}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('period_verification.png', dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved: period_verification.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Export <a id='sec11'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.1: Export all results to JSON\n",
    "\n",
    "results = {\n",
    "    'metadata': {\n",
    "        'notebook': 'K7_PINN_Step5_Reconstruction',\n",
    "        'date': time.strftime('%Y-%m-%d'),\n",
    "        'gpu': gpu_name if GPU_AVAILABLE else 'CPU',\n",
    "        'training_time_min': float(total_time / 60),\n",
    "        'n_params': int(n_params),\n",
    "        'n_epochs': int(PHASE_3_END),\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'G2MetricPINN',\n",
    "        'fourier_freq': 48,\n",
    "        'hidden_dims': [256, 256, 256, 128],\n",
    "        'perturbation_scale': 0.1,\n",
    "        'n_params': int(n_params),\n",
    "    },\n",
    "    'metric': {\n",
    "        'det_mean': float(np.mean(det_all)),\n",
    "        'det_std': float(np.std(det_all)),\n",
    "        'det_target': float(DET_G),\n",
    "        'det_deviation_pct': float(100 * abs(np.mean(det_all) - DET_G) / DET_G),\n",
    "        'eigenvalue_min': float(eigvals_all.min()),\n",
    "        'eigenvalue_max': float(eigvals_all.max()),\n",
    "        'eigenvalue_mean': float(eigvals_all.mean()),\n",
    "        'positive_definite': bool(np.all(eigvals_all > 0)),\n",
    "        'condition_number_mean': float((eigvals_all.max(axis=1)/eigvals_all.min(axis=1)).mean()),\n",
    "        'g_mean_diagonal': np.diag(g_mean).tolist(),\n",
    "        'g_mean_matrix': g_mean.tolist(),\n",
    "    },\n",
    "    'spectral': {\n",
    "        'lambda1': float(lambda1_final),\n",
    "        'lambda1_times_Hstar': float(lambda1_final * H_STAR),\n",
    "        'target_lambda1': float(LAMBDA1),\n",
    "        'target_lambda1_times_Hstar': 14.0,\n",
    "        'deviation_pct': float(100 * abs(lambda1_final - LAMBDA1) / LAMBDA1),\n",
    "    },\n",
    "    'torsion': {\n",
    "        'mean': float(np.mean(torsion_vals)),\n",
    "        'max': float(np.max(torsion_vals)),\n",
    "        'joyce_bound': 0.1,\n",
    "        'passed': bool(np.max(torsion_vals) < 0.1),\n",
    "    },\n",
    "    'periods': {str(int(T)): pr for T, pr in period_results.items()},\n",
    "    'scale_evolution': scale_metrics,\n",
    "    'convergence': {\n",
    "        'final_loss': float(history['loss'][-1]) if history['loss'] else None,\n",
    "        'best_loss': float(best_loss),\n",
    "        'n_spectral_evals': len(history['lambda1']),\n",
    "    },\n",
    "    'validation': {\n",
    "        'det_passed': bool(abs(np.mean(det_all) - DET_G) / DET_G < 0.01),\n",
    "        'pd_passed': bool(np.all(eigvals_all > 0)),\n",
    "        'torsion_passed': bool(np.max(torsion_vals) < 0.1),\n",
    "        'spectral_passed': bool(abs(lambda1_final - LAMBDA1) / LAMBDA1 < 0.15),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('k7_pinn_step5_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('Saved: k7_pinn_step5_results.json')\n",
    "\n",
    "# Training history\n",
    "with open('k7_pinn_step5_history.json', 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f)\n",
    "print('Saved: k7_pinn_step5_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.2: Export NumPy arrays and model checkpoints\n",
    "\n",
    "np.save('k7_pinn_step5_metric.npy', g_all)\n",
    "np.save('k7_pinn_step5_phi.npy', phi_all)\n",
    "np.save('k7_pinn_step5_coords.npy', x_eval.cpu().numpy())\n",
    "print(f'Saved: k7_pinn_step5_metric.npy ({g_all.shape})')\n",
    "print(f'Saved: k7_pinn_step5_phi.npy ({phi_all.shape})')\n",
    "print(f'Saved: k7_pinn_step5_coords.npy')\n",
    "\n",
    "# Multi-scale metric\n",
    "g_multiscale = []\n",
    "for T in T_SCALES:\n",
    "    with torch.no_grad():\n",
    "        g_T = model.metric(x_eval[:10000], np.log(T)).cpu().numpy()\n",
    "        g_multiscale.append(g_T)\n",
    "g_multiscale = np.stack(g_multiscale, axis=0)\n",
    "np.save('k7_pinn_step5_metric_multiscale.npy', g_multiscale)\n",
    "print(f'Saved: k7_pinn_step5_metric_multiscale.npy ({g_multiscale.shape})')\n",
    "\n",
    "# Copy to Drive if available\n",
    "if DRIVE_DIR:\n",
    "    import shutil\n",
    "    for f in ['k7_pinn_step5_results.json', 'k7_pinn_step5_history.json',\n",
    "              'k7_pinn_step5_metric.npy', 'k7_pinn_step5_phi.npy']:\n",
    "        shutil.copy(f, os.path.join(DRIVE_DIR, f))\n",
    "    print(f'Copied to Google Drive: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.3: Final model save\n",
    "\n",
    "torch.save(model.state_dict(), 'k7_pinn_step5_final.pt')\n",
    "print(f'Saved: k7_pinn_step5_final.pt ({n_params:,} parameters)')\n",
    "\n",
    "if DRIVE_DIR:\n",
    "    import shutil\n",
    "    shutil.copy('k7_pinn_step5_final.pt', os.path.join(DRIVE_DIR, 'k7_pinn_step5_final.pt'))\n",
    "    for fig_name in ['training_curves.png', 'spectral_convergence.png', 'metric_field.png',\n",
    "                     'det_stability.png', 'torsion_map.png', 'period_verification.png']:\n",
    "        if os.path.exists(fig_name):\n",
    "            shutil.copy(fig_name, os.path.join(DRIVE_DIR, fig_name))\n",
    "    print('All figures and model copied to Google Drive.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary & Conclusions <a id='sec12'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12.1: Final summary\n",
    "\n",
    "print('=' * 70)\n",
    "print('  STEP 5 RESULTS: PINN RECONSTRUCTION OF K7 METRIC')\n",
    "print('=' * 70)\n",
    "\n",
    "V = results['validation']\n",
    "M = results['metric']\n",
    "S = results['spectral']\n",
    "T = results['torsion']\n",
    "\n",
    "print(f'''\n",
    "  MODEL\n",
    "    Architecture: G2MetricPINN ({n_params:,} params)\n",
    "    Training: {PHASE_3_END} epochs, {total_time/60:.1f} min on {gpu_name if GPU_AVAILABLE else \"CPU\"}\n",
    "\n",
    "  METRIC\n",
    "    det(g): {M[\"det_mean\"]:.6f} +/- {M[\"det_std\"]:.6f}  (target: {DET_G})\n",
    "    Deviation: {M[\"det_deviation_pct\"]:.3f}%\n",
    "    Positive definite: {M[\"positive_definite\"]}\n",
    "    Condition number: {M[\"condition_number_mean\"]:.4f}\n",
    "    PASSED: {\"YES\" if V[\"det_passed\"] else \"NO\"} (< 1% deviation)\n",
    "\n",
    "  SPECTRAL GAP\n",
    "    lambda1: {S[\"lambda1\"]:.6f}  (target: {LAMBDA1:.6f})\n",
    "    lambda1 * H*: {S[\"lambda1_times_Hstar\"]:.4f}  (target: 14.0)\n",
    "    Deviation: {S[\"deviation_pct\"]:.2f}%\n",
    "    PASSED: {\"YES\" if V[\"spectral_passed\"] else \"NO\"} (< 15% deviation)\n",
    "\n",
    "  TORSION\n",
    "    Mean: {T[\"mean\"]:.6f}\n",
    "    Max:  {T[\"max\"]:.6f}  (Joyce bound: 0.1)\n",
    "    PASSED: {\"YES\" if V[\"torsion_passed\"] else \"NO\"}\n",
    "\n",
    "  OVERALL: {sum(V.values())}/4 criteria passed\n",
    "''')\n",
    "\n",
    "print('  OUTPUTS:')\n",
    "print('    k7_pinn_step5_results.json    — all metrics')\n",
    "print('    k7_pinn_step5_history.json    — training curves')\n",
    "print('    k7_pinn_step5_metric.npy      — 50K metric tensors (50000,7,7)')\n",
    "print('    k7_pinn_step5_phi.npy         — 50K 3-forms (50000,35)')\n",
    "print('    k7_pinn_step5_final.pt        — trained model')\n",
    "print('    training_curves.png           — 4-panel training diagnostics')\n",
    "print('    spectral_convergence.png      — lambda1*H* vs epoch')\n",
    "print('    metric_field.png              — metric component distributions')\n",
    "print('    det_stability.png             — determinant stability')\n",
    "print('    torsion_map.png               — torsion visualization')\n",
    "print('    period_verification.png       — period integrals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Notebook Produces\n",
    "\n",
    "The PINN learns a position-dependent G₂ structure $\\varphi(x)$ on the TCS neck of K₇,\n",
    "yielding the metric $g_{ij}(x) = \\frac{1}{6}\\sum_{k,l}\\varphi_{ikl}\\varphi_{jkl}$.\n",
    "\n",
    "**Data for the exhaustive document:**\n",
    "- Full 7×7 metric tensor at 50,000 points\n",
    "- Metric at 5 energy scales (T = 100, 1000, 10K, 40K, 75K)\n",
    "- Spectral gap convergence curve (Rayleigh quotient estimates)\n",
    "- 77 period integrals verified against prime-spectral targets\n",
    "- Torsion field mapped across the manifold\n",
    "- Determinant stability (spatial and scale-dependent)\n",
    "- Complete training diagnostics\n",
    "\n",
    "**Connection to Steps 1–4:**\n",
    "- The 77 moduli $\\Pi_k(T)$ from the mollified Dirichlet polynomial (Steps 1–2)\n",
    "  constrain the PINN's period integrals\n",
    "- The G₂ decomposition (Step 4) ensures only 14 local DOF (not 35)\n",
    "- The metric Jacobian $\\partial g/\\partial\\Pi_k$ from Step 4 guides the warm-start\n",
    "- The E₈/K3 lattice infrastructure (Step 4) determines the 42 global modes\n",
    "\n",
    "---\n",
    "*GIFT Framework — Step 5: PINN Reconstruction*"
   ]
  }
 ]
}