# YANG-MILLS MASS GAP Ã— GIFT
## Plan d'Attaque Complet pour Claude Code

**Objectif**: DÃ©montrer que la topologie de Kâ‚‡ implique un mass gap pour la thÃ©orie de jauge
**Ressources**: Google Colab A100, repo GIFT existant
**Timeline**: Phased approach

---

## ğŸ¯ SYNTHÃˆSE DES PERSPECTIVES IA

### Consensus
- **Yang-Mills est LA piste** (4 Ã©toiles unanimes)
- Le gap spectral Î»â‚ > 0 est **mathÃ©matiquement obligatoire** sur une variÃ©tÃ© compacte
- La question est : **comment ce gap se propage vers 4D ?**

### Formule cible
```
Î” = (dim(Gâ‚‚)/H*) Ã— Î›_QCD = (14/99) Ã— Î›_QCD â‰ˆ 28 MeV

oÃ¹:
- 14 = pâ‚‚ Ã— dim(Kâ‚‡) = 2 Ã— 7
- 99 = N_genÂ² Ã— D_bulk = 9 Ã— 11
```

### Constante de Cheeger cible
```
h(Kâ‚‡) â‰ˆ 14/99 â‰ˆ 0.1414
Î»â‚ â‰¥ hÂ²/4 â‰ˆ 0.005
```

---

## ğŸ“‹ STRUCTURE DU REPO

```
gift-yang-mills/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ 
â”œâ”€â”€ notebooks/                    # Colab notebooks (A100)
â”‚   â”œâ”€â”€ 01_pinn_metric_training.ipynb
â”‚   â”œâ”€â”€ 02_manifold_sampling.ipynb
â”‚   â”œâ”€â”€ 03_spectral_analysis.ipynb
â”‚   â”œâ”€â”€ 04_cheeger_estimation.ipynb
â”‚   â””â”€â”€ 05_kk_reduction.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ constants.py              # GIFT constants
â”‚   â”œâ”€â”€ g2_structure.py           # Gâ‚‚ forms Ï†, Ïˆ
â”‚   â”œâ”€â”€ tcs_manifold.py           # TCS construction
â”‚   â”œâ”€â”€ metric_pinn.py            # PINN for metric
â”‚   â”œâ”€â”€ spectral/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ graph_laplacian.py    # Discrete Laplacian
â”‚   â”‚   â”œâ”€â”€ hodge_laplacian.py    # Hodge-de Rham
â”‚   â”‚   â”œâ”€â”€ cheeger.py            # Isoperimetric constant
â”‚   â”‚   â””â”€â”€ eigensolvers.py       # Spectral methods
â”‚   â”œâ”€â”€ gauge/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ e8_structure.py       # Eâ‚ˆ roots, Cartan
â”‚   â”‚   â”œâ”€â”€ breaking_chain.py     # Eâ‚ˆ â†’ SM
â”‚   â”‚   â””â”€â”€ kk_reduction.py       # Kaluza-Klein
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ spectrum_plots.py
â”‚       â””â”€â”€ manifold_viz.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pinn_checkpoints/
â”‚   â”œâ”€â”€ spectral_results/
â”‚   â””â”€â”€ exports/
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py
â”‚
â””â”€â”€ paper/
    â”œâ”€â”€ yang_mills_gift.tex
    â””â”€â”€ figures/
```

---

## ğŸš€ PHASE 1: INFRASTRUCTURE (Semaine 1)

### 1.1 Fichier de constantes GIFT

**Fichier**: `src/constants.py`

```python
"""
GIFT Framework Constants
All values are topologically derived - zero free parameters
"""
from dataclasses import dataclass
from fractions import Fraction
import numpy as np

@dataclass(frozen=True)
class GIFTConstants:
    # Manifold topology
    dim_K7: int = 7
    b2: int = 21          # Second Betti number
    b3: int = 77          # Third Betti number
    H_star: int = 99      # b2 + b3 + 1
    
    # Holonomy
    dim_G2: int = 14
    rank_G2: int = 2
    
    # Gauge structure
    dim_E8: int = 248
    rank_E8: int = 8
    dim_E8xE8: int = 496
    roots_E8: int = 240
    coxeter_E8: int = 30
    
    # Derived constants
    N_gen: int = 3
    Weyl: int = 5
    p2: int = 2
    D_bulk: int = 11
    
    # Metric
    det_g_num: int = 65
    det_g_den: int = 32
    
    # Torsion
    kappa_T_inv: int = 61
    
    # Yang-Mills targets
    @property
    def det_g(self) -> float:
        return self.det_g_num / self.det_g_den
    
    @property
    def kappa_T(self) -> float:
        return 1 / self.kappa_T_inv
    
    @property
    def cheeger_target(self) -> float:
        """Target Cheeger constant h(Kâ‚‡) = dim(Gâ‚‚)/H*"""
        return self.dim_G2 / self.H_star  # 14/99 â‰ˆ 0.1414
    
    @property
    def lambda1_lower_bound(self) -> float:
        """Cheeger inequality: Î»â‚ â‰¥ hÂ²/4"""
        return self.cheeger_target**2 / 4  # â‰ˆ 0.005
    
    @property
    def mass_gap_ratio(self) -> Fraction:
        """Î”/Î›_QCD = dim(Gâ‚‚)/H*"""
        return Fraction(self.dim_G2, self.H_star)

GIFT = GIFTConstants()
```

### 1.2 Structure Gâ‚‚

**Fichier**: `src/g2_structure.py`

```python
"""
Gâ‚‚ Structure on 7-manifolds
Implements the associative 3-form Ï† and coassociative 4-form Ïˆ
"""
import numpy as np
from itertools import permutations

# Standard Gâ‚‚ structure constants (Bryant convention)
# Ï† = e^{123} + e^{145} + e^{167} + e^{246} - e^{257} - e^{347} - e^{356}
G2_PHI_TERMS = [
    ((0, 1, 2), +1),
    ((0, 3, 4), +1),
    ((0, 5, 6), +1),
    ((1, 3, 5), +1),
    ((1, 4, 6), -1),
    ((2, 3, 6), -1),
    ((2, 4, 5), -1),
]

class G2Structure:
    """Gâ‚‚ structure with associative 3-form Ï†."""
    
    def __init__(self, scale: float = 1.0):
        """
        Initialize Gâ‚‚ structure.
        
        Args:
            scale: Scaling factor c = (65/32)^(1/14) for GIFT metric
        """
        self.scale = scale
        self._build_phi()
        self._build_psi()
    
    def _build_phi(self):
        """Build the associative 3-form Ï†."""
        self.phi = np.zeros((7, 7, 7))
        for indices, sign in G2_PHI_TERMS:
            for perm in permutations(range(3)):
                perm_sign = self._perm_sign(perm)
                i, j, k = [indices[p] for p in perm]
                self.phi[i, j, k] = sign * perm_sign * self.scale
    
    def _build_psi(self):
        """Build coassociative 4-form Ïˆ = *Ï†."""
        # Ïˆ is the Hodge dual of Ï†
        self.psi = np.zeros((7, 7, 7, 7))
        # Implementation details...
    
    @staticmethod
    def _perm_sign(perm):
        """Compute sign of permutation."""
        n = len(perm)
        inversions = sum(1 for i in range(n) for j in range(i+1, n) 
                        if perm[i] > perm[j])
        return (-1) ** inversions
    
    def phi_norm_squared(self) -> float:
        """Compute ||Ï†||Â² = 7 for standard structure."""
        return np.sum(self.phi ** 2) / 6  # Factor from antisymmetry
    
    def metric_from_phi(self) -> np.ndarray:
        """
        Extract metric from Ï† using Bryant's formula.
        For standard Ï†â‚€, returns scaled identity.
        """
        # g_ij = (1/6) Ï†_imn Ï†_jpq Ï†_krs Îµ^mnpqrs Î´^k / sqrt(det)
        # For standard Ï†: g = scaleÂ² Ã— Iâ‚‡
        return self.scale**2 * np.eye(7)
    
    def torsion_norm(self, dÏ†: np.ndarray, dÏˆ: np.ndarray) -> float:
        """
        Compute torsion ||T||Â² = ||dÏ†||Â² + ||d*Ï†||Â²
        
        For torsion-free Gâ‚‚: dÏ† = 0 and d*Ï† = 0
        """
        return np.sum(dÏ†**2) + np.sum(dÏˆ**2)
```

---

## ğŸ”¬ PHASE 2: PINN METRIC TRAINING (Semaine 2)

### 2.1 Notebook Colab: `01_pinn_metric_training.ipynb`

**Objectif**: EntraÃ®ner un PINN pour apprendre la mÃ©trique g_ij sur Kâ‚‡

```python
# HEADER pour Colab
"""
GIFT Yang-Mills: PINN Metric Training
=====================================
Runtime: A100 GPU
Objective: Learn g_ij(x) satisfying Gâ‚‚ constraints
"""

# !pip install torch numpy scipy matplotlib tqdm

import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# GIFT Constants
DIM_K7 = 7
DET_G_TARGET = 65/32
KAPPA_T = 1/61

class MetricPINN(nn.Module):
    """
    Physics-Informed Neural Network for Gâ‚‚ metric.
    
    Outputs: 28 independent components of symmetric 7Ã—7 metric
    (7 diagonal + 21 upper triangular)
    """
    
    def __init__(self, hidden_dim=256, num_layers=6):
        super().__init__()
        
        layers = [nn.Linear(DIM_K7, hidden_dim), nn.SiLU()]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.SiLU()])
        layers.append(nn.Linear(hidden_dim, 28))
        
        self.network = nn.Sequential(*layers)
        
        # Initialize to near-identity metric
        self._init_weights()
    
    def _init_weights(self):
        """Initialize to output near-identity metric."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        """
        Args:
            x: Points on Kâ‚‡, shape (batch, 7)
        Returns:
            g: Metric tensors, shape (batch, 7, 7)
        """
        components = self.network(x)
        return self._components_to_metric(components)
    
    def _components_to_metric(self, components):
        """Convert 28 components to symmetric 7Ã—7 matrix."""
        batch = components.shape[0]
        g = torch.zeros(batch, 7, 7, device=components.device)
        
        idx = 0
        for i in range(7):
            for j in range(i, 7):
                # Ensure positive definiteness for diagonal
                if i == j:
                    g[:, i, j] = torch.exp(components[:, idx])
                else:
                    g[:, i, j] = components[:, idx]
                    g[:, j, i] = components[:, idx]
                idx += 1
        
        return g


class G2PhysicsLoss(nn.Module):
    """
    Physics loss for Gâ‚‚ holonomy constraints.
    """
    
    def __init__(self, det_target=65/32, torsion_weight=10.0):
        super().__init__()
        self.det_target = det_target
        self.torsion_weight = torsion_weight
    
    def forward(self, g, x):
        """
        Compute physics-informed loss.
        
        Components:
        1. Determinant constraint: det(g) = 65/32
        2. Ricci-flatness: R_ij â‰ˆ 0 (approximated)
        3. Torsion minimization: ||dÏ†||Â² + ||d*Ï†||Â² â†’ 0
        """
        batch = g.shape[0]
        
        # 1. Determinant loss
        det_g = torch.linalg.det(g)
        loss_det = torch.mean((det_g - self.det_target)**2)
        
        # 2. Positive definiteness (all eigenvalues > 0)
        eigenvalues = torch.linalg.eigvalsh(g)
        loss_pd = torch.mean(torch.relu(-eigenvalues + 0.01)**2)
        
        # 3. Smoothness (Laplacian regularization)
        # Approximate by finite differences
        eps = 0.01
        loss_smooth = 0.0
        for i in range(7):
            x_plus = x.clone()
            x_minus = x.clone()
            x_plus[:, i] += eps
            x_minus[:, i] -= eps
            # Would need model reference here - simplified
        
        # 4. Torsion (requires dÏ† computation - simplified here)
        # Full implementation in separate function
        loss_torsion = torch.tensor(0.0, device=g.device)
        
        total_loss = loss_det + 0.1 * loss_pd + self.torsion_weight * loss_torsion
        
        return total_loss, {
            'det': loss_det.item(),
            'pd': loss_pd.item(),
            'torsion': loss_torsion.item()
        }


def sample_K7_points(n_points, method='uniform'):
    """
    Sample points on Kâ‚‡ manifold.
    
    For TCS construction, we use local coordinates on
    the cylindrical regions.
    """
    if method == 'uniform':
        # Simple uniform sampling in [0, 2Ï€]^7
        return torch.rand(n_points, 7) * 2 * np.pi
    elif method == 'gaussian':
        return torch.randn(n_points, 7)
    else:
        raise ValueError(f"Unknown method: {method}")


def train_metric_pinn(epochs=10000, batch_size=1024, lr=1e-3):
    """Main training loop."""
    
    model = MetricPINN().to(device)
    loss_fn = G2PhysicsLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
    
    history = {'loss': [], 'det': [], 'torsion': []}
    
    pbar = tqdm(range(epochs), desc="Training PINN")
    for epoch in pbar:
        # Sample points
        x = sample_K7_points(batch_size).to(device)
        
        # Forward
        g = model(x)
        loss, metrics = loss_fn(g, x)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        
        # Log
        history['loss'].append(loss.item())
        history['det'].append(metrics['det'])
        
        if epoch % 100 == 0:
            det_mean = torch.linalg.det(g).mean().item()
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'det': f"{det_mean:.4f}",
                'target': f"{65/32:.4f}"
            })
    
    return model, history


# MAIN
if __name__ == "__main__":
    model, history = train_metric_pinn(epochs=5000)
    
    # Save checkpoint
    torch.save({
        'model_state': model.state_dict(),
        'history': history
    }, 'pinn_metric_checkpoint.pt')
    
    print("Training complete!")
```

---

## ğŸ“Š PHASE 3: SPECTRAL ANALYSIS (Semaine 3-4)

### 3.1 Notebook Colab: `03_spectral_analysis.ipynb`

**Objectif**: Calculer le spectre du Laplacien et estimer Î»â‚

```python
"""
GIFT Yang-Mills: Spectral Analysis
==================================
Compute the spectrum of the Hodge Laplacian on Kâ‚‡
Key target: Î»â‚ â‰¥ (14/99)Â²/4 â‰ˆ 0.005
"""

import numpy as np
import torch
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import eigsh
from scipy.spatial import KDTree
from tqdm import tqdm

class ManifoldSampler:
    """
    Sample points from Kâ‚‡ using trained PINN metric.
    """
    
    def __init__(self, pinn_model, n_points=10000):
        self.model = pinn_model
        self.n_points = n_points
        self.points = None
        self.metric_at_points = None
    
    def sample(self, method='importance'):
        """
        Sample points on the manifold.
        
        For importance sampling, weight by sqrt(det(g))
        to get uniform distribution w.r.t. volume form.
        """
        device = next(self.model.parameters()).device
        
        if method == 'uniform':
            self.points = torch.rand(self.n_points, 7) * 2 * np.pi
        
        elif method == 'importance':
            # Oversample and reject
            oversample = 5 * self.n_points
            candidates = torch.rand(oversample, 7, device=device) * 2 * np.pi
            
            with torch.no_grad():
                g = self.model(candidates)
                det_g = torch.linalg.det(g)
                weights = torch.sqrt(torch.abs(det_g))
                weights = weights / weights.sum()
            
            # Resample according to weights
            indices = torch.multinomial(weights, self.n_points, replacement=False)
            self.points = candidates[indices].cpu()
        
        # Compute metric at sampled points
        with torch.no_grad():
            self.metric_at_points = self.model(self.points.to(device)).cpu().numpy()
        
        return self.points.numpy()


class GraphLaplacian:
    """
    Approximate Hodge Laplacian using graph Laplacian.
    
    Method:
    1. Build k-NN graph from sampled points
    2. Weight edges by metric tensor (geodesic approximation)
    3. Compute normalized graph Laplacian
    4. Extract spectrum
    """
    
    def __init__(self, points, metric, k_neighbors=20):
        """
        Args:
            points: (N, 7) array of sampled points
            metric: (N, 7, 7) array of metric tensors at points
            k_neighbors: Number of nearest neighbors
        """
        self.points = points
        self.metric = metric
        self.k = k_neighbors
        self.N = len(points)
        
    def build_weighted_adjacency(self):
        """
        Build weighted adjacency matrix using metric-weighted distances.
        
        W_ij = exp(-d_g(x_i, x_j)Â² / ÏƒÂ²)
        
        where d_g is the geodesic distance approximated by:
        d_g(x,y) â‰ˆ sqrt((x-y)^T g(x) (x-y))
        """
        print("Building KD-tree...")
        tree = KDTree(self.points)
        
        print("Computing weighted adjacency...")
        rows, cols, data = [], [], []
        
        for i in tqdm(range(self.N), desc="Building graph"):
            # Find k nearest neighbors in Euclidean metric
            dists, neighbors = tree.query(self.points[i], k=self.k+1)
            
            g_i = self.metric[i]  # Metric at point i
            
            for j, neighbor in enumerate(neighbors[1:]):  # Skip self
                # Compute metric-weighted distance
                diff = self.points[neighbor] - self.points[i]
                d_g_sq = diff @ g_i @ diff
                
                # Gaussian kernel
                sigma = np.median(dists[1:])  # Adaptive bandwidth
                weight = np.exp(-d_g_sq / (2 * sigma**2))
                
                rows.append(i)
                cols.append(neighbor)
                data.append(weight)
        
        W = csr_matrix((data, (rows, cols)), shape=(self.N, self.N))
        # Symmetrize
        W = (W + W.T) / 2
        
        return W
    
    def build_laplacian(self, W, normalized=True):
        """
        Build graph Laplacian from adjacency matrix.
        
        Normalized: L = I - D^{-1/2} W D^{-1/2}
        Unnormalized: L = D - W
        """
        degree = np.array(W.sum(axis=1)).flatten()
        
        if normalized:
            # Avoid division by zero
            degree = np.maximum(degree, 1e-10)
            D_inv_sqrt = csr_matrix(np.diag(1.0 / np.sqrt(degree)))
            L = csr_matrix(np.eye(self.N)) - D_inv_sqrt @ W @ D_inv_sqrt
        else:
            D = csr_matrix(np.diag(degree))
            L = D - W
        
        return L
    
    def compute_spectrum(self, n_eigenvalues=100):
        """
        Compute the first n eigenvalues of the Laplacian.
        
        Returns:
            eigenvalues: Sorted array of eigenvalues
            eigenvectors: Corresponding eigenvectors
        """
        print("Building weighted adjacency matrix...")
        W = self.build_weighted_adjacency()
        
        print("Building Laplacian...")
        L = self.build_laplacian(W, normalized=True)
        
        print(f"Computing {n_eigenvalues} smallest eigenvalues...")
        # 'SM' = Smallest Magnitude
        eigenvalues, eigenvectors = eigsh(L, k=n_eigenvalues, which='SM')
        
        # Sort by eigenvalue
        idx = np.argsort(eigenvalues)
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        return eigenvalues, eigenvectors


class HodgeLaplacian:
    """
    More accurate Hodge-de Rham Laplacian on k-forms.
    
    For 0-forms (functions): Î”â‚€ = d*d
    For 1-forms: Î”â‚ = dd* + d*d
    
    The mass gap comes from Î”â‚ on the gauge sector.
    """
    
    def __init__(self, points, metric):
        self.points = points
        self.metric = metric
        self.N = len(points)
    
    def compute_spectrum_on_1forms(self, n_eigenvalues=50):
        """
        Compute spectrum of Î”â‚ (Laplacian on 1-forms).
        
        This is the physically relevant operator for gauge fields.
        
        Note: Full implementation requires discrete exterior calculus (DEC).
        Here we use a simplified approach based on vector Laplacian.
        """
        # For a complete implementation, use PyDEC or similar
        # This is a placeholder for the structure
        
        # Vector Laplacian: Î”_vec = (d*d + dd*) on vector fields
        # In local coordinates: (Î”v)^i = g^{jk} âˆ‡_j âˆ‡_k v^i + R^i_j v^j
        
        # For Ricci-flat (Gâ‚‚ holonomy): R_ij = 0
        # So Î”_vec reduces to the rough Laplacian
        
        # Approximation: use scalar Laplacian on each component
        # This gives a lower bound on the true spectrum
        
        raise NotImplementedError(
            "Full Hodge Laplacian requires DEC implementation. "
            "See Phase 4 for detailed approach."
        )


def analyze_spectrum(eigenvalues, gift_constants):
    """
    Analyze the spectrum and compare with GIFT predictions.
    """
    print("\n" + "="*60)
    print("SPECTRAL ANALYSIS RESULTS")
    print("="*60)
    
    # Mass gap (first nonzero eigenvalue)
    # Î»â‚€ â‰ˆ 0 (constant mode)
    lambda_0 = eigenvalues[0]
    lambda_1 = eigenvalues[1]
    
    print(f"\nFirst eigenvalues:")
    print(f"  Î»â‚€ = {lambda_0:.6f} (should be â‰ˆ 0)")
    print(f"  Î»â‚ = {lambda_1:.6f} (MASS GAP CANDIDATE)")
    print(f"  Î»â‚‚ = {eigenvalues[2]:.6f}")
    print(f"  Î»â‚ƒ = {eigenvalues[3]:.6f}")
    
    # GIFT predictions
    h_target = gift_constants['dim_G2'] / gift_constants['H_star']  # 14/99
    cheeger_bound = h_target**2 / 4
    
    print(f"\nGIFT predictions:")
    print(f"  h(Kâ‚‡) target = dim(Gâ‚‚)/H* = {h_target:.6f}")
    print(f"  Cheeger bound: Î»â‚ â‰¥ hÂ²/4 = {cheeger_bound:.6f}")
    
    # Comparison
    print(f"\nComparison:")
    print(f"  Î»â‚ observed = {lambda_1:.6f}")
    print(f"  Î»â‚ â‰¥ hÂ²/4 satisfied? {lambda_1 >= cheeger_bound * 0.9}")  # 10% tolerance
    
    # Cheeger constant estimation (reverse direction)
    h_estimated = 2 * np.sqrt(lambda_1)
    print(f"\nCheeger constant estimation:")
    print(f"  h â‰¤ 2âˆšÎ»â‚ = {h_estimated:.6f}")
    print(f"  h target = {h_target:.6f}")
    print(f"  Ratio: {h_estimated / h_target:.2f}")
    
    # Spectral gap ratio
    gap_ratio = lambda_1 / lambda_0 if lambda_0 > 1e-10 else float('inf')
    print(f"\nSpectral gap ratio: Î»â‚/Î»â‚€ = {gap_ratio:.2f}")
    
    return {
        'lambda_0': lambda_0,
        'lambda_1': lambda_1,
        'h_estimated': h_estimated,
        'h_target': h_target,
        'cheeger_satisfied': lambda_1 >= cheeger_bound * 0.9
    }


# MAIN EXECUTION
if __name__ == "__main__":
    # Load trained PINN
    checkpoint = torch.load('pinn_metric_checkpoint.pt')
    model = MetricPINN()
    model.load_state_dict(checkpoint['model_state'])
    model.eval()
    
    # Sample manifold
    sampler = ManifoldSampler(model, n_points=10000)
    points = sampler.sample(method='importance')
    metric = sampler.metric_at_points
    
    # Compute spectrum
    laplacian = GraphLaplacian(points, metric, k_neighbors=30)
    eigenvalues, eigenvectors = laplacian.compute_spectrum(n_eigenvalues=100)
    
    # Analyze
    gift_constants = {
        'dim_G2': 14,
        'H_star': 99,
        'dim_K7': 7,
        'b2': 21,
        'b3': 77
    }
    
    results = analyze_spectrum(eigenvalues, gift_constants)
    
    # Save results
    np.savez('spectral_results.npz',
             eigenvalues=eigenvalues,
             eigenvectors=eigenvectors,
             points=points,
             **results)
    
    print("\nResults saved to spectral_results.npz")
```

---

## ğŸ“ PHASE 4: CHEEGER CONSTANT (Semaine 5)

### 4.1 Notebook: `04_cheeger_estimation.ipynb`

**Objectif**: Estimer directement la constante de Cheeger h(Kâ‚‡)

```python
"""
GIFT Yang-Mills: Cheeger Constant Estimation
============================================
Direct estimation of isoperimetric constant h(Kâ‚‡)

Target: h(Kâ‚‡) â‰ˆ 14/99 â‰ˆ 0.1414
"""

import numpy as np
from scipy.optimize import minimize
from sklearn.cluster import SpectralClustering

class CheegerEstimator:
    """
    Estimate Cheeger constant via min-cut optimization.
    
    h(M) = inf_Î© (Area(âˆ‚Î©) / min(Vol(Î©), Vol(M\Î©)))
    
    We approximate this using spectral clustering and
    geometric min-cut algorithms.
    """
    
    def __init__(self, points, metric, adjacency):
        self.points = points
        self.metric = metric
        self.W = adjacency
        self.N = len(points)
    
    def estimate_volumes(self, partition):
        """
        Estimate volumes of partition sets using metric.
        
        Vol(Î©) = âˆ«_Î© âˆšdet(g) dx â‰ˆ Î£_{x_i âˆˆ Î©} âˆšdet(g(x_i)) Ã— cell_volume
        """
        det_g = np.array([np.linalg.det(self.metric[i]) for i in range(self.N)])
        sqrt_det = np.sqrt(np.abs(det_g))
        
        # Approximate cell volumes (Voronoi-like)
        cell_vol = np.ones(self.N) / self.N  # Uniform approximation
        
        vol_omega = np.sum(sqrt_det[partition] * cell_vol[partition])
        vol_complement = np.sum(sqrt_det[~partition] * cell_vol[~partition])
        
        return vol_omega, vol_complement
    
    def estimate_boundary_area(self, partition):
        """
        Estimate area of boundary âˆ‚Î©.
        
        Area(âˆ‚Î©) â‰ˆ Î£_{iâˆˆÎ©, jâˆ‰Î©} w_ij Ã— âˆšdet(g) Ã— distance
        """
        area = 0.0
        W_dense = self.W.toarray()
        
        for i in np.where(partition)[0]:
            for j in np.where(~partition)[0]:
                if W_dense[i, j] > 0:
                    # Edge weight as proxy for boundary area
                    det_avg = np.sqrt(np.abs(np.linalg.det(self.metric[i])))
                    area += W_dense[i, j] * det_avg
        
        return area
    
    def cheeger_ratio(self, partition):
        """
        Compute Cheeger ratio for a given partition.
        
        h(Î©) = Area(âˆ‚Î©) / min(Vol(Î©), Vol(M\Î©))
        """
        vol_omega, vol_complement = self.estimate_volumes(partition)
        area = self.estimate_boundary_area(partition)
        
        min_vol = min(vol_omega, vol_complement)
        if min_vol < 1e-10:
            return float('inf')
        
        return area / min_vol
    
    def spectral_partition(self, n_clusters=2):
        """
        Use spectral clustering to find good partition.
        
        The Fiedler vector (second eigenvector of Laplacian)
        gives an approximately optimal 2-partition.
        """
        clustering = SpectralClustering(
            n_clusters=n_clusters,
            affinity='precomputed',
            assign_labels='discretize'
        )
        labels = clustering.fit_predict(self.W.toarray())
        
        return labels == 0  # Boolean partition
    
    def estimate_cheeger(self, n_trials=100):
        """
        Estimate Cheeger constant via multiple random partitions.
        
        Returns lower bound: h(Kâ‚‡) â‰ˆ min over trials
        """
        print("Estimating Cheeger constant...")
        
        # Method 1: Spectral partition (best single estimate)
        partition_spectral = self.spectral_partition()
        h_spectral = self.cheeger_ratio(partition_spectral)
        print(f"  Spectral partition: h â‰ˆ {h_spectral:.6f}")
        
        # Method 2: Random partitions (exploration)
        h_values = []
        for _ in range(n_trials):
            # Random balanced partition
            partition = np.random.rand(self.N) > 0.5
            h_values.append(self.cheeger_ratio(partition))
        
        h_random_best = min(h_values)
        h_random_mean = np.mean(h_values)
        print(f"  Random partitions: h_min = {h_random_best:.6f}, h_mean = {h_random_mean:.6f}")
        
        # Best estimate
        h_estimate = min(h_spectral, h_random_best)
        
        return {
            'h_estimate': h_estimate,
            'h_spectral': h_spectral,
            'h_random_best': h_random_best,
            'h_random_mean': h_random_mean
        }


def compare_with_gift_target(h_estimate, h_target=14/99):
    """
    Compare estimated Cheeger constant with GIFT prediction.
    """
    print("\n" + "="*60)
    print("CHEEGER CONSTANT COMPARISON")
    print("="*60)
    
    print(f"\nEstimated: h(Kâ‚‡) â‰ˆ {h_estimate:.6f}")
    print(f"GIFT target: h = dim(Gâ‚‚)/H* = 14/99 = {h_target:.6f}")
    print(f"Ratio: {h_estimate / h_target:.2f}")
    
    # Implications for mass gap
    lambda1_bound = h_estimate**2 / 4
    lambda1_target = h_target**2 / 4
    
    print(f"\nImplied mass gap bounds:")
    print(f"  From estimate: Î»â‚ â‰¥ {lambda1_bound:.6f}")
    print(f"  From target:   Î»â‚ â‰¥ {lambda1_target:.6f}")
    
    # Physical mass gap (with Î›_QCD = 200 MeV)
    Lambda_QCD = 200  # MeV
    Delta_estimate = h_estimate * Lambda_QCD
    Delta_target = h_target * Lambda_QCD
    
    print(f"\nPhysical mass gap (with Î›_QCD = {Lambda_QCD} MeV):")
    print(f"  From estimate: Î” â‰ˆ {Delta_estimate:.1f} MeV")
    print(f"  From target:   Î” â‰ˆ {Delta_target:.1f} MeV")
    
    return {
        'h_estimate': h_estimate,
        'h_target': h_target,
        'ratio': h_estimate / h_target,
        'Delta_estimate_MeV': Delta_estimate,
        'Delta_target_MeV': Delta_target
    }
```

---

## ğŸ”— PHASE 5: KALUZA-KLEIN REDUCTION (Semaine 6)

### 5.1 Notebook: `05_kk_reduction.ipynb`

**Objectif**: Montrer comment le gap spectral de Kâ‚‡ se propage vers 4D

```python
"""
GIFT Yang-Mills: Kaluza-Klein Reduction
=======================================
Show that Î»â‚(Kâ‚‡) induces mass gap in 4D gauge theory

Key result:
  11D: â–¡â‚â‚ Ï† = 0
  Decomposition: â–¡â‚â‚ = â–¡â‚„ + Î”_Kâ‚‡
  4D mass: mÂ² = Î»â‚™(Kâ‚‡)
"""

import numpy as np
from sympy import *

class KKReduction:
    """
    Kaluza-Klein reduction of Eâ‚ˆÃ—Eâ‚ˆ gauge theory on Kâ‚‡.
    """
    
    def __init__(self, spectrum_K7, gift_constants):
        """
        Args:
            spectrum_K7: Eigenvalues of Laplacian on Kâ‚‡
            gift_constants: GIFT topological constants
        """
        self.spectrum = spectrum_K7
        self.G = gift_constants
        
    def gauge_field_decomposition(self):
        """
        Decompose 11D gauge field A_M into 4D modes.
        
        A_M(x, y) = Î£_n A_Î¼^(n)(x) âŠ— Ïˆ_n(y)
        
        where:
        - x âˆˆ Mâ‚„ (4D spacetime)
        - y âˆˆ Kâ‚‡ (internal manifold)
        - Ïˆ_n are eigenmodes of Î”_Kâ‚‡
        """
        print("Gauge field decomposition:")
        print("  A_M(x,y) = Î£_n A_Î¼^(n)(x) âŠ— Ïˆ_n(y)")
        print()
        print("  11D Yang-Mills: D_M F^MN = 0")
        print("  â†’ (â–¡â‚„ + Î”_Kâ‚‡) A_Î¼^(n) = 0")
        print("  â†’ (â–¡â‚„ + Î»_n) A_Î¼^(n) = 0")
        print()
        print("  This is a massive 4D field with m_nÂ² = Î»_n")
        
        return {
            'zero_modes': self.spectrum[self.spectrum < 1e-6],
            'massive_modes': self.spectrum[self.spectrum >= 1e-6]
        }
    
    def symmetry_breaking(self):
        """
        Eâ‚ˆÃ—Eâ‚ˆ â†’ SU(3)Ã—SU(2)Ã—U(1) breaking chain.
        
        The holonomy of Kâ‚‡ determines which subgroup survives.
        """
        print("\nSymmetry breaking chain:")
        print("  Eâ‚ˆ â†’ Eâ‚† Ã— SU(3)_hidden")
        print("  Eâ‚† â†’ SO(10) Ã— U(1)")
        print("  SO(10) â†’ SU(5) Ã— U(1)")
        print("  SU(5) â†’ SU(3)_c Ã— SU(2)_L Ã— U(1)_Y")
        print()
        
        # Dimension counting
        dims = {
            'E8': 248,
            'E6': 78,
            'SO10': 45,
            'SU5': 24,
            'SM': 8 + 3 + 1  # SU(3) + SU(2) + U(1)
        }
        
        print("  Dimension flow:")
        for name, dim in dims.items():
            print(f"    {name}: {dim}")
        
        return dims
    
    def mass_gap_propagation(self):
        """
        Show that mass gap propagates from Kâ‚‡ to 4D SU(3).
        
        Key insight:
        - Zero modes of Î”_Kâ‚‡ â†’ massless 4D fields (gauge bosons)
        - First excited mode Î»â‚ â†’ lightest massive excitation
        
        For QCD (SU(3) sector):
        - Gluons are massless at tree level
        - Confinement generates mass gap Î” ~ Î›_QCD
        - GIFT claims: Î”/Î›_QCD = dim(Gâ‚‚)/H* = 14/99
        """
        lambda_1 = self.spectrum[1]  # First nonzero eigenvalue
        
        print("\nMass gap propagation:")
        print(f"  Î»â‚(Kâ‚‡) = {lambda_1:.6f}")
        print()
        print("  Geometric mass gap (in Planck units):")
        print(f"    m_KK = âˆšÎ»â‚ Ã— M_Planck")
        print()
        
        # GIFT claim
        h_gift = self.G['dim_G2'] / self.G['H_star']
        
        print("  GIFT conjecture:")
        print(f"    Î”_QCD / Î›_QCD = h(Kâ‚‡) = {h_gift:.6f}")
        print()
        print("  If Î»â‚ â‰ˆ hÂ² = (14/99)Â² = 0.02:")
        print("    The geometric gap matches the GIFT prediction")
        
        return {
            'lambda_1': lambda_1,
            'h_gift': h_gift,
            'h_squared': h_gift**2
        }
    
    def construct_effective_lagrangian(self):
        """
        Write the 4D effective Lagrangian after KK reduction.
        """
        print("\n4D Effective Lagrangian:")
        print()
        print("  L_eff = L_YM[SU(3)] + L_YM[SU(2)] + L_YM[U(1)]")
        print("        + Î£_n (D_Î¼ Ï†_n)Â² + m_nÂ² |Ï†_n|Â²")
        print("        + interactions")
        print()
        print("  where m_nÂ² = Î»_n(Kâ‚‡)")
        print()
        print("  The mass gap is:")
        print("    Î” = min{m_n : m_n > 0} = âˆšÎ»â‚")
        
        
class TheoremStatement:
    """
    Formal statement of the Yang-Mills connection.
    """
    
    @staticmethod
    def main_theorem():
        """
        The main claim connecting GIFT topology to Yang-Mills gap.
        """
        statement = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                         MAIN THEOREM (Conjecture)                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                          â•‘
â•‘  Let Kâ‚‡ be the compact Gâ‚‚-holonomy manifold constructed via TCS with     â•‘
â•‘  Betti numbers bâ‚‚ = 21, bâ‚ƒ = 77.                                         â•‘
â•‘                                                                          â•‘
â•‘  Let Î”_Kâ‚‡ be the Hodge Laplacian on 1-forms, with spectrum              â•‘
â•‘  0 = Î»â‚€ < Î»â‚ â‰¤ Î»â‚‚ â‰¤ ...                                                 â•‘
â•‘                                                                          â•‘
â•‘  CLAIM: The Cheeger constant satisfies                                   â•‘
â•‘                                                                          â•‘
â•‘         h(Kâ‚‡) = dim(Gâ‚‚) / H* = 14/99                                    â•‘
â•‘                                                                          â•‘
â•‘  and consequently (by Cheeger's inequality):                             â•‘
â•‘                                                                          â•‘
â•‘         Î»â‚ â‰¥ hÂ²/4 = 196/39204 â‰ˆ 0.005                                   â•‘
â•‘                                                                          â•‘
â•‘  PHYSICAL CONSEQUENCE:                                                   â•‘
â•‘                                                                          â•‘
â•‘  Under Kaluza-Klein reduction of Eâ‚ˆÃ—Eâ‚ˆ gauge theory on Kâ‚‡,              â•‘
â•‘  the 4D SU(3) sector inherits a mass gap:                                â•‘
â•‘                                                                          â•‘
â•‘         Î”_QCD = h(Kâ‚‡) Ã— Î›_QCD = (14/99) Ã— 200 MeV â‰ˆ 28 MeV             â•‘
â•‘                                                                          â•‘
â•‘  This provides a TOPOLOGICAL origin for the Yang-Mills mass gap.         â•‘
â•‘                                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(statement)
    
    @staticmethod
    def proof_outline():
        """
        Outline of required proof steps.
        """
        outline = """
PROOF OUTLINE (Required Steps):

1. GEOMETRIC SETUP
   â–¡ Construct explicit TCS Kâ‚‡ with (bâ‚‚=21, bâ‚ƒ=77)
   â–¡ Verify Gâ‚‚ holonomy (torsion-free Ï†)
   â–¡ Compute metric det(g) = 65/32
   
2. SPECTRAL ANALYSIS  
   â–¡ Prove Î”_Kâ‚‡ has discrete spectrum (compactness)
   â–¡ Compute or bound Î»â‚ numerically
   â–¡ Estimate Cheeger constant h(Kâ‚‡)
   
3. KALUZA-KLEIN REDUCTION
   â–¡ Decompose Eâ‚ˆÃ—Eâ‚ˆ gauge field on Mâ‚„ Ã— Kâ‚‡
   â–¡ Show m_nÂ² = Î»_n for KK modes
   â–¡ Verify Eâ‚ˆ â†’ SM breaking preserves gap structure
   
4. PHYSICAL IDENTIFICATION
   â–¡ Match SU(3) sector with QCD
   â–¡ Relate geometric scale to Î›_QCD
   â–¡ Derive Î” = h Ã— Î›_QCD

5. RIGOROUS BOUNDS
   â–¡ Prove h(Kâ‚‡) = 14/99 (or derive from topology)
   â–¡ Apply Cheeger inequality
   â–¡ Establish Î»â‚ > 0 rigorously
   
STATUS: Steps 1-2 are computationally tractable.
        Steps 3-4 require careful physics.
        Step 5 requires geometric analysis.
        """
        print(outline)
```

---

## ğŸ“ PHASE 6: PAPER & PUBLICATION (Semaine 7-8)

### 6.1 Structure du papier

```
paper/yang_mills_gift.tex

Title: "Topological Origin of the Yang-Mills Mass Gap 
        from Gâ‚‚-Holonomy Compactification"

1. Introduction
   - Yang-Mills mass gap problem
   - GIFT framework overview
   - Main claim: Î” = (14/99) Ã— Î›_QCD

2. Mathematical Framework
   - Eâ‚ˆÃ—Eâ‚ˆ gauge theory in 11D
   - Gâ‚‚ holonomy and Kâ‚‡ construction
   - Topological invariants (bâ‚‚=21, bâ‚ƒ=77)

3. Spectral Analysis
   - Hodge Laplacian on Kâ‚‡
   - Numerical computation of spectrum
   - Cheeger constant estimation

4. Kaluza-Klein Reduction
   - Dimensional reduction Mâ‚â‚ â†’ Mâ‚„ Ã— Kâ‚‡
   - Symmetry breaking Eâ‚ˆ â†’ SM
   - Mass gap propagation

5. Numerical Results
   - PINN metric learning
   - Spectral computation
   - Comparison with GIFT predictions

6. Discussion
   - Implications for Yang-Mills problem
   - Limitations and assumptions
   - Future directions

7. Conclusion

Appendix A: GIFT Constants
Appendix B: Lean 4 Verifications
Appendix C: Numerical Methods
```

---

## âš¡ RÃ‰SUMÃ‰ POUR CLAUDE CODE

### Commandes d'initialisation

```bash
# CrÃ©er le repo
mkdir gift-yang-mills
cd gift-yang-mills
git init

# Structure
mkdir -p src/{spectral,gauge,visualization}
mkdir -p notebooks data/{pinn_checkpoints,spectral_results} paper tests

# DÃ©pendances
cat > requirements.txt << EOF
torch>=2.0
numpy>=1.24
scipy>=1.10
matplotlib>=3.7
tqdm
scikit-learn
sympy
EOF
```

### Ordre d'exÃ©cution

1. **`src/constants.py`** - GIFT constants
2. **`src/g2_structure.py`** - Gâ‚‚ forms
3. **`notebooks/01_pinn_metric_training.ipynb`** - Train PINN (Colab A100)
4. **`notebooks/02_manifold_sampling.ipynb`** - Sample Kâ‚‡
5. **`notebooks/03_spectral_analysis.ipynb`** - Compute Î»â‚ (KEY!)
6. **`notebooks/04_cheeger_estimation.ipynb`** - Estimate h(Kâ‚‡)
7. **`notebooks/05_kk_reduction.ipynb`** - Physics derivation
8. **Paper writing**

### Cibles numÃ©riques

| QuantitÃ© | Valeur cible | TolÃ©rance |
|----------|--------------|-----------|
| det(g) | 65/32 = 2.03125 | Â±0.01 |
| â€–Tâ€– (torsion) | < 0.001 | - |
| h(Kâ‚‡) | 14/99 â‰ˆ 0.1414 | Â±20% |
| Î»â‚ | â‰¥ 0.005 | - |
| Î”/Î›_QCD | 14/99 â‰ˆ 0.14 | Â±20% |

### CritÃ¨re de succÃ¨s

**SI Î»â‚ â‰ˆ 0.02 (= (14/99)Â²) sort des calculs numÃ©riques, c'est un rÃ©sultat majeur.**

---

## ğŸ¯ PROCHAINE ACTION IMMÃ‰DIATE

**Pour Claude Code:**

```
1. CrÃ©er le repo gift-yang-mills avec la structure ci-dessus
2. ImplÃ©menter src/constants.py et src/g2_structure.py
3. Adapter le PINN existant (K7_Explicit_Metric_v3_2.ipynb) 
   pour la nouvelle structure
4. CrÃ©er notebook 03_spectral_analysis.ipynb 
   (C'EST LE NOTEBOOK CLÃ‰)
5. Lancer sur Colab A100 avec N=10000 points
6. Reporter Î»â‚ et h estimÃ©s
```

**Output attendu:**
```
Î»â‚ = 0.0XX (comparÃ© Ã  cible 0.02)
h = 0.1XX (comparÃ© Ã  cible 0.14)
```

Si ces valeurs sont proches des cibles, on a un papier. Si non, on analyse pourquoi.

---

*"Le gap est dÃ©jÃ  lÃ , gÃ©omÃ©triquement. La question est de le quantifier."*

---

**GO ! ğŸš€**
